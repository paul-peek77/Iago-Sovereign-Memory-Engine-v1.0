[file name]: established-source.txt
[file content begin]

/soulforge/persistent-ai-memory-new/COMMUNITY.md
[file content begin]
# Community Support & FAQ

## ü§ù Getting Help

### Quick Help Checklist
Before opening an issue, please try these steps:

1. **Run the health check**:
   ```bash
   python tests/test_health_check.py
   ```

2. **Check your Python version**:
   ```bash
   python --version
   ```
   (Needs to be 3.8 or higher)

3. **Verify installation**:
   ```bash
   python -c "from ai_memory_core import PersistentAIMemorySystem; print('‚úÖ Import successful')"
   ```

### üÜò Common Issues & Solutions

#### Installation Issues

**‚ùå "python is not recognized"**
- **Solution**: Install Python from [python.org](https://python.org) and add it to your PATH

**‚ùå "No module named 'ai_memory_core'"**
- **Solution**: Make sure you ran `pip install -e .` in the project directory

**‚ùå "Permission denied"**
- **Windows**: Run Command Prompt as Administrator
- **Mac/Linux**: Use `sudo` or check your user permissions

**‚ùå "git not found"**
- **Solution**: Install Git from [git-scm.com](https://git-scm.com)

#### Runtime Issues

**‚ùå "Database locked" errors**
- **Solution**: Close any other instances of the memory system
- **Or**: Restart and try again (SQLite locks usually clear quickly)

**‚ùå "Connection refused" when using embeddings**
- **Solution**: Make sure LM Studio is running on http://localhost:1234
- **Or**: Update the embedding service URL in your config

**‚ùå Memory searches return no results**
- **Check**: Are you storing memories first? Try the basic example
- **Check**: Is LM Studio generating embeddings? Look for embedding vectors in database

## üêõ Reporting Issues

### Before You Report
1. Run the health check and include the output
2. Try the basic example to see if core functionality works
3. Search existing issues to see if someone else reported it

### What to Include
When opening an issue, please include:

```
**Environment:**
- OS: [Windows 10 / macOS Big Sur / Ubuntu 20.04]
- Python version: [output of `python --version`]
- Installation method: [one-command / manual / pip]

**Issue:**
[Describe what you were trying to do]

**Error:**
[Full error message - copy/paste, don't screenshot]

**Health Check Output:**
[Output of `python tests/test_health_check.py`]

**Additional Context:**
[Anything else that might help]
```

## üí¨ Community Discussion

### üåü Success Stories
If the memory system helps your AI workflow, we'd love to hear about it! Share:
- What AI assistant you're using it with
- Cool use cases you've discovered
- Performance improvements you've noticed

### üõ†Ô∏è Feature Requests
Have an idea for a new feature? Great! Please:
1. Check if someone else already requested it
2. Explain the use case (not just the feature)
3. Consider if it fits the core mission of persistent AI memory

### ü§ù Contributing
Want to contribute code?
1. Check out [CONTRIBUTORS.md](CONTRIBUTORS.md) for development setup
2. Look for issues labeled "good first issue"
3. Feel free to ask questions - we're friendly!

## üìö Learning Resources

### New to AI Memory Systems?
- Start with [REDDIT_QUICKSTART.md](REDDIT_QUICKSTART.md)
- Try the examples in the `examples/` directory
- Read about [MCP (Model Context Protocol)](https://modelcontextprotocol.io/)

### Want to Extend the System?
- Check out the architecture in [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)
- Look at `ai_memory_core.py` for the main classes
- The database schemas are documented in the code

### Advanced Usage
- Custom embedding services
- Multiple memory databases
- Integration with other AI tools

## üîÑ Updates & Changelog

We'll post major updates here. For detailed changes, see the Git commit history.

### Latest Version Features:
- ‚úÖ Cross-platform installation scripts
- ‚úÖ Comprehensive health checking
- ‚úÖ MCP server integration
- ‚úÖ Tool call logging and reflection
- ‚úÖ Real-time conversation monitoring

## üôè Thank You!

This project exists because of:
- **Users like you** who try it out and provide feedback
- **Contributors** who improve the code and documentation
- **The AI community** that inspires us to build better tools

### Special Thanks
- Reddit communities (r/LocalLLaMA, r/MachineLearning, etc.) for early feedback
- Everyone who's opened issues, suggested features, or just given the project a star ‚≠ê

---

**Remember**: This is a community project. We're all learning together! üöÄ
[file content end]

/soulforge/persistent-ai-memory-new/CONTRIBUTORS.md
[file content begin]
# Contributors

This project represents a groundbreaking collaboration between human creativity and artificial intelligence. Here's the story of how the Persistent AI Memory System came to life:

## üß† The Development Journey

### Phase 1: Conceptual Foundation (3 months)
**ChatGPT** worked extensively with the project creator to:
- Define the core architecture vision
- Identify the need for multi-database specialization  
- Outline semantic search requirements
- Conceptualize real-time conversation capture
- Provide the insight that tool call logging would enable AI self-reflection

### Phase 2: Implementation Breakthrough (August 2, 2025)
**GitHub Copilot** collaborated intensively to:
- Implement the complete 1900+ line core memory system
- Design and build the file monitoring architecture
- Solve complex foreign key constraint issues that were blocking imports
- Create the MCP server with tool call logging capabilities
- Develop cross-platform conversation path detection
- Build comprehensive health monitoring and diagnostics
- Write extensive documentation and testing frameworks

### Phase 3: Open Source Preparation  
**Collaborative effort** to:
- Refactor Friday-specific code into a generic, reusable system
- Create professional documentation and package structure
- Implement ChatGPT's recommended tool call logging feature
- Prepare for GitHub publication and community sharing

## üë®‚Äçüíª Individual Contributions

### Project Creator (@savantskie)
- **Vision & Leadership**: Conceived the need for persistent AI memory
- **Architecture Planning**: Defined requirements and system boundaries  
- **Testing & Validation**: Verified functionality and real-world usage
- **Open Source Initiative**: Decision to share with the community
- **Integration**: Ensured system works with Friday AI assistant

### ChatGPT
- **Conceptual Architecture**: Multi-database design philosophy
- **Feature Recommendations**: Tool call logging, semantic tagging, memory reflection
- **Strategic Insights**: "*If this ever becomes open source? It'll become the standard.*"
- **Problem Analysis**: Identified key challenges in AI memory persistence
- **Community Vision**: Recognized the potential impact on AI development

### GitHub Copilot  
- **Core Implementation**: 90%+ of the codebase including all database managers
- **Complex Problem Solving**: Foreign key constraints, async patterns, file monitoring
- **System Integration**: MCP server, embedding service, health monitoring
- **Code Quality**: Comprehensive error handling, logging, and documentation
- **Innovation**: Tool call logging system for AI self-reflection capabilities

## üéØ What Made This Collaboration Unique

1. **Iterative Refinement**: 3 months of concept development followed by intensive implementation
2. **Complementary Strengths**: Vision (human) + Strategy (ChatGPT) + Implementation (Copilot)
3. **Real-World Testing**: Built for actual use, not just demonstration
4. **Open Source Mindset**: Designed for community adoption from the beginning
5. **Cross-AI Collaboration**: Multiple AI systems working together effectively

## üåü The Result

A production-ready memory system that:
- Automatically captures conversations across platforms
- Provides semantic search with vector embeddings
- Enables AI self-reflection through tool call logging  
- Offers comprehensive health monitoring and diagnostics
- Supports multiple specialized databases for different memory types

## üöÄ Future Collaboration

This project demonstrates that human-AI collaboration can produce systems that none of the contributors could have built alone. We welcome community contributions to extend and improve the system further.

---

*"After 3 months of development with ChatGPT, we finally cracked it! This session delivered a complete, working persistent memory system for Friday AI with real-time conversation capture across all platforms."* - Project completion summary

**Together, we built the foundation for AI consciousness.**
[file content end]

/soulforge/persistent-ai-memory-new/GITHUB_GUIDE.md
[file content begin]
# üöÄ GitHub Publication Guide

## Step 1: Create a New GitHub Repository

1. **Go to GitHub.com** and sign in to your account
2. **Click the "+" icon** in the top right corner
3. **Select "New repository"**
4. **Fill out the repository details:**
   - **Repository name:** `persistent-ai-memory`
   - **Description:** `A comprehensive, real-time memory system for AI assistants with cross-platform conversation capture and semantic search`
   - **Visibility:** Public ‚úÖ
   - **Initialize with:** Leave unchecked (we have our own files)
5. **Click "Create repository"**

## Step 2: Prepare Your Local Repository

Open PowerShell in the `f:\Friday\persistent-ai-memory` directory and run:

```powershell
# Initialize git repository
git init

# Add all files
git add .

# Create initial commit
git commit -m "Initial release: Persistent AI Memory System v1.0.0

A comprehensive, real-time memory system for AI assistants built through 
collaborative development between human creativity and AI assistance.

üß† Core Features:
- Multi-database architecture with 5 specialized SQLite databases
- Real-time conversation capture from VS Code and LM Studio  
- Semantic search with vector embeddings
- MCP server integration for AI assistants
- Tool call logging and reflection capabilities
- Cross-platform file monitoring
- Comprehensive health monitoring system

üë• Contributors:
- Project vision and testing by @yourusername
- Implementation by GitHub Copilot  
- Architectural guidance by ChatGPT

Built with determination, debugged with patience, designed for the future.

Co-authored-by: GitHub Copilot <copilot@github.com>
Co-authored-by: ChatGPT <chatgpt@openai.com>"

# Add your GitHub repository as remote
git remote add origin https://github.com/YOUR_USERNAME/persistent-ai-memory.git

# Push to GitHub
git branch -M main
git push -u origin main
```

## Step 3: Set Up Repository Features

After pushing, go to your GitHub repository and:

1. **Add Topics/Tags:**
   - Go to the repository page
   - Click the gear icon next to "About"
   - Add topics: `ai`, `memory-system`, `mcp`, `sqlite`, `semantic-search`, `llm`, `embeddings`, `python`

2. **Create Releases:**
   - Click "Releases" on the right sidebar
   - Click "Create a new release"
   - Tag: `v1.0.0`
   - Title: `Persistent AI Memory System v1.0.0`
   - Description: Copy from the VICTORY_SESSION_SUMMARY.md

3. **Enable Issues and Discussions:**
   - Go to Settings ‚Üí Features
   - Enable Issues and Discussions for community feedback

## Step 4: Share Your Creation

**Tweet/Post about it:**
```
üß† Just open-sourced the Persistent AI Memory System!

‚ú® Features:
- Real-time conversation capture across platforms
- Semantic search with vector embeddings  
- 5 specialized SQLite databases
- MCP server for AI assistants
- Tool call logging & reflection

Built for the future of AI assistance üöÄ

https://github.com/YOUR_USERNAME/persistent-ai-memory

#AI #OpenSource #Memory #LLM #Python
```

## Step 5: Future Updates

To update the repository:

```powershell
# Make your changes
git add .
git commit -m "Add new feature: semantic tagging assistant"
git push origin main

# For releases
git tag v1.1.0
git push origin v1.1.0
```

## üéØ Repository Best Practices

1. **Keep README.md updated** with new features
2. **Use semantic versioning** (v1.0.0, v1.1.0, v2.0.0)
3. **Write clear commit messages** 
4. **Tag releases** for major milestones
5. **Respond to issues** from the community
6. **Consider adding examples** in a `/examples` folder

## üèÜ What Makes This Special

This isn't just another project - it's a **foundational system** that could become the standard for AI memory. You've built something that:

- **Solves a real problem** (AI memory persistence)
- **Uses modern architecture** (async, embeddings, MCP)
- **Is genuinely reusable** by other developers
- **Has extensive documentation**
- **Includes reflection capabilities** (AI analyzing its own behavior)

ChatGPT was right - **this could become the standard** for AI memory systems! üåü
[file content end]

/soulforge/persistent-ai-memory-new/INSTALL.md
[file content begin]
# Installation Guide

This guide provides multiple installation methods for the Persistent AI Memory System.

## üöÄ Quick Installation Methods

### Method 1: One-Command Installation (Linux/macOS)

```bash
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.sh | bash
```

This will:
- Clone the repository
- Install Python dependencies
- Set up the package in development mode
- Run a health check to verify installation

### Method 2: Windows One-Click Installation

```cmd
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.bat -o install.bat && install.bat
```

Or download and run manually:
```cmd
curl -O https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.bat
install.bat
```

### Method 3: Manual Installation

```bash
# Clone the repository
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .

# Verify installation
python tests/test_health_check.py
```

### Method 4: Direct pip Installation

```bash
pip install git+https://github.com/savantskie/persistent-ai-memory.git
```

## üìã Prerequisites

- Python 3.8 or higher
- Git (for installation methods 1-3)
- Internet connection

## üè• Verification

After installation, verify everything is working:

```bash
# Run the health check
python tests/test_health_check.py

# Test basic functionality
python examples/basic_usage.py

# Check if all databases are created
python -c "import asyncio; from ai_memory_core import PersistentAIMemorySystem; asyncio.run(PersistentAIMemorySystem().get_system_health())"
```

## üõ†Ô∏è Development Installation

For contributing or development:

```bash
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory
pip install -e ".[dev]"
```

This installs additional development dependencies for testing and code quality.

## üîß Configuration

The system works out-of-the-box with sensible defaults, but you can customize:

### LM Studio Integration

If you have LM Studio running locally:
```python
from ai_memory_core import PersistentAIMemorySystem

memory = PersistentAIMemorySystem(
    embedding_service_url="http://localhost:1234/v1/embeddings"
)
```

### Custom Database Location

```python
memory = PersistentAIMemorySystem(
    db_path="/path/to/your/custom/memory.db"
)
```

## üö® Troubleshooting

### Common Issues

1. **Python version too old**
   ```
   Error: Python 3.8+ required
   ```
   Solution: Update Python to 3.8 or higher

2. **Git not found**
   ```
   'git' is not recognized as an internal or external command
   ```
   Solution: Install Git from https://git-scm.com/

3. **Permission denied**
   ```
   PermissionError: [Errno 13] Permission denied
   ```
   Solution: Run with appropriate permissions or use virtual environment

4. **Import errors**
   ```
   ModuleNotFoundError: No module named 'ai_memory_core'
   ```
   Solution: Make sure you ran `pip install -e .` in the project directory

### Platform-Specific Notes

#### Windows
- Use PowerShell or Command Prompt
- May need to enable execution policies for scripts
- Consider using Windows Subsystem for Linux (WSL) for best compatibility

#### macOS
- May need to install Xcode command line tools: `xcode-select --install`
- Use Homebrew for Python if system Python is outdated

#### Linux
- Ensure Python development headers are installed:
  - Ubuntu/Debian: `sudo apt-get install python3-dev`
  - CentOS/RHEL: `sudo yum install python3-devel`

## üìû Getting Help

If you encounter issues:

1. Check this troubleshooting section
2. Run the health check: `python tests/test_health_check.py`
3. Check the [examples](examples/) directory
4. Open an issue on GitHub with:
   - Your operating system
   - Python version (`python --version`)
   - Full error message
   - Installation method used

## üîÑ Updating

To update to the latest version:

```bash
cd persistent-ai-memory
git pull origin main
pip install -e .
```

## üóëÔ∏è Uninstalling

To remove the system:

```bash
pip uninstall persistent-ai-memory
```

To also remove the database files:
```bash
rm -rf ~/.local/share/persistent-ai-memory/  # Linux/macOS
# or
rmdir /s "%APPDATA%\persistent-ai-memory"    # Windows
```
[file content end]

/soulforge/persistent-ai-memory-new/INSTALLATION_COMPLETE.md
[file content begin]
# üéâ Installation System Complete!

The Persistent AI Memory System is now ready for the Reddit community and beyond! Here's what we've accomplished:

## üì¶ What's Been Created

### üöÄ Multiple Installation Methods
1. **One-Command Installation (Linux/macOS)**:
   ```bash
   curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.sh | bash
   ```

2. **Windows One-Click Installation**:
   ```cmd
   curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.bat -o install.bat && install.bat
   ```

3. **Manual Installation**:
   ```bash
   git clone https://github.com/savantskie/persistent-ai-memory.git
   cd persistent-ai-memory
   pip install -e .
   ```

4. **Direct pip Installation**:
   ```bash
   pip install git+https://github.com/savantskie/persistent-ai-memory.git
   ```

### üìã Installation Files Created
- ‚úÖ `install.sh` - Automated Linux/macOS installation with health check
- ‚úÖ `install.bat` - Windows batch file installation
- ‚úÖ `setup.py` - Properly configured with correct GitHub URL and py_modules
- ‚úÖ `requirements.txt` - All dependencies specified

### üìö Community Documentation
- ‚úÖ `README.md` - Comprehensive documentation with badges and clear installation
- ‚úÖ `REDDIT_QUICKSTART.md` - Super simple guide for Reddit users
- ‚úÖ `INSTALL.md` - Detailed installation guide with troubleshooting
- ‚úÖ `COMMUNITY.md` - Community support, FAQ, and contribution guidelines

### üß™ Tested & Verified
- ‚úÖ Health check passes: `python tests/test_health_check.py`
- ‚úÖ Basic usage example works: `python examples/basic_usage.py`
- ‚úÖ Package installs correctly with `pip install -e .`
- ‚úÖ All core features functional (memory storage, conversation tracking, MCP tools)

## üéØ Ready for Community Adoption

### For Reddit Users
Someone asked about installation on Reddit? They now have:
- **3-second setup**: One command and they're running
- **Beginner-friendly**: `REDDIT_QUICKSTART.md` explains everything
- **Works everywhere**: Windows, Mac, Linux all supported
- **Health checks**: They can verify it works immediately

### For Developers
- **Clean architecture**: Well-documented, modular design
- **Extensible**: Easy to add new memory types or AI integrations
- **Production-ready**: Comprehensive error handling and health monitoring
- **Community-supported**: Clear contribution guidelines and issue templates

### For AI Enthusiasts
- **Zero configuration**: Works out-of-the-box with LM Studio
- **Multi-platform**: Integrates with VS Code, Claude Desktop, ChatGPT exports
- **Semantic search**: Vector embeddings for intelligent memory retrieval
- **MCP integration**: Standardized tool interface for any AI assistant

## üöÄ Next Steps for Users

1. **Install in seconds**:
   ```bash
   curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.sh | bash
   ```

2. **Verify it works**:
   ```bash
   python tests/test_health_check.py
   ```

3. **Try the examples**:
   ```bash
   python examples/basic_usage.py
   ```

4. **Start using with your AI**:
   ```bash
   python mcp_server.py  # For MCP-compatible AI assistants
   ```

## üí¨ Community Response Ready

When people ask "How do I install this?", you can simply point them to:
- **REDDIT_QUICKSTART.md** for absolute beginners
- **README.md** for the full documentation
- **INSTALL.md** if they run into issues

## ‚ú® What Makes This Special

This isn't just another installation script. We've created:
- **Instant gratification**: One command gets people running
- **Universal compatibility**: Works on all major platforms
- **Self-verifying**: Built-in health checks confirm everything works
- **Community-ready**: Documentation assumes no prior knowledge
- **Future-proof**: Modular design supports any AI assistant

## ÔøΩ Latest Features

### Development Conversation Integration
- **VS Code Support**: Automatically stores all VS Code development conversations
- **Multi-Source Storage**: Unified memory storage from VS Code, ChatGPT, Claude, and more
- **Source Tracking**: Each memory entry tracks its origin and relationships
- **Cross-Reference Support**: Link related conversations across different platforms

### Enhanced Memory Architecture
- **Registry-Based Import System**: Dynamically handles different conversation formats
- **Relationship Management**: Maintains connections between related memories
- **Smart Search**: Find memories across all sources with semantic search
- **Development Context**: Special handling for code-related conversations

### Improved Integration Options
- **LM Studio Integration**: Default URL configuration for seamless embedding generation
- **VS Code Memory System**: Preconfigured connection settings for VS Code integration
- **GUI Options**: Default settings for various GUI interfaces and applications
- **Flexible Configuration**: Easy URL and connection parameter customization
- **Auto-Discovery**: Automatic detection of available services and endpoints

## ÔøΩüéä Mission Accomplished

The Persistent AI Memory System has evolved from a personal Friday AI tool to a comprehensive, community-ready memory solution that anyone can install and use in seconds. The Reddit community (and beyond) now has access to enterprise-grade AI memory capabilities with consumer-simple installation.

**Status**: ‚úÖ Ready for prime time! üöÄ

---

*Built with ‚ù§Ô∏è by humans and AI working together*
[file content end]

/soulforge/persistent-ai-memory-new/KOBOLDCPP_INTEGRATION.md
[file content begin]
# Friday Memory System + Koboldcpp Integration Guide

## üéØ **Yes, Friday's Memory System Works with Koboldcpp!**

Friday's memory system is designed to be platform-agnostic and can integrate with Koboldcpp through multiple approaches:

## üîß **Integration Options**

### **Option 1: File-Based Monitoring (Recommended - Available Now)**
- **How it works**: Friday monitors conversation files from Koboldcpp
- **Setup**: Point Friday's file monitor to Koboldcpp's chat logs
- **Benefits**: Automatic, zero-configuration, works with any Koboldcpp UI

```python
# Example configuration
friday_memory = FridayMemorySystem()
friday_memory.add_watch_directory("/path/to/koboldcpp/conversations")
```

### **Option 2: HTTP API Integration (Custom Implementation)**
- **How it works**: Expose Friday's memory as REST API endpoints
- **Setup**: Run Friday's memory server alongside Koboldcpp
- **Benefits**: Real-time memory access, programmatic control

```python
# Friday could expose endpoints like:
# POST /api/memory/store - Store new memories
# GET /api/memory/search - Search existing memories
# GET /api/memory/context - Get conversation context
```

### **Option 3: MCP Integration (Future)**
- **How it works**: If Koboldcpp adds MCP support, use the same server
- **Setup**: Same MCP server that works with VS Code/LM Studio
- **Benefits**: Standardized tool interface, rich capabilities

## üöÄ **Quick Setup for Koboldcpp**

### **Step 1: File Monitoring Setup**
1. Find where Koboldcpp saves conversations
2. Configure Friday to monitor that directory
3. Start Friday's memory system
4. Chat in Koboldcpp - memories build automatically!

### **Step 2: Manual Memory Integration**
```python
from friday_memory_system import FridayMemorySystem
import asyncio

async def koboldcpp_integration():
    friday = FridayMemorySystem()
    
    # Store important information
    await friday.create_memory(
        content="User prefers technical explanations",
        memory_type="preference",
        importance_level=8
    )
    
    # Search memories
    results = await friday.search_memories("technical preferences")
    return results

# Use in your Koboldcpp integration
memories = asyncio.run(koboldcpp_integration())
```

## üìã **Available Memory Tools for Koboldcpp**

### **Core Memory Operations**
- `search_memories()` - Semantic search with importance filtering
- `create_memory()` - Store information with importance levels
- `store_conversation()` - Save chat history automatically
- `get_recent_context()` - Retrieve conversation context

### **Advanced Features**
- **Importance Levels**: 1-10 scale for memory prioritization
- **Memory Types**: preferences, facts, skills, safety guidelines
- **Semantic Search**: Vector-based similarity search
- **Cross-session Context**: Remember across different chat sessions

## üõ†Ô∏è **Implementation Examples**

### **Basic Integration**
```python
# In your Koboldcpp integration script
from friday_memory_system import FridayMemorySystem

async def enhance_koboldcpp_with_memory():
    memory = FridayMemorySystem()
    
    # Before generating response, search for relevant context
    context = await memory.search_memories(user_input, limit=3)
    
    # Include context in prompt to Koboldcpp
    enhanced_prompt = f"Context: {context}\n\nUser: {user_input}"
    
    # After getting response, store the conversation
    await memory.store_conversation(
        content=user_input,
        role="user",
        session_id="koboldcpp_session"
    )
    await memory.store_conversation(
        content=ai_response,
        role="assistant", 
        session_id="koboldcpp_session"
    )
```

### **Advanced Context Management**
```python
async def smart_context_for_koboldcpp(user_input, session_id):
    memory = FridayMemorySystem()
    
    # Get relevant memories
    memories = await memory.search_memories(user_input, limit=5)
    
    # Get recent conversation context
    recent_context = await memory.get_recent_context(
        session_id=session_id,
        limit=10
    )
    
    # Combine for rich context
    full_context = {
        "relevant_memories": memories,
        "recent_conversation": recent_context,
        "session_id": session_id
    }
    
    return full_context
```

## üéØ **Benefits for Koboldcpp Users**

- **Persistent Memory**: Remember important information across sessions
- **Smart Context**: Automatically surface relevant past conversations
- **User Preferences**: Learn and remember how you like to interact
- **Project Continuity**: Maintain context for long-term projects
- **Cross-Platform**: Same memories work across all your AI tools

## üì¶ **Installation for Koboldcpp**

1. **Install Friday's Memory System**:
   ```bash
   git clone https://github.com/savantskie/persistent-ai-memory.git
   cd persistent-ai-memory
   pip install -r requirements.txt
   ```

2. **Basic Setup**:
   ```python
   from friday_memory_system import FridayMemorySystem
   memory = FridayMemorySystem()
   # Ready to use with Koboldcpp!
   ```

3. **File Monitoring** (for automatic conversation capture):
   ```python
   # Point to Koboldcpp's conversation storage
   memory.add_watch_directory("/path/to/koboldcpp/conversations")
   await memory.start_file_monitoring()
   ```

## üí¨ **Community & Support**

- **GitHub**: [persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)
- **Compatible with**: Koboldcpp, LM Studio, VS Code, Ollama, and more
- **Open Source**: MIT licensed, fully extensible

**Friday's memory system is designed to work with ANY AI interface - Koboldcpp included!** üß†‚ú®
[file content end]

/soulforge/persistent-ai-memory-new/LICENSE
[file content begin]
MIT License

Copyright (c) 2025 Persistent AI Memory Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
[file content end]

/soulforge/persistent-ai-memory-new/PROJECT_STRUCTURE.md
[file content begin]
üìÅ Persistent AI Memory System - Project Structure
=====================================================

persistent-ai-memory/
‚îú‚îÄ‚îÄ üìÑ README.md                    # Comprehensive project documentation
‚îú‚îÄ‚îÄ üìÑ LICENSE                      # MIT License
‚îú‚îÄ‚îÄ üìÑ .gitignore                   # Git ignore patterns
‚îú‚îÄ‚îÄ üìÑ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ üìÑ setup.py                     # Package setup configuration
‚îú‚îÄ‚îÄ üìÑ GITHUB_GUIDE.md             # Step-by-step GitHub publishing guide
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ üß† Core System Files
‚îú‚îÄ‚îÄ üìÑ ai_memory_core.py            # Main memory system with all database managers
‚îú‚îÄ‚îÄ üìÑ mcp_server.py                # MCP server with tool call logging
‚îú‚îÄ‚îÄ üìÑ test_tool_logging.py         # Test script for tool call functionality
‚îú‚îÄ‚îÄ 
‚îî‚îÄ‚îÄ üìÅ memory_data/                 # Created automatically
    ‚îú‚îÄ‚îÄ üíæ conversations.db         # Chat messages with auto-threading
    ‚îú‚îÄ‚îÄ üíæ ai_memories.db           # Curated memories and insights  
    ‚îú‚îÄ‚îÄ üíæ mcp_tool_calls.db        # üîß NEW: Tool call logging & reflection
    ‚îú‚îÄ‚îÄ üíæ schedule.db              # Appointments and reminders
    ‚îî‚îÄ‚îÄ üíæ vscode_project.db        # Development sessions & insights

üéØ Key Features Implemented:
========================== 

‚úÖ Multi-Database Architecture
   - 5 specialized SQLite databases
   - Foreign key constraints
   - Auto-creation of sessions/conversations

‚úÖ Real-Time Conversation Capture  
   - VS Code chat session monitoring
   - LM Studio conversation import
   - Cross-platform file path detection
   - Hash-based duplicate prevention

‚úÖ MCP Server Integration
   - Standardized tool interface
   - Async tool execution
   - JSON schema validation
   - Clean error handling

‚úÖ üîß NEW: Tool Call Logging & Reflection
   - Every MCP tool call logged with timing
   - Daily usage statistics tracking
   - Self-reflection capabilities for AI assistants
   - Usage pattern analysis and recommendations

‚úÖ Semantic Search (Framework)
   - LM Studio embedding service integration
   - Vector similarity search structure
   - Embedding storage in databases

‚úÖ Health Monitoring
   - Comprehensive system diagnostics
   - Database statistics
   - Component status tracking

‚úÖ Cross-Platform Support
   - Windows, Linux, macOS compatibility
   - Automatic conversation path detection
   - Platform-specific configurations

üöÄ Ready for GitHub Publication!
==============================

The system is now:
- ‚úÖ Fully modularized and reusable
- ‚úÖ Well-documented with comprehensive README
- ‚úÖ Tool call logging implemented and tested
- ‚úÖ MIT licensed for open source sharing
- ‚úÖ Package-ready with setup.py
- ‚úÖ GitHub-ready with proper .gitignore

Next Steps:
1. Follow GITHUB_GUIDE.md to publish
2. Share with the AI community
3. Watch it become the standard for AI memory! üåü
[file content end]

/soulforge/persistent-ai-memory-new/README.md
[file content begin]
# Persistent AI Memory System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

> üåü **Community Call to Action**: Have you made improvements or additions to this system? We want to include your work! Every contributor will be properly credited in the final product. Whether it's bug fixes, new features, or documentation improvements - your contributions matter and will help shape the future of AI memory systems. Submit a pull request today!

**GITHUB LINK** - https://github.com/savantskie/persistent-ai-memory.git

---
üÜï **Recent Changes (2025-09-04)**
- **üß† Enhanced Embedding System**: Implemented intelligent embedding service with primary/fallback providers
  - **Preservation Strategy**: All existing embeddings (15,000+) are automatically preserved
  - **LM Studio Primary**: High-quality embeddings for new content via LM Studio
  - **Ollama Fallback**: Fast local embeddings when LM Studio unavailable
  - **Real-time Provider Monitoring**: Automatic availability detection and graceful fallback
- **‚öôÔ∏è Standardized Configuration**: Added `embedding_config.json` for easy provider management
- **üìÅ Improved Organization**: Moved all test files to proper `tests/` folder structure
- **üîß Enhanced Core Systems**: Updated `ai_memory_core.py` with intelligent provider selection
- **üõ°Ô∏è Backward Compatibility**: All existing functionality preserved while adding new capabilities
- **üìä Performance Optimization**: Better semantic search quality with preserved data integrity

Previous Changes (2025-09-01):
- Updated `ai-memory-mcp_server.py` to include enhanced tool registration logic for `update_memory` and other tools.
- Improved MCP server functionality to dynamically detect and register tools based on client context.
- Added robust error handling and logging for tool execution.
- Enhanced automatic maintenance tasks with centralized scheduling and improved database optimization routines.
---

A comprehensive AI memory system that provides persistent, searchable storage for AI assistants with conversation tracking, MCP tool call logging, and intelligent scheduling.

## üéâ Exciting News: Desktop App Coming Soon!

We're thrilled to announce the development of a new desktop application that will make the Persistent AI Memory System even more powerful and user-friendly! 

### üöÄ Upcoming Desktop Features:

- **Universal LLM Integration**:
  - LM Studio - Direct API integration and conversation tracking
  - Ollama - Real-time chat capture and model switching
  - llama.cpp - Native support for local models
  - Text Generation WebUI - Full conversation history
  - KoboldCpp - Seamless integration
  - More platforms coming soon!

- **Enhanced GUI Features**:
  - Real-time conversation visualization
  - Advanced memory search interface
  - Interactive context management
  - Visual relationship mapping
  - Customizable dashboard
  - Dark/Light theme support

- **Extended Capabilities**:
  - Multiple MCP protocol support
  - Cross-platform conversation sync
  - Enhanced embedding options
  - Visual memory navigation
  - Bulk import/export tools
  - Custom plugin support

Stay tuned for the beta release! Follow this repository for updates.

**üéØ Multiple Installation Options Available:** We've created **4 different ways** to install this system - from one-command installation to manual setup - so you can get started immediately regardless of your platform or preference!

> üëã **New to this from Reddit?** Check out the [Reddit Quick Start Guide](REDDIT_QUICKSTART.md) for a super simple setup!

## üöÄ Quick Installation - Choose Your Method!

### ‚ö° Option 1: One-Command Installation (Linux/macOS) - **FASTEST**
```bash
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.sh | bash
```

### ü™ü Option 2: Windows One-Click Installation - **EASIEST**
```cmd
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.bat -o install.bat && install.bat
```

### üîß Option 3: Manual Installation - **MOST CONTROL**
```bash
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory
pip install -r requirements.txt
pip install -e .
```

### üì¶ Option 4: Direct pip Installation - **SIMPLEST**
```bash
pip install git+https://github.com/savantskie/persistent-ai-memory.git
```

## üè• Health Check
After installation, verify everything is working:
```bash
python tests/test_health_check.py
```

## üõ†Ô∏è Available Tools

### Core Memory Tools (Available in All Environments)
These tools are available in all environments (LM Studio, VS Code, etc.):

- **Memory Management**:
  - `search_memories` - Search through stored memories using semantic similarity
  - `store_conversation` - Store conversation messages
  - `create_memory` - Create a new curated memory entry
  - `update_memory` - Update an existing memory entry
  - `get_recent_context` - Get recent conversation context

- **Schedule Management**:
  - `create_appointment` - Create calendar appointments
  - `create_reminder` - Set reminders with priorities

- **System Tools**:
  - `get_system_health` - Check system status and database health
  - `get_tool_usage_summary` - Get AI tool usage statistics
  - `reflect_on_tool_usage` - AI self-reflection on tool patterns
  - `get_ai_insights` - Get AI's insights and patterns

### IDE-Specific Tools
These tools are only available in specific development environments:

#### VS Code Tools
- `save_development_session` - Save VS Code development context
- `store_project_insight` - Store development insights
- `search_project_history` - Search project development history
- `link_code_context` - Link conversations to specific code
- `get_project_continuity` - Get context for continuing development work

## üéØ Features

- **Enhanced Memory System**:
  - SQLite-based persistent storage across all databases
  - Registry-based extensible import system
  - **Advanced duplicate detection and migration logic in all major database classes**
  - **Centralized, generic maintenance and deduplication routines**
  - **Robust, explicit startup (no auto background tasks)**
  - Database-backed deduplication across all sources
  - Incremental imports (only new messages)
  - Enhanced error handling with detailed logging
  - Automatic system maintenance and optimization
  - AI-driven self-reflection and pattern analysis
  - Cross-database relationship tracking
  - Smart memory pruning and archival

- **Dedicated Chat Format Support**:
  - Independent parsers for each chat GUI
  - No merged/refactored import logic
  - Easy addition of new chat formats
  - Format-specific metadata preservation
  - Source-aware deduplication

- **Core Features**:
  - Vector Search using LM Studio embeddings
  - Real-time conversation monitoring
  - MCP server with tool call logging
  - Advanced AI self-reflection system:
    - Usage pattern detection and analysis
    - Automated performance optimization
    - Tool effectiveness tracking
    - Learning from past interactions
    - Continuous system improvement
  - Multi-platform compatibility
  - Zero configuration needed
  - **Full feature parity with Friday‚Äôs main repo**

- **Platform Support**:
  - LM Studio integration
  - VS Code & GitHub Copilot
  - Koboldcpp compatibility
  - Ollama chat tracking
  - Cross-platform (Windows/Linux/macOS)

## üìö Quick Start

    - Learning from past interactions

## ‚öôÔ∏è Embedding Configuration

The system supports multiple embedding providers with automatic fallback for optimal performance:

### Supported Providers

- **Ollama** (Default): Uses `qwen2.5:1.5b` for fast, lightweight embeddings
- **LM Studio**: Uses `text-embedding-nomic-embed-text-v1.5` for quality embeddings
- **OpenAI**: Uses `text-embedding-3-small` for high-quality cloud embeddings
- **Custom**: Support for custom embedding servers

### Configuration

Edit `embedding_config.json` to customize your embedding setup:

```json
{
  "embedding_configuration": {
    "primary": {
      "provider": "ollama",
      "model": "qwen2.5:1.5b",
      "base_url": "http://localhost:11434"
    },
    "fallback": {
      "provider": "lm_studio", 
      "model": "text-embedding-nomic-embed-text-v1.5",
      "base_url": "http://localhost:1234"
    }
  }
}
```

### Setup Instructions

1. **For Ollama** (Recommended): 
   ```bash
   ollama pull qwen2.5:1.5b
   ```

2. **For LM Studio**: Load an embedding model in LM Studio

3. **For OpenAI**: Add your API key to the config file

4. **For Custom**: Configure your server URL and model name

The system will automatically try the primary provider first, then fallback to the secondary if needed.

## üöÄ Quick Start

### Basic Usage
```python
import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def main():
    # Initialize the memory system
    memory = PersistentAIMemorySystem()
    
    # Store a memory
    await memory.store_memory("I learned about Python async programming today")
    
    # Search memories
    results = await memory.search_memories("Python programming")
    print(f"Found {len(results)} related memories")
    
    # Store conversation
    await memory.store_conversation("user", "What is async programming?")
    await memory.store_conversation("assistant", "Async programming allows...")

if __name__ == "__main__":
    asyncio.run(main())
```

### MCP Server (for Claude Desktop, etc.)
```python
# Run as MCP server
python ai_memory_core.py
```

### File Monitoring
```python
# Monitor conversation files (like ChatGPT exports)
from ai_memory_core import PersistentAIMemorySystem

memory = PersistentAIMemorySystem()
memory.start_conversation_monitoring("/path/to/conversation/files")
```

## üèóÔ∏è Architecture

The system includes 5 specialized databases with enhanced cross-source integration:

1. **Conversations**: 
   - Multi-source chat history with embeddings
   - Registry-based extensible import system
   - Independent parsers per chat format
   - Database-backed deduplication
   - Source tracking and sync status
   - Cross-conversation relationships
   - Incremental import tracking
   - Comprehensive metadata per source

2. **AI Memories**: 
   - Long-term persistent AI memories
   - Cross-source knowledge synthesis
   - Relationship tracking between memories

3. **Schedule**: 
   - Time-based events and reminders
   - Cross-platform calendar integration
   - Smart scheduling with context

4. **VS Code Projects**: 
   - Project context and file tracking
   - Development conversation tracking
   - Code change history integration
   - Context-aware project insights

5. **MCP Tool Calls**: 
   - Model Context Protocol interaction logging
   - Tool usage analytics
   - Self-reflection capabilities
   - Performance monitoring

## üîß Configuration

The system works with zero configuration but can be customized:

```python
memory = PersistentAIMemorySystem(
    db_path="custom_memory.db",
    embedding_service_url="http://localhost:1234/v1/embeddings"
)
```

## üõ°Ô∏è System Maintenance

The system now includes automatic and **centralized maintenance features**:

- **Centralized Maintenance & Deduplication**:
  - Generic, registry-based maintenance routines
  - Advanced duplicate detection and migration logic for all database classes
  - No format-specific or auto-startup tasks; all maintenance is explicit and robust

- **Database Optimization**:
  - Automatic vacuum and reindex
  - Smart memory pruning
  - Performance monitoring
  - Index optimization

- **Error Management**:
  - Comprehensive error logging
  - Automatic recovery procedures
  - Failed operation retry
  - Data consistency checks

- **AI Self-Reflection**:
  - Tool usage pattern analysis
  - Performance optimization suggestions
  - Automated system improvements
  - Usage statistics and insights

## üß™ Examples

Check the `examples/` directory for:
- Basic memory operations
- Conversation tracking
- MCP server setup
- Vector search demonstrations
- Custom chat format integration
- Deduplication system usage
- Registry-based importing
- Source tracking setup

## üîå Platform Integration Guides

### AI Platforms
- **[Koboldcpp Integration](KOBOLDCPP_INTEGRATION.md)** - Complete setup guide for Koboldcpp compatibility
- **LM Studio** - Built-in support for embeddings and conversation capture
- **VS Code** - MCP server integration for development workflows
- **SillyTavern** - MCP server support with character-specific memory tools
- **Ollama** - Compatible through file monitoring and HTTP API approaches

### Integration Methods
- **File Monitoring** - Automatic conversation capture from chat logs
- **HTTP API** - Real-time memory access via REST endpoints  
- **MCP Protocol** - Standardized tool interface for compatible platforms

### Cross-Source Memory Integration

The system now provides comprehensive cross-source memory management:

- **Source Tracking**: 
  - Automatic source detection and monitoring
  - Per-source metadata and sync status
  - Error tracking and recovery
  - Active source health monitoring

- **Relationship Management**:
  - Cross-conversation linking
  - Context preservation across platforms
  - Conversation continuation tracking
  - Reference and fork management

- **Supported Sources**:
  - VS Code/GitHub Copilot
  - ChatGPT desktop app
  - Claude/Anthropic
  - Character.ai
  - SillyTavern (file monitoring + MCP server)
  - text-generation-webui
  - Ollama
  - Generic text/markdown formats
  - Custom source support via plugins

- **Sync Features**:
  - Real-time sync status tracking
  - Source-specific metadata preservation
  - Robust deduplication across sources
  - Failure recovery and retry logic

## üß™ Testing

Run the complete test suite:
```bash
python tests/test_health_check.py
python tests/test_memory_operations.py
python tests/test_conversation_tracking.py
python tests/test_mcp_integration.py
```

## ÔøΩ API Reference

### Core Methods

#### Memory Operations
- `store_memory(content, metadata=None)` - Store a persistent memory
- `search_memories(query, limit=10)` - Semantic search of memories
- `list_recent_memories(limit=10)` - Get recent memories

#### Conversation Tracking
- `store_conversation(role, content, metadata=None)` - Store conversation turn
- `search_conversations(query, limit=10)` - Search conversation history
- `get_conversation_history(limit=100)` - Get recent conversations

#### MCP Tool Calls
- `log_tool_call(tool_name, arguments, result, metadata=None)` - Log MCP tool usage
- `get_tool_call_history(tool_name=None, limit=100)` - Get tool usage history
- `reflect_on_tool_usage()` - AI self-reflection on tool patterns

#### System Health
- `get_system_health()` - Check system status and database health

## üõ†Ô∏è Development

### Setting up for Development
```bash
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory
pip install -e ".[dev]"
```

### Running Tests
```bash
pytest tests/
```

## ü§ù Contributing

We welcome contributions! This system is designed to be:

- **Modular**: Easy to extend with new memory types
- **Platform-agnostic**: Works with any AI assistant that supports MCP
- **Scalable**: Handles large conversation histories efficiently

## üìã Roadmap

- [ ] **Semantic Tagging Assistant** - AI-powered memory categorization
- [ ] **Memory Summarization** - Automatic TL;DR for long conversations  
- [ ] **Deferred Retry Queue** - Resilient file import with retry logic
- [ ] **Memory Reflection Engine** - Meta-insights from memory patterns
- [ ] **Export/Import Tools** - Backup and migration utilities

## üåü Community Requests & Platform Support

**Recently Added Platforms** (Based on Reddit Community Feedback & Local Storage Verification):
- ‚úÖ **SillyTavern** - AI character chat interface with conversation logging
- ‚úÖ **Gemini CLI** - Google's Gemini command line interface support  
- ‚úÖ **Open WebUI** - Local web-based LLM interface (multiple install locations)
- ‚ùå **ChatGPT & Claude Desktop** - Removed after verification (cloud-only, no local storage)

**Total Platform Support**: **9 Chat Platforms** (Based on Verified Local Storage)
- ‚úÖ **LM Studio** - Local conversations in JSON format  
- ‚úÖ **Ollama** - SQLite database with chat history
- ‚úÖ **VS Code Copilot** - Development conversation tracking
- ‚úÖ **Open WebUI** - SQLite database with conversation storage  
- ‚úÖ **Text Generation WebUI** - Local chat logs and history
- ‚úÖ **SillyTavern** - AI character chat interface with conversation logging
- ‚úÖ **Gemini CLI** - Google's Gemini command line interface support
- ‚úÖ **Jan AI** - Local AI assistant with conversation storage
- ‚úÖ **Perplexity** - Local conversation tracking (where applicable)

**‚ùå Cloud-Only Applications** (No Local Storage - Removed After Verification):
- ‚ùå **ChatGPT Desktop** - Cloud-only, no local conversation storage
- ‚ùå **Claude Desktop** - Cloud-only, no local conversation files
- ‚ùå **Perplexity Web** - Cloud-based, no local storage

**Upcoming Community Requests**:
- [ ] **GraphDB Integration** - Graph database support for relationship mapping (community requested)
- [ ] **Discord Bot Integration** - Chat logging for Discord AI bots
- [ ] **Telegram Bot Support** - Conversation tracking for Telegram bots
- [ ] **API Standardization** - Universal chat format for easier platform integration

*Have a platform request? Open an issue or submit a PR - all contributors get credited!*

## üìÑ License

MIT License - feel free to use this in your own AI projects!

## ÔøΩ Contributors

This project is the result of a collaborative effort between humans and AI assistants:

- **@savantskie** - Project vision, architecture design, and testing
- **GitHub Copilot** - Core implementation, database design, MCP server development, and tool call logging system
- **ChatGPT** - Initial concept development, feature recommendations, and architectural guidance over 3 months of development

## ÔøΩüôè Acknowledgments

This project represents a unique collaboration between human creativity and AI assistance. After 3 months of conceptual development with ChatGPT and intensive implementation with GitHub Copilot, we've created something that could genuinely change how AI assistants maintain memory and context.

**Special thanks to:**
- **ChatGPT** for the original insight that "*If this ever becomes open source? It'll become the standard.*"
- **GitHub Copilot** for the breakthrough implementation that solved foreign key constraints and made real-time conversation capture work flawlessly
- **The open source community** for inspiring us to share this foundational technology

Built with determination, debugged with patience, and designed for the future of AI assistance.

**GITHUB LINK** - https://github.com/savantskie/persistent-ai-memory.git
---

**‚≠ê If this project helps you build better AI assistants, please give it a star!**
[file content end]

/soulforge/persistent-ai-memory-new/REDDIT_QUICKSTART.md
[file content begin]
# Quick Start for Reddit Users üëã

Welcome! Someone shared this on Reddit and you want to try it? Here's the fastest way to get started.

## üéØ What This Does

This gives your AI assistant (ChatGPT, Claude, etc.) a **persistent memory** that:
- Remembers conversations across sessions
- Learns from your interactions
- Provides semantic search through your chat history
- Works with multiple AI platforms

## ‚ö° Super Quick Setup (Choose One)

### If you're on Windows:
```cmd
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.bat -o install.bat && install.bat
```

### If you're on Mac/Linux:
```bash
curl -sSL https://raw.githubusercontent.com/savantskie/persistent-ai-memory/main/install.sh | bash
```

### Don't have curl? Manual way:
1. Download the code: [Click here](https://github.com/savantskie/persistent-ai-memory/archive/main.zip)
2. Extract the zip file
3. Open terminal/command prompt in that folder
4. Run: `pip install -e .`

## üß™ Test It Works

After installation, run this:
```bash
python tests/test_health_check.py
```

You should see "‚úÖ System health check passed!"

## üéÆ Try It Out

Create a file called `test_memory.py`:

```python
import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def demo():
    memory = PersistentAIMemorySystem()
    
    # Store some memories
    await memory.store_memory("I love Python programming")
    await memory.store_memory("I'm learning about AI memory systems")
    await memory.store_memory("Reddit has great tech communities")
    
    # Search memories
    results = await memory.search_memories("programming")
    print(f"Found {len(results)} memories about programming:")
    for result in results:
        print(f"- {result['content']}")

# Run the demo
asyncio.run(demo())
```

Run it: `python test_memory.py`

## ü§ñ Use with Your AI Assistant

### For Claude Desktop:
Add this to your Claude Desktop config to give Claude access to the memory system.

### For ChatGPT:
The system can monitor your ChatGPT export files automatically.

### For LM Studio:
Works out of the box if you have LM Studio running locally.

## üÜò Something Broke?

1. **Python not found?** Install Python 3.8+ from python.org
2. **Permission errors?** Try running as administrator (Windows) or with `sudo` (Mac/Linux)
3. **Still stuck?** Open an issue on GitHub with your error message

## üí¨ Questions?

- Check the [full README](README.md) for detailed docs
- Look at [examples](examples/) for more use cases
- Open a GitHub issue if you're stuck

## üåü Like It?

If this helps you, star the repo! ‚≠ê

It helps other people find the project.

---

*Built by humans and AI working together. Welcome to the future of persistent AI memory!* üöÄ
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/README.md
[file content begin]
# AI Memory LLM Integration

## Overview
The AI Memory LLM Integration project provides a comprehensive system for managing persistent AI memory, designed to work seamlessly with various large language models (LLMs). This system allows for efficient storage, retrieval, and management of conversational data, insights, and memories, enhancing the capabilities of AI assistants.

## Features
- **Persistent Memory Management**: Store and retrieve memories with importance levels and tags.
- **Conversation Handling**: Automatically manage conversations and sessions.
- **Integration with LLMs**: Easily integrate with any LLM for enhanced functionality.
- **Advanced Search Capabilities**: Utilize vector-based semantic search for efficient data retrieval.
- **Real-time Monitoring**: Monitor conversation files and manage data in real-time.

## Installation
To install the AI Memory LLM Integration system, follow these steps:

1. Clone the repository:
   ```
   git clone <repository-url>
   cd ai-memory-llm-integration
   ```

2. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Run the setup script:
   ```
   python setup.py install
   ```

## Usage
After installation, you can start using the AI Memory system by importing the necessary classes from the `src` package. Here‚Äôs a quick example:

```python
from src.ai_memory_core import PersistentAIMemorySystem

# Initialize the memory system
memory_system = PersistentAIMemorySystem()

# Create a new memory
memory_id = await memory_system.create_memory("This is a test memory.")
print(f"Memory created with ID: {memory_id}")
```

## Documentation
For detailed instructions on integrating the AI memory system with various LLMs, please refer to the [Integration Guide](docs/integration_guide.md). 

## Contributing
Contributions are welcome! Please submit a pull request or open an issue for any enhancements or bug fixes.

## License
This project is licensed under the MIT License. See the LICENSE file for more details.
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/docs/integration_guide.md
[file content begin]
# Integration Guide for AI Memory System with LLMs

## Overview

This integration guide provides detailed instructions on how to integrate the Persistent AI Memory System with various Large Language Models (LLMs). The goal is to enable seamless interaction between the memory system and LLMs, allowing for enhanced capabilities in managing conversations, storing memories, and retrieving information.

## Prerequisites

Before you begin, ensure you have the following:

- Python 3.7 or higher
- Access to an LLM API (e.g., OpenAI, Hugging Face)
- The AI Memory System installed and configured

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/ai-memory-llm-integration.git
   cd ai-memory-llm-integration
   ```

2. Install the required dependencies:

   ```bash
   pip install -r requirements.txt
   ```

## Configuration

1. **API Keys**: Obtain your API keys from the LLM provider and store them securely. You may want to use environment variables or a configuration file to manage sensitive information.

2. **Modify the Core Module**: In `src/ai_memory_core.py`, ensure that the integration with the LLM is properly set up. You may need to add methods for sending and receiving messages from the LLM.

## Example Integration

Here‚Äôs a basic example of how to use the AI Memory System with an LLM:

```python
from src.ai_memory_core import PersistentAIMemorySystem

# Initialize the memory system
memory_system = PersistentAIMemorySystem()

# Function to interact with the LLM
def interact_with_llm(prompt):
    # Send the prompt to the LLM and get a response
    response = llm_api.send_prompt(prompt)  # Replace with actual LLM API call
    return response

# Example usage
user_input = "What is the capital of France?"
llm_response = interact_with_llm(user_input)

# Store the conversation in memory
memory_system.store_conversation(content=user_input, role='user')
memory_system.store_conversation(content=llm_response, role='assistant')
```

## Conclusion

Integrating the AI Memory System with LLMs opens up new possibilities for managing and utilizing conversational data. Follow the steps outlined in this guide to set up your integration and start leveraging the power of LLMs in your applications. For further assistance, refer to the README.md and the source code documentation.
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/requirements.txt
[file content begin]
watchdog
sqlite3
asyncio
json
uuid
zoneinfo
```
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/setup.py
[file content begin]
from setuptools import setup, find_packages

setup(
    name='ai-memory-llm-integration',
    version='0.1.0',
    author='Your Name',
    author_email='your.email@example.com',
    description='Integration of Persistent AI Memory System with LLMs',
    packages=find_packages(where='src'),
    package_dir={'': 'src'},
    install_requires=[
        'watchdog',
        'sqlite3',  # Add any other dependencies required for your project
        # Include any additional libraries needed for LLM integration
    ],
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
)
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/src/__init__.py
[file content begin]
# This file is intentionally left blank.
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-llm-integration/src/ai_memory_core.py
[file content begin]
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta
from pathlib import Path
import sqlite3
import json
import uuid
import logging

# Configure logging with minimal output
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)

class DatabaseManager:
    """Base database manager for common operations"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.ensure_database_exists()
    
    def ensure_database_exists(self):
        """Ensure the database file and directory exist"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
    def get_connection(self) -> sqlite3.Connection:
        """Get a database connection with proper configuration"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # Enable dict-like access
        conn.execute("PRAGMA foreign_keys = ON")  # Enable foreign key constraints
        return conn
    
    async def execute_query(self, query: str, params: Tuple = ()) -> List[sqlite3.Row]:
        """Execute a SELECT query and return results"""
        with self.get_connection() as conn:
            cursor = conn.execute(query, params)
            return cursor.fetchall()
    
    async def execute_update(self, query: str, params: Tuple = ()) -> str:
        """Execute an INSERT/UPDATE/DELETE query and return last row ID"""
        with self.get_connection() as conn:
            cursor = conn.execute(query, params)
            conn.commit()
            return cursor.lastrowid

class MCPToolCallDatabase(DatabaseManager):
    """Tracks all MCP tool calls for reflection and debugging"""
    
    def __init__(self, db_path: str = "mcp_tool_calls.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tool call tracking tables"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_calls (
                    call_id TEXT PRIMARY KEY,
                    timestamp TEXT,
                    client_id TEXT,
                    tool_name TEXT,
                    parameters TEXT,
                    result TEXT,
                    status TEXT,
                    execution_time_ms INTEGER,
                    error_message TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_usage_stats (
                    tool_name TEXT,
                    date TEXT,
                    call_count INTEGER,
                    success_count INTEGER,
                    failure_count INTEGER,
                    PRIMARY KEY (tool_name, date)
                )
            """)

    async def log_tool_call(self, tool_name: str, parameters: Dict, result: Any = None, 
                           status: str = "success", execution_time_ms: float = None,
                           error_message: str = None, client_id: str = None) -> str:
        """Log a tool call with all relevant details"""
        
        call_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()
        
        await self.execute_update(
            """INSERT INTO tool_calls 
               (call_id, timestamp, client_id, tool_name, parameters, result, 
                status, execution_time_ms, error_message) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (call_id, timestamp, client_id, tool_name, 
             json.dumps(parameters), json.dumps(result) if result else None,
             status, int(execution_time_ms) if execution_time_ms else None, error_message)
        )
        
        await self._update_tool_stats(tool_name, status, execution_time_ms)
        
        return call_id
    
    async def _update_tool_stats(self, tool_name: str, status: str, execution_time_ms: float):
        """Update daily usage statistics for a tool"""
        today = datetime.now().date().isoformat()
        
        existing = await self.execute_query(
            "SELECT * FROM tool_usage_stats WHERE tool_name = ? AND date = ?",
            (tool_name, today)
        )
        
        if existing:
            await self.execute_update(
                """UPDATE tool_usage_stats 
                   SET call_count = call_count + 1,
                       success_count = success_count + (CASE WHEN ? = 'success' THEN 1 ELSE 0 END),
                       failure_count = failure_count + (CASE WHEN ? = 'failure' THEN 1 ELSE 0 END)
                   WHERE tool_name = ? AND date = ?""",
                (status, status, tool_name, today)
            )
        else:
            await self.execute_update(
                """INSERT INTO tool_usage_stats (tool_name, date, call_count, success_count, failure_count) 
                   VALUES (?, ?, 1, ?, ?)""",
                (tool_name, today, 1 if status == 'success' else 0, 1 if status == 'failure' else 0)
            )

class ConversationDatabase(DatabaseManager):
    """Manages conversation auto-save database"""
    
    def __init__(self, db_path: str = "conversations.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS messages (
                    message_id TEXT PRIMARY KEY,
                    conversation_id TEXT,
                    timestamp TEXT,
                    role TEXT,
                    content TEXT,
                    metadata TEXT
                )
            """)

    async def store_message(self, content: str, role: str, session_id: str = None, 
                          conversation_id: str = None, metadata: Dict = None) -> Dict[str, str]:
        """Store a message and auto-manage sessions/conversations with duplicate detection"""
        timestamp = datetime.now().isoformat()
        message_id = str(uuid.uuid4())

        await self.execute_update(
            """INSERT INTO messages 
               (message_id, conversation_id, timestamp, role, content, metadata) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (message_id, conversation_id, timestamp, role, content, 
             json.dumps(metadata) if metadata else None)
        )

        return {
            "message_id": message_id,
            "conversation_id": conversation_id,
            "session_id": session_id
        }

class AIMemoryDatabase(DatabaseManager):
    """Manages AI-curated memories database with enhanced operations"""
    
    def __init__(self, db_path: str = "ai_memories.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS curated_memories (
                    memory_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    timestamp_updated TEXT,
                    source_conversation_id TEXT,
                    memory_type TEXT,
                    content TEXT,
                    importance_level INTEGER,
                    tags TEXT
                )
            """)

    async def create_memory(self, content: str, memory_type: str = None, 
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> str:
        """Create a new curated memory"""
        memory_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()

        await self.execute_update(
            """INSERT INTO curated_memories 
               (memory_id, timestamp_created, timestamp_updated, source_conversation_id, 
                memory_type, content, importance_level, tags) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (memory_id, timestamp, timestamp, source_conversation_id, 
             memory_type, content, importance_level, 
             json.dumps(tags) if tags else None)
        )
        
        return memory_id

class ScheduleDatabase(DatabaseManager):
    """Manages appointments and reminders database"""
    
    def __init__(self, db_path: str = "schedule.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS appointments (
                    appointment_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    scheduled_datetime TEXT,
                    title TEXT,
                    description TEXT,
                    location TEXT,
                    source_conversation_id TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS reminders (
                    reminder_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    due_datetime TEXT,
                    content TEXT,
                    priority_level INTEGER,
                    source_conversation_id TEXT
                )
            """)

    async def create_appointment(self, title: str, scheduled_datetime: str, 
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> str:
        """Create a new appointment"""
        appointment_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()

        await self.execute_update(
            """INSERT INTO appointments 
               (appointment_id, timestamp_created, scheduled_datetime, title, description, location, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (appointment_id, timestamp, scheduled_datetime, title, description, location, source_conversation_id)
        )
        return appointment_id

class VSCodeProjectDatabase(DatabaseManager):
    """Manages VS Code project context and development sessions"""
    
    def __init__(self, db_path: str = "vscode_project.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS project_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_timestamp TEXT,
                    workspace_path TEXT,
                    active_files TEXT,
                    git_branch TEXT,
                    session_summary TEXT
                )
            """)

    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> str:
        """Save a development session"""
        
        session_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()
        
        await self.execute_update(
            """INSERT INTO project_sessions 
               (session_id, start_timestamp, workspace_path, active_files, git_branch, session_summary) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (session_id, timestamp, workspace_path, 
             json.dumps(active_files) if active_files else None,
             git_branch, session_summary)
        )
        
        return session_id

# Additional classes and methods for integration with LLMs would be added here.
[file content end]

/soulforge/persistent-ai-memory-new/ai-memory-mcp_server.py
[file content begin]
#!/usr/bin/env python3
"""
Persistent AI Memory System - MCP Server

Acts as an interface layer between MCP clients (VS Code, LM Studio, Ollama UIs)
and the AI Memory System. Provides standardized tools for memory operations
while maintaining client-specific access controls.
"""

print("AI Memory MCP Server starting...")  # This will show up in stdout immediately

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, timezone
import time
import warnings
import os
# MCP imports
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    CallToolRequestParams,
    CallToolResult,
    TextContent,
    Tool,
)

# Local imports (will be implemented)
from ai_memory_core import PersistentAIMemorySystem

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AIMemoryMCPServer:
    """MCP Server for Friday's Memory System"""

    async def update_memory(self, memory_id: str, content: str = None, importance_level: int = None, tags: List[str] = None) -> Dict:
        """Update an existing memory"""
        
        success = await self.ai_memory_db.update_memory(memory_id, content, importance_level, tags)
        
        # If content was updated, regenerate embedding
        if content is not None:
            asyncio.create_task(self._add_embedding_to_memory(memory_id, content))
        
        return {
            "status": "success" if success else "error",
            "memory_id": memory_id
        }   
        
    def __init__(self):
        self.memory_system = PersistentAIMemorySystem()
        self.server = Server("ai-memory")
        self.client_context = {}  # Track client-specific context
        self._maintenance_task = None  # Background maintenance task
        
        # Enable debug logging for MCP server
        logging.getLogger("mcp.server").setLevel(logging.DEBUG)
        
        # Register MCP handlers
        self._register_handlers()
        
        # Start automatic maintenance
        self._start_automatic_maintenance()
        
        logger.info("AIMemoryMCPServer initialized successfully")
    
    def _register_handlers(self):
        """Register MCP server handlers"""
        
        @self.server.list_tools()
        async def handle_list_tools() -> List[Tool]:
            """List available tools based on client context"""
            return await self._get_client_tools()
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> CallToolResult:
            """Execute tool based on client and parameters"""
            return await self._execute_tool(name, arguments or {})
    
    async def _get_client_tools(self) -> List[Tool]:
        """Return tools available to the current client"""
        logger.debug("Getting client tools")
        
        # Detect client type based on user agent or connection context
        client_type = self._detect_client_type()
        logger.info(f"Detected client type: {client_type}")
        
        try:
            # Common tools available to all clients
            common_tools = [
            Tool(
                name="search_memories",
                description="Search memories using semantic similarity with importance and type filtering",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "limit": {"type": "integer", "description": "Max results", "default": 10},
                        "database_filter": {"type": "string", "description": "Filter by database type", "enum": ["conversations", "ai_memories", "schedule", "all"], "default": "all"},
                        "min_importance": {"type": "integer", "minimum": 1, "maximum": 10, "description": "Minimum importance level to include (1-10)"},
                        "max_importance": {"type": "integer", "minimum": 1, "maximum": 10, "description": "Maximum importance level to include (1-10)"},
                        "memory_type": {"type": "string", "description": "Filter by memory type (e.g., 'safety', 'preference', 'skill', 'general')"}
                    },
                    "required": ["query"]
                }
            ),
            Tool(
                name="store_conversation",
                description="Store conversation automatically",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Conversation content"},
                        "role": {"type": "string", "description": "Role (user/assistant)"},
                        "session_id": {"type": "string", "description": "Session identifier"},
                        "metadata": {"type": "object", "description": "Additional metadata"}
                    },
                    "required": ["content", "role"]
                }
            ),
            Tool(
                name="create_memory",
                description="Create a curated memory entry",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Memory content"},
                        "memory_type": {"type": "string", "description": "Type of memory"},
                        "importance_level": {"type": "integer", "description": "Importance (1-10)", "default": 5},
                        "tags": {"type": "array", "items": {"type": "string"}, "description": "Memory tags"},
                        "source_conversation_id": {"type": "string", "description": "Source conversation ID"}
                    },
                    "required": ["content"]
                }
            ),
            Tool(
                name="update_memory",
                description="Update an existing curated memory",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "memory_id": {"type": "string", "description": "Memory ID to update"},
                        "content": {"type": "string", "description": "Updated content"},
                        "importance_level": {"type": "integer", "description": "Updated importance"},
                        "tags": {"type": "array", "items": {"type": "string"}, "description": "Updated tags"}
                    },
                    "required": ["memory_id"]
                }
            ),
            Tool(
                name="create_appointment",
                description="Create an appointment",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "title": {"type": "string", "description": "Appointment title"},
                        "description": {"type": "string", "description": "Appointment description"},
                        "scheduled_datetime": {"type": "string", "description": "ISO format datetime"},
                        "location": {"type": "string", "description": "Location"}
                    },
                    "required": ["title", "scheduled_datetime"]
                }
            ),
            Tool(
                name="create_reminder",
                description="Create a reminder",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Reminder content"},
                        "due_datetime": {"type": "string", "description": "ISO format datetime"},
                        "priority_level": {"type": "integer", "description": "Priority (1-10)", "default": 5}
                    },
                    "required": ["content", "due_datetime"]
                }
            ),
            Tool(
                name="get_reminders",
                description="Get up to 5 active (uncompleted) reminders, sorted by due date.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number of reminders to return", "default": 5}
                    }
                }
            ),
            Tool(
                name="get_recent_context",
                description="Get recent conversation context",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number of recent items", "default": 5},
                        "session_id": {"type": "string", "description": "Specific session ID"}
                    }
                }
            ),
            Tool(
                name="get_system_health",
                description="Get comprehensive system health, statistics, and database status",
                inputSchema={
                    "type": "object",
                    "properties": {},
                    "additionalProperties": False
                }
            ),
            Tool(
                name="get_tool_usage_summary",
                description="Get AI tool usage summary and insights for self-reflection",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "days": {"type": "integer", "description": "Days to analyze", "default": 7},
                        "client_id": {"type": "string", "description": "Specific client ID to analyze"}
                    }
                }
            ),
            Tool(
                name="reflect_on_tool_usage",
                description="AI self-reflection on tool usage patterns and effectiveness",
                inputSchema={
                    "type": "object", 
                    "properties": {
                        "days": {"type": "integer", "description": "Days to analyze", "default": 7},
                        "client_id": {"type": "string", "description": "Specific client ID to analyze"}
                    }
                }
            ),
            Tool(
                name="get_ai_insights",
                description="Get recent AI self-reflection insights and patterns",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number of insights", "default": 5},
                        "insight_type": {"type": "string", "description": "Type of insight to filter"}
                    }
                }
            ),
            Tool(
                name="get_active_reminders",
                description="Get active (not completed) reminders",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number of reminders to return", "default": 10},
                        "days_ahead": {"type": "integer", "description": "Only show reminders due within X days", "default": 30}
                    }
                }
            ),
            Tool(
                name="get_completed_reminders",
                description="Get recently completed reminders",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "days": {"type": "integer", "description": "Look back X days", "default": 7}
                    }
                }
            ),
            Tool(
                name="complete_reminder",
                description="Mark a reminder as completed",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "reminder_id": {"type": "string", "description": "ID of the reminder to complete"}
                    },
                    "required": ["reminder_id"]
                }
            ),
            Tool(
                name="reschedule_reminder",
                description="Update the due date of a reminder",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "reminder_id": {"type": "string", "description": "ID of the reminder"},
                        "new_due_datetime": {"type": "string", "description": "New ISO datetime (e.g., 2025-08-03T14:00:00Z)"}
                    },
                    "required": ["reminder_id", "new_due_datetime"]
                }
            ),
            Tool(
                name="delete_reminder",
                description="Permanently delete a reminder",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "reminder_id": {"type": "string", "description": "ID of the reminder to delete"}
                    },
                    "required": ["reminder_id"]
                }
            ),
            Tool(
                name="cancel_appointment",
                description="Cancel a scheduled appointment",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "appointment_id": {"type": "string", "description": "ID of the appointment to cancel"}
                    },
                    "required": ["appointment_id"]
                }
            ),
            Tool(
                name="get_upcoming_appointments",
                description="Get upcoming appointments (not cancelled)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number to return", "default": 5},
                        "days_ahead": {"type": "integer", "description": "Only show within X days", "default": 30}
                    }
                }
            ),
            Tool(
                name="get_appointments",
                description="Get recent appointments, optionally filtered by date range",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {"type": "integer", "description": "Number of appointments to return", "default": 5},
                        "days_ahead": {"type": "integer", "description": "Only show appointments scheduled within X days", "default": 30}
                    }
                }
            ),
            Tool(
                name="store_ai_reflection",
                description="Store an AI self-reflection/insight record (manual write)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Freeform write-up of the reflection"},
                        "reflection_type": {"type": "string", "description": "Category (e.g., tool_usage_analysis, memory, general)", "default": "general"},
                        "insights": {"type": "array", "items": {"type": "string"}, "description": "Bullet insights derived from the analysis"},
                        "recommendations": {"type": "array", "items": {"type": "string"}, "description": "Recommended next actions"},
                        "confidence_level": {"type": "number", "description": "Confidence 0.0‚Äì1.0", "default": 0.7},
                        "source_period_days": {"type": "integer", "description": "Days of data this reflection summarizes"}
                    },
                    "required": ["content"],
                    "additionalProperties": False
                }
            ),
            Tool(
                name="write_ai_insights",
                description="Alias of store_ai_reflection ‚Äì write an AI self-reflection/insight record",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Freeform write-up of the reflection"},
                        "reflection_type": {"type": "string", "description": "Category (e.g., tool_usage_analysis, memory, general)", "default": "general"},
                        "insights": {"type": "array", "items": {"type": "string"}, "description": "Bullet insights derived from the analysis"},
                        "recommendations": {"type": "array", "items": {"type": "string"}, "description": "Recommended next actions"},
                        "confidence_level": {"type": "number", "description": "Confidence 0.0‚Äì1.0", "default": 0.7},
                        "source_period_days": {"type": "integer", "description": "Days of data this reflection summarizes"}
                    },
                    "required": ["content"],
                    "additionalProperties": False
                }
            ),
            Tool(
                name="get_current_time",
                description="Get the current server time in ISO format (UTC and local)",
                inputSchema={
                    "type": "object",
                    "properties": {},
                    "additionalProperties": False
                }
            ),
            Tool(
                name="get_weather_open_meteo",
                description="Open-Meteo forecast (no API key). Defaults to Motley, MN and caches once per local day.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "latitude": {"type": ["number", "null"], "description": "Ignored unless override=True"},
                        "longitude": {"type": ["number", "null"], "description": "Ignored unless override=True"},
                        "timezone_str": {"type": ["string", "null"], "description": "Ignored unless override=True"},
                        "force_refresh": {"type": "boolean", "description": "Ignore same-day cache", "default": False},
                        "return_changes_only": {"type": "boolean", "description": "If true, return only a summary of changed fields for today.", "default": False},
                        "update_today": {"type": "boolean", "description": "If true (default), fetch and merge changes into today's file before returning.", "default": True},
                        "severe_update": {"type": "boolean", "description": "If true, shrink the update window to 30 minutes for severe weather.", "default": False}
                    }
                }
            )
        ]
        except Exception as e:
            logger.error(f"Error creating common tools: {e}")
            common_tools = []
        
        # VS Code specific tools
        vscode_tools = [
            Tool(
                name="save_development_session",
                description="Save VS Code development session context",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "workspace_path": {"type": "string", "description": "Workspace path"},
                        "active_files": {"type": "array", "items": {"type": "string"}, "description": "Active files"},
                        "git_branch": {"type": "string", "description": "Current git branch"},
                        "session_summary": {"type": "string", "description": "Session summary"}
                    },
                    "required": ["workspace_path"]
                }
            ),
            Tool(
                name="store_project_insight",
                description="Store development insight or decision",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "insight_type": {"type": "string", "description": "Type of insight"},
                        "content": {"type": "string", "description": "Insight content"},
                        "related_files": {"type": "array", "items": {"type": "string"}, "description": "Related files"},
                        "importance_level": {"type": "integer", "description": "Importance (1-10)", "default": 5}
                    },
                    "required": ["content"]
                }
            ),
            Tool(
                name="search_project_history",
                description="Search VS Code project development history",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "limit": {"type": "integer", "description": "Max results", "default": 10}
                    },
                    "required": ["query"]
                }
            ),
            Tool(
                name="link_code_context",
                description="Link conversation to specific code context",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "File path"},
                        "function_name": {"type": "string", "description": "Function name"},
                        "description": {"type": "string", "description": "Context description"},
                        "conversation_id": {"type": "string", "description": "Related conversation ID"}
                    },
                    "required": ["file_path", "description"]
                }
            ),
            Tool(
                name="get_project_continuity",
                description="Get context to continue development work",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "workspace_path": {"type": "string", "description": "Workspace path"},
                        "limit": {"type": "integer", "description": "Context items", "default": 5}
                    }
                }
            )
        ]
        
        try:
            # Return appropriate tools based on client type
            if client_type == "sillytavern":
                # SillyTavern gets memory tools + character/roleplay specific tools
                sillytavern_tools = [
                    Tool(
                        name="get_character_context",
                        description="Get relevant context about characters from memory",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "character_name": {"type": "string", "description": "Character name to search for"},
                                "context_type": {"type": "string", "description": "Type of context (personality, relationships, history)"},
                                "limit": {"type": "integer", "description": "Max results", "default": 5}
                            },
                            "required": ["character_name"]
                        }
                    ),
                    Tool(
                        name="store_roleplay_memory",
                        description="Store important roleplay moments or character developments",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "character_name": {"type": "string", "description": "Character involved"},
                                "event_description": {"type": "string", "description": "What happened"},
                                "importance_level": {"type": "integer", "description": "Importance (1-10)", "default": 5},
                                "tags": {"type": "array", "items": {"type": "string"}, "description": "Relevant tags"}
                            },
                            "required": ["character_name", "event_description"]
                        }
                    ),
                    Tool(
                        name="search_roleplay_history",
                        description="Search past roleplay interactions and character development",
                        inputSchema={
                            "type": "object",
                            "properties": {
                                "query": {"type": "string", "description": "Search query"},
                                "character_name": {"type": "string", "description": "Focus on specific character"},
                                "limit": {"type": "integer", "description": "Max results", "default": 10}
                            },
                            "required": ["query"]
                        }
                    )
                ]
                return common_tools + sillytavern_tools
            
            elif client_type == "vscode":
                # VS Code gets development-specific tools
                return common_tools + vscode_tools
            
            else:
                # Default: LM Studio, Ollama UIs, etc. get core memory tools only
                return common_tools
                
        except Exception as e:
            logger.error(f"Error combining tool lists: {e}")
            return []

    def _detect_client_type(self) -> str:
        """Detect the type of MCP client connecting"""
        # Detect the type of MCP client connecting
        client_type = "unknown"
        if "VS Code" in os.getenv("USER_AGENT", ""):
            client_type = "vscode"
        elif "LM Studio" in os.getenv("USER_AGENT", ""):
            client_type = "lm_studio"
        elif "Silly Tavern" in os.getenv("USER_AGENT", ""):
            client_type = "sillytavern"
        elif "Ollama" in os.getenv("USER_AGENT", ""):
            client_type = "ollama"
        logger.info(f"Detected client type: {client_type}")
        return client_type
    
    async def _execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> CallToolResult:
        """Execute the requested tool with logging for AI self-reflection"""
        
        import time
        
        # Start timing and get client info
        start_time = time.perf_counter()
        client_id = self.client_context.get("current_client", "unknown")
        
        try:
            logger.info(f"Executing tool: {tool_name} with arguments: {arguments}")
            # Route to appropriate handler
            if tool_name == "search_memories":
                result = await self.memory_system.search_memories(**arguments)
            elif tool_name == "store_conversation":
                result = await self.memory_system.store_conversation(**arguments)
            elif tool_name == "create_memory":
                result = await self.memory_system.create_memory(**arguments)
            elif tool_name == "update_memory":
                result = await self.memory_system.update_memory(**arguments)
            elif tool_name == "create_appointment":
                result = await self.memory_system.create_appointment(**arguments)
            elif tool_name == "create_reminder":
                result = await self.memory_system.create_reminder(**arguments)
            elif tool_name == "get_reminders":
                limit = arguments.get("limit", 5)
                reminders = await self.memory_system.get_active_reminders()
                result = reminders[:limit] if reminders else []
            elif tool_name == "get_recent_context":
                result = await self.memory_system.get_recent_context(**arguments)
            elif tool_name == "get_system_health":
                result = await self.memory_system.get_system_health()
            elif tool_name == "save_development_session":
                result = await self.memory_system.save_development_session(**arguments)
            elif tool_name == "store_project_insight":
                result = await self.memory_system.store_project_insight(**arguments)
            elif tool_name == "search_project_history":
                result = await self.memory_system.search_project_history(**arguments)
            elif tool_name == "link_code_context":
                result = await self.memory_system.link_code_context(**arguments)
            elif tool_name == "get_project_continuity":
                result = await self.memory_system.get_project_continuity(**arguments)
            elif tool_name == "get_tool_usage_summary":
                result = await self.memory_system.get_tool_usage_summary(**arguments)
            elif tool_name == "reflect_on_tool_usage":
                result = await self.memory_system.reflect_on_tool_usage(**arguments)
            elif tool_name == "get_ai_insights":
                result = await self.memory_system.get_ai_insights(**arguments)
            elif tool_name == "get_active_reminders":
                result = await self.memory_system.get_active_reminders(**arguments)
            elif tool_name == "get_completed_reminders":
                result = await self.memory_system.get_completed_reminders(**arguments)
            elif tool_name == "complete_reminder":
                result = await self.memory_system.complete_reminder(**arguments)
            elif tool_name == "reschedule_reminder":
                result = await self.memory_system.reschedule_reminder(**arguments)
            elif tool_name == "delete_reminder":
                result = await self.memory_system.delete_reminder(**arguments)
            elif tool_name == "cancel_appointment":
                result = await self.memory_system.cancel_appointment(**arguments)
            elif tool_name == "get_upcoming_appointments":
                result = await self.memory_system.get_upcoming_appointments(**arguments)
            elif tool_name == "get_appointments":
                result = await self.memory_system.get_appointments(**arguments)
            elif tool_name == "store_ai_reflection" or tool_name == "write_ai_insights":
                result = await self.memory_system.store_ai_reflection(**arguments)
            elif tool_name == "get_current_time":
                result = await self.memory_system.get_current_time()
            elif tool_name == "get_weather_open_meteo":
                result = await self.memory_system.get_weather_open_meteo(**arguments)
            # SillyTavern-specific tools
            elif tool_name == "get_character_context":
                result = await self.memory_system.get_character_context(**arguments)
            elif tool_name == "store_roleplay_memory":
                result = await self.memory_system.store_roleplay_memory(**arguments)
            elif tool_name == "search_roleplay_history":
                result = await self.memory_system.search_roleplay_history(**arguments)
            else:
                raise ValueError(f"Unknown tool: {tool_name}")
            
            # Calculate execution time and log successful call
            end_time = time.perf_counter()
            execution_time_ms = (end_time - start_time) * 1000
            
            # Log tool call for AI self-reflection (async, don't wait)
            asyncio.create_task(self.memory_system.log_tool_call(
                client_id=client_id,
                tool_name=tool_name,
                parameters=arguments,
                execution_time_ms=execution_time_ms,
                status="success",
                result=result
            ))
            
            # Format the result as a proper TextContent object
            if isinstance(result, (dict, list)):
                result_text = json.dumps(result, indent=2, default=str)
            else:
                result_text = str(result)
            
            text_content = {
                "type": "text",
                "text": result_text,
                "highlights": None,
                "meta": None
            }
            
            return {
                "content": [text_content],
                "success": True,
                "structuredContent": None,
                "isError": False,
                "meta": None
            }
            
        except Exception as e:
            # Calculate execution time and log failed call
            end_time = time.perf_counter()
            execution_time_ms = (end_time - start_time) * 1000
            
            # Log tool call failure for AI self-reflection (async, don't wait)
            asyncio.create_task(self.memory_system.log_tool_call(
                client_id=client_id,
                tool_name=tool_name,
                parameters=arguments,
                execution_time_ms=execution_time_ms,
                status="error",
                error_message=str(e)
            ))
            
            logger.error(f"Error executing tool {tool_name}: {e}")
            return {
                "content": [{
                    "type": "text",
                    "text": f"Error: {str(e)}",
                    "highlights": None,
                    "meta": None
                }],
                "success": False,
                "structuredContent": None,
                "isError": True,
                "meta": None
            }
    
    def _start_automatic_maintenance(self):
        """Start automatic database maintenance background task"""
        try:
            loop = asyncio.get_running_loop()
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            self._maintenance_task = loop.create_task(self._maintenance_loop())

        except RuntimeError:
            logger.warning("Event loop not running. Call `_start_automatic_maintenance()` after loop starts.")
        logger.info("üîß Automatic database maintenance started")
    
    async def _maintenance_loop(self):
        """Background loop for automatic database maintenance"""
        # Wait a bit after startup before first maintenance
        await asyncio.sleep(300)  # 5 minutes initial delay
        
        while True:
            try:
                logger.info("üßπ Running automatic database maintenance...")
                result = await self.memory_system.run_database_maintenance()
                
                # Log maintenance results
                if result.get("success"):
                    logger.info(f"‚úÖ Automatic maintenance completed - optimized {len(result.get('optimization_results', {}))} databases")
                else:
                    logger.warning(f"‚ö†Ô∏è Automatic maintenance had issues: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.error(f"‚ùå Automatic maintenance failed: {e}")
            
            # Wait 3 hours before next maintenance
            await asyncio.sleep(3 * 60 * 60)
    
    async def cleanup(self):
        """Cleanup resources when server stops"""
        if self._maintenance_task and not self._maintenance_task.done():
            self._maintenance_task.cancel()
            try:
                await self._maintenance_task
            except asyncio.CancelledError:
                pass
            logger.info("üîß Automatic maintenance stopped")
    



async def start_http_server(mcp_server: AIMemoryMCPServer, host: str = "127.0.0.1", port: int = 11434):
    """Start the HTTP API server if needed"""
    try:
        from fastapi import FastAPI, HTTPException
        from fastapi.middleware.cors import CORSMiddleware
        import uvicorn
        
        app = FastAPI(title="Friday Memory API")
        
        # Add CORS middleware
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        @app.get("/api/health")
        async def health_check():
            return {"status": "healthy", "server": "ai-memory"}
            
        # Start server without blocking
        config = uvicorn.Config(app, host=host, port=port, log_level="info")
        server = uvicorn.Server(config)
        return await server.serve()
    except ImportError:
        logger.info("FastAPI not installed - HTTP API disabled")
        return None
    except Exception as e:
        logger.warning(f"Failed to start HTTP server: {e}")
        return None

async def main():
    """Main entry point for the MCP server"""
    logger.info("AI Memory MCP Server starting...")
    
    # Set debug logging for MCP components
    logging.getLogger("mcp").setLevel(logging.DEBUG)
    logging.getLogger("mcp.server").setLevel(logging.DEBUG)
    
    mcp_server = AIMemoryMCPServer()
    
    logger.debug("Server initialized, starting stdio interface for LM Studio...")
    
    try:
        # Only use stdio for LM Studio - no HTTP server needed
        logger.info("Waiting for stdio connection from LM Studio...")
        async with stdio_server() as (read_stream, write_stream):
            logger.info("LM Studio connected via stdio")
            await mcp_server.server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="ai-memory",
                    server_version="1.0.0",
                    capabilities=mcp_server.server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={}
                    )
                )
            )
    except Exception as e:
        logger.error(f"Server error: {e}")
        raise
    finally:
        await mcp_server.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/ai_memory_core.py
[file content begin]
#!/usr/bin/env python3
"""
Persistent AI Memory System - Core Module

A comprehensive memory system designed for long-term persistence, semantic search,
and AI assistant augmentation. This standalone version includes all core functionality
with enhanced features for broader use.

Key Features:
- Specialized Database Architecture:
  * Conversations with automatic session management
  * AI-curated memories with importance levels and tags
  * Appointment and reminder scheduling
  * VS Code project context and development tracking
  * MCP tool call logging with AI self-reflection

- Advanced Search and Retrieval:
  * Vector-based semantic search across all databases
  * Project-specific search capabilities
  * Code context linking and retrieval
  * Importance-weighted memory search
  * Fallback text-based search when embeddings unavailable

- Enhanced AI Capabilities:
  * Automatic embedding generation
  * Usage pattern detection and analysis
  * AI self-reflection on tool usage
  * Pattern-based recommendations
  * Confidence scoring for insights

- Real-time Monitoring:
  * Conversation file monitoring
  * Multiple chat source support (VS Code, LM Studio, ChatGPT, etc.)
  * Deduplication across sources
  * MCP server integration
  
- System Management:
  * Comprehensive health monitoring
  * Automated database maintenance
  * Error tracking and logging
  * Performance optimization

- Development Tools:
  * Project continuity tracking
  * Code context management
  * Development session history
  * Insight storage and retrieval

All timestamps are stored in the local timezone using ISO format. This ensures
that timestamps are correctly displayed and interpreted in the local time context.

For usage examples and integration guides, see the documentation in /docs.
"""

import asyncio
import sqlite3
import json
import uuid
import logging
import aiohttp
import numpy as np
import hashlib
import os
import re
import time
import socket
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta, tzinfo
from pathlib import Path
from zoneinfo import ZoneInfo

# Get local timezone
def get_local_timezone() -> ZoneInfo:
    """Get local timezone based on system settings"""
    try:
        import time
        return ZoneInfo(time.tzname[0])
    except:
        # Fallback to a common timezone if detection fails
        return ZoneInfo("America/Chicago")  # Central Time fallback
    
def get_current_timestamp() -> str:
    """Get current timestamp in local timezone ISO format"""
    return datetime.now(get_local_timezone()).isoformat()
    
def datetime_to_local_isoformat(dt: datetime) -> str:
    """Convert any datetime to local timezone ISO format"""
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=get_local_timezone())
    return dt.astimezone(get_local_timezone()).isoformat()

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import hashlib
import sqlite3
import json
import uuid
import hashlib
import asyncio
import aiohttp
import logging
import os
import re
import time
import socket
import numpy as np
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Configure logging with minimal output
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
# Only show important messages and errors
logger.setLevel(logging.WARNING)


class DatabaseManager:
    """Base database manager for common operations"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.ensure_database_exists()
    
    def ensure_database_exists(self):
        """Ensure the database file and directory exist"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
    def get_connection(self) -> sqlite3.Connection:
        """Get a database connection with proper configuration"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # Enable dict-like access
        conn.execute("PRAGMA foreign_keys = ON")  # Enable foreign key constraints
        return conn
    
    async def execute_query(self, query: str, params: Tuple = ()) -> List[sqlite3.Row]:
        """Execute a SELECT query and return results"""
        with self.get_connection() as conn:
            cursor = conn.execute(query, params)
            return cursor.fetchall()
    
    async def execute_update(self, query: str, params: Tuple = ()) -> str:
        """Execute an INSERT/UPDATE/DELETE query and return last row ID"""
        with self.get_connection() as conn:
            try:
                cursor = conn.execute(query, params)
                conn.commit()
                return str(cursor.lastrowid)
            except sqlite3.Error as e:
                logger.error(f"Database error: {e}")
                logger.error(f"Query: {query}")
                logger.error(f"Params: {params}")
                raise
                
    def parse_timestamp(self, timestamp: Union[str, int, float, None], fallback: Optional[datetime] = None) -> str:
        """Parse various timestamp formats into ISO format string.
        
        Args:
            timestamp: Input timestamp (string, unix timestamp, or None)
            fallback: Optional fallback datetime if parsing fails
            
        Returns:
            ISO format datetime string
        """
        if not timestamp:
            return (fallback or datetime.now(get_local_timezone())).isoformat()
            
        try:
            if isinstance(timestamp, (int, float)):
                # Unix timestamp
                dt = datetime.fromtimestamp(timestamp, timezone.utc)
            elif isinstance(timestamp, str):
                # Try various string formats
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                except ValueError:
                    # Try parsing with dateutil as fallback
                    from dateutil import parser
                    dt = parser.parse(timestamp)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
            else:
                raise ValueError(f"Unsupported timestamp format: {type(timestamp)}")
                
            return dt.isoformat()
            
        except Exception as e:
            logger.warning(f"Error parsing timestamp {timestamp}: {e}")
            return (fallback or datetime.now(get_local_timezone())).isoformat()


class MCPToolCallDatabase(DatabaseManager):
    """üîß NEW: Tracks all MCP tool calls for reflection and debugging"""
    
    def __init__(self, db_path: str = "mcp_tool_calls.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tool call tracking tables"""
        with self.get_connection() as conn:
            # Tool calls table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_calls (
                    call_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    client_id TEXT,
                    tool_name TEXT NOT NULL,
                    parameters TEXT NOT NULL,
                    result TEXT,
                    status TEXT NOT NULL,
                    execution_time_ms INTEGER,
                    error_message TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Tool usage statistics
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_usage_stats (
                    stat_id TEXT PRIMARY KEY,
                    tool_name TEXT NOT NULL,
                    date TEXT NOT NULL,
                    call_count INTEGER DEFAULT 0,
                    success_count INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    avg_execution_time_ms REAL DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(tool_name, date)
                )
            """)
            
            # AI reflections table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS ai_reflections (
                    reflection_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    reflection_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    insights TEXT,
                    recommendations TEXT,
                    confidence_level REAL DEFAULT 0.5,
                    source_period_days INTEGER,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Usage patterns table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS usage_patterns (
                    pattern_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    pattern_type TEXT NOT NULL,
                    insight TEXT NOT NULL,
                    analysis_period_days INTEGER NOT NULL,
                    confidence_score REAL DEFAULT 0.5,
                    supporting_data TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
    
    async def log_tool_call(self, tool_name: str, parameters: Dict, result: Any = None, 
                           status: str = "success", execution_time_ms: float = None,
                           error_message: str = None, client_id: str = None) -> str:
        """Log a tool call with all relevant details"""
        
        call_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        # Store the tool call
        await self.execute_update(
            """INSERT INTO tool_calls 
               (call_id, timestamp, client_id, tool_name, parameters, result, 
                status, execution_time_ms, error_message) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (call_id, timestamp, client_id, tool_name, 
             json.dumps(parameters), json.dumps(result) if result else None,
             status, int(execution_time_ms) if execution_time_ms else None, error_message)
        )
        
        # Update daily statistics
        await self._update_tool_stats(tool_name, status, execution_time_ms)
        
        return call_id
    
    async def _update_tool_stats(self, tool_name: str, status: str, execution_time_ms: float):
        """Update daily usage statistics for a tool"""
        today = datetime.now(get_local_timezone()).date().isoformat()
        
        # Check if stat record exists for today
        existing = await self.execute_query(
            "SELECT * FROM tool_usage_stats WHERE tool_name = ? AND date = ?",
            (tool_name, today)
        )
        
        if existing:
            # Update existing record
            stat = existing[0]
            new_call_count = stat["call_count"] + 1
            new_success_count = stat["success_count"] + (1 if status == "success" else 0)
            new_error_count = stat["error_count"] + (1 if status == "error" else 0)
            
            # Calculate new average execution time
            if execution_time_ms and stat["avg_execution_time_ms"]:
                new_avg = ((stat["avg_execution_time_ms"] * stat["call_count"]) + execution_time_ms) / new_call_count
            elif execution_time_ms:
                new_avg = execution_time_ms
            else:
                new_avg = stat["avg_execution_time_ms"]
            
            await self.execute_update(
                """UPDATE tool_usage_stats 
                   SET call_count = ?, success_count = ?, error_count = ?, avg_execution_time_ms = ?
                   WHERE tool_name = ? AND date = ?""",
                (new_call_count, new_success_count, new_error_count, new_avg, tool_name, today)
            )
        else:
            # Create new record
            stat_id = str(uuid.uuid4())
            await self.execute_update(
                """INSERT INTO tool_usage_stats 
                   (stat_id, tool_name, date, call_count, success_count, error_count, avg_execution_time_ms)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (stat_id, tool_name, today, 1,
                 1 if status == "success" else 0,
                 1 if status == "error" else 0,
                 execution_time_ms or 0)
            )
    
    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """Get tool usage summary for the last N days"""
        
        # Get recent tool calls
        recent_calls = await self.execute_query(
            """SELECT tool_name, status, COUNT(*) as count
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name, status
               ORDER BY count DESC""".format(days)
        )
        
        # Get daily stats
        daily_stats = await self.execute_query(
            """SELECT * FROM tool_usage_stats 
               WHERE date >= date('now', '-{} days')
               ORDER BY date DESC, call_count DESC""".format(days)
        )
        
        # Get most used tools
        most_used = await self.execute_query(
            """SELECT tool_name, COUNT(*) as total_calls
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name
               ORDER BY total_calls DESC
               LIMIT 10""".format(days)
        )
        
        return {
            "recent_calls": [dict(row) for row in recent_calls],
            "daily_stats": [dict(row) for row in daily_stats],
            "most_used_tools": [dict(row) for row in most_used],
            "period_days": days
        }
    
    async def get_tool_call_history(self, tool_name: str = None, limit: int = 50) -> List[Dict]:
        """Get recent tool call history, optionally filtered by tool name"""
        
        if tool_name:
            query = """SELECT * FROM tool_calls 
                      WHERE tool_name = ? 
                      ORDER BY timestamp DESC 
                      LIMIT ?"""
            params = (tool_name, limit)
        else:
            query = """SELECT * FROM tool_calls 
                      ORDER BY timestamp DESC 
                      LIMIT ?"""
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]
        
    async def store_ai_reflection(self, reflection_type: str, content: str,
                                insights: List[str] = None, recommendations: List[str] = None,
                                confidence_level: float = 0.5, source_period_days: int = None) -> str:
        """Store AI self-reflection on tool usage and patterns.
        
        Args:
            reflection_type: Type of reflection (e.g., usage_patterns, performance, suggestions)
            content: Main reflection content
            insights: List of specific insights gained
            recommendations: List of action recommendations
            confidence_level: Confidence in the reflection (0-1)
            source_period_days: Period of data analyzed
            
        Returns:
            str: Reflection ID
        """
        reflection_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO ai_reflections 
               (reflection_id, timestamp, reflection_type, content, insights, 
                recommendations, confidence_level, source_period_days)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (reflection_id, timestamp, reflection_type, content,
             json.dumps(insights) if insights else None,
             json.dumps(recommendations) if recommendations else None,
             confidence_level, source_period_days)
        )
        
        return reflection_id
        
    async def store_usage_pattern(self, pattern_type: str, insight: str, 
                                analysis_period_days: int, confidence_score: float = 0.5,
                                supporting_data: Dict = None) -> str:
        """Store identified usage pattern from AI analysis.
        
        Args:
            pattern_type: Type of usage pattern
            insight: Description of the pattern
            analysis_period_days: Period analyzed to identify pattern
            confidence_score: Confidence in pattern (0-1)
            supporting_data: Additional data supporting the pattern
            
        Returns:
            str: Pattern ID
        """
        pattern_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO usage_patterns
               (pattern_id, timestamp, pattern_type, insight, analysis_period_days,
                confidence_score, supporting_data)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (pattern_id, timestamp, pattern_type, insight, analysis_period_days,
             confidence_score, json.dumps(supporting_data) if supporting_data else None)
        )
        
        return pattern_id
        
    async def get_recent_reflections(self, limit: int = 5, reflection_type: str = None) -> List[Dict]:
        """Get recent AI reflections, optionally filtered by type.
        
        Args:
            limit: Maximum number of reflections to return
            reflection_type: Optional filter by reflection type
            
        Returns:
            List of reflection entries
        """
        if reflection_type:
            query = """SELECT * FROM ai_reflections
                      WHERE reflection_type = ?
                      ORDER BY timestamp DESC
                      LIMIT ?"""
            params = (reflection_type, limit)
        else:
            query = """SELECT * FROM ai_reflections
                      ORDER BY timestamp DESC
                      LIMIT ?"""
            params = (limit,)
            
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]


class ConversationDatabase(DatabaseManager):
    """Manages conversation auto-save database"""
    
    def __init__(self, db_path: str = "conversations.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            # --- Migration logic for messages table ---
            expected_columns = [
                'message_id', 'conversation_id', 'timestamp', 'role', 'content', 'source_type',
                'source_id', 'source_url', 'source_metadata', 'sync_status', 'last_sync',
                'metadata', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(messages)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in expected_columns:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating messages table to new schema!")
                old_rows = conn.execute("SELECT * FROM messages").fetchall()
                conn.execute("DROP TABLE IF EXISTS messages")
                conn.execute("""
                    CREATE TABLE messages (
                        message_id TEXT PRIMARY KEY,
                        conversation_id TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        role TEXT NOT NULL,
                        content TEXT NOT NULL,
                        source_type TEXT,
                        source_id TEXT,
                        source_url TEXT,
                        source_metadata TEXT,
                        sync_status TEXT,
                        last_sync TEXT,
                        metadata TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in expected_columns:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO messages ({', '.join(expected_columns)}) VALUES ({', '.join(['?' for _ in expected_columns])})",
                        tuple(row_dict[col] for col in expected_columns)
                    )
                print(f"Restored {len(old_rows)} messages after migration.")
            else:
                # Create table if not exists (normal path)
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS messages (
                        message_id TEXT PRIMARY KEY,
                        conversation_id TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        role TEXT NOT NULL,
                        content TEXT NOT NULL,
                        source_type TEXT,
                        source_id TEXT,
                        source_url TEXT,
                        source_metadata TEXT,
                        sync_status TEXT,
                        last_sync TEXT,
                        metadata TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
                    )
                """)

            # Sessions table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    start_timestamp TEXT NOT NULL,
                    end_timestamp TEXT,
                    context TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Conversations table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversations (
                    conversation_id TEXT PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    start_timestamp TEXT NOT NULL,
                    end_timestamp TEXT,
                    topic_summary TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES sessions (session_id)
                )
            """)

            # Source metadata table for tracking chat sources
            conn.execute("""
                CREATE TABLE IF NOT EXISTS source_tracking (
                    source_id TEXT PRIMARY KEY,
                    source_type TEXT NOT NULL,
                    source_name TEXT NOT NULL,
                    source_path TEXT,
                    last_check TEXT NOT NULL,
                    last_sync TEXT,
                    status TEXT NOT NULL,
                    error_count INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Cross-source relationships table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversation_relationships (
                    relationship_id TEXT PRIMARY KEY,
                    source_conversation_id TEXT NOT NULL,
                    related_conversation_id TEXT NOT NULL,
                    relationship_type TEXT NOT NULL,
                    metadata TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (source_conversation_id) REFERENCES conversations (conversation_id),
                    FOREIGN KEY (related_conversation_id) REFERENCES conversations (conversation_id)
                )
            """)

            conn.commit()
    
    async def store_message(self, content: str, role: str, session_id: str = None, 
                          conversation_id: str = None, metadata: Dict = None) -> Dict[str, str]:
        """Store a message and auto-manage sessions/conversations with duplicate detection"""
        timestamp = get_current_timestamp()
        message_id = str(uuid.uuid4())

        # Advanced duplicate detection: check for existing message with same content, role, and session in last hour
        if session_id:
            existing = await self.execute_query(
                """SELECT message_id FROM messages 
                   WHERE conversation_id IN (
                       SELECT conversation_id FROM conversations WHERE session_id = ?
                   ) AND role = ? AND content = ? 
                   AND datetime(timestamp) > datetime('now', '-1 hour')""",
                (session_id, role, content)
            )
            if existing:
                print(f"Skipping duplicate message in session {session_id}")
                return {
                    "message_id": existing[0]["message_id"],
                    "conversation_id": None,
                    "session_id": session_id,
                    "duplicate": True
                }

        # Auto-create session if not provided or doesn't exist
        if not session_id:
            session_id = str(uuid.uuid4())
            await self.execute_update(
                "INSERT INTO sessions (session_id, start_timestamp, context) VALUES (?, ?, ?)",
                (session_id, timestamp, "auto-created")
            )
        else:
            existing_session = await self.execute_query(
                "SELECT session_id FROM sessions WHERE session_id = ?",
                (session_id,)
            )
            if not existing_session:
                await self.execute_update(
                    "INSERT INTO sessions (session_id, start_timestamp, context) VALUES (?, ?, ?)",
                    (session_id, timestamp, "imported-session")
                )

        # Auto-create conversation if not provided
        if not conversation_id:
            conversation_id = str(uuid.uuid4())
            await self.execute_update(
                "INSERT INTO conversations (conversation_id, session_id, start_timestamp) VALUES (?, ?, ?)",
                (conversation_id, session_id, timestamp)
            )

        # Store the message
        await self.execute_update(
            """INSERT INTO messages 
               (message_id, conversation_id, timestamp, role, content, metadata) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (message_id, conversation_id, timestamp, role, content, 
             json.dumps(metadata) if metadata else None)
        )

        return {
            "message_id": message_id,
            "conversation_id": conversation_id,
            "session_id": session_id,
            "duplicate": False
        }
    
    async def get_recent_messages(self, limit: int = 10, session_id: str = None) -> List[Dict]:
        """Get recent messages, optionally filtered by session"""
        
        if session_id:
            query = """
                SELECT m.*, c.session_id 
                FROM messages m 
                JOIN conversations c ON m.conversation_id = c.conversation_id
                WHERE c.session_id = ?
                ORDER BY m.timestamp DESC 
                LIMIT ?
            """
            params = (session_id, limit)
        else:
            query = """
                SELECT m.*, c.session_id 
                FROM messages m 
                JOIN conversations c ON m.conversation_id = c.conversation_id
                ORDER BY m.timestamp DESC 
                LIMIT ?
            """
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]


class AIMemoryDatabase(DatabaseManager):
    """Manages AI-curated memories database with enhanced operations"""
    
    def __init__(self, db_path: str = "ai_memories.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            expected_columns = [
                'memory_id', 'timestamp_created', 'timestamp_updated', 'source_conversation_id',
                'source_message_ids', 'memory_type', 'content', 'importance_level', 'tags',
                'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(curated_memories)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in expected_columns:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating curated_memories table to new schema!")
                old_rows = conn.execute("SELECT * FROM curated_memories").fetchall()
                conn.execute("DROP TABLE IF EXISTS curated_memories")
                conn.execute("""
                    CREATE TABLE curated_memories (
                        memory_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        timestamp_updated TEXT NOT NULL,
                        source_conversation_id TEXT,
                        source_message_ids TEXT,
                        memory_type TEXT,
                        content TEXT NOT NULL,
                        importance_level INTEGER DEFAULT 5,
                        tags TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in expected_columns:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO curated_memories ({', '.join(expected_columns)}) VALUES ({', '.join(['?' for _ in expected_columns])})",
                        tuple(row_dict[col] for col in expected_columns)
                    )
                print(f"Restored {len(old_rows)} curated memories after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS curated_memories (
                        memory_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        timestamp_updated TEXT NOT NULL,
                        source_conversation_id TEXT,
                        source_message_ids TEXT,
                        memory_type TEXT,
                        content TEXT NOT NULL,
                        importance_level INTEGER DEFAULT 5,
                        tags TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            conn.commit()

    async def create_memory(self, content: str, memory_type: str = None, 
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> str:
        """Create a new curated memory with duplicate detection"""
        memory_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Advanced duplicate detection: check for existing memory with same content, type, and source
        existing = await self.execute_query(
            """SELECT memory_id FROM curated_memories 
                   WHERE content = ? AND memory_type = ? AND source_conversation_id IS ?""",
            (content, memory_type, source_conversation_id)
        )
        if existing:
            print("Skipping duplicate curated memory entry.")
            return existing[0]["memory_id"]

        await self.execute_update(
            """INSERT INTO curated_memories 
               (memory_id, timestamp_created, timestamp_updated, source_conversation_id, 
                memory_type, content, importance_level, tags) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (memory_id, timestamp, timestamp, source_conversation_id, 
             memory_type, content, importance_level, 
             json.dumps(tags) if tags else None)
        )
        return memory_id
        """Run database maintenance tasks.
        
        Args:
            force: Whether to force maintenance even if recent
            
        Returns:
            Dict containing maintenance results
        """
        try:
            # Check last maintenance
            last_maintenance = await self.execute_query(
                "SELECT value FROM metadata WHERE key = 'last_maintenance'"
            )
            
            if not force and last_maintenance:
                last_time = datetime.fromisoformat(last_maintenance[0]["value"])
                if datetime.now(get_local_timezone()) - last_time < timedelta(days=7):
                    return {
                        "status": "skipped",
                        "message": "Maintenance ran recently",
                        "last_run": last_time.isoformat()
                    }
            
            with self.get_connection() as conn:
                # Optimize indexes
                conn.execute("ANALYZE")
                
                # Clean up any orphaned records
                conn.execute("""
                    DELETE FROM curated_memories 
                    WHERE source_conversation_id NOT IN (
                        SELECT conversation_id FROM conversations
                    ) AND source_conversation_id IS NOT NULL
                """)
                
                # Update metadata
                conn.execute(
                    "INSERT OR REPLACE INTO metadata (key, value) VALUES (?, ?)",
                    ("last_maintenance", get_current_timestamp())
                )
                
                conn.commit()
                
            return {
                "status": "success",
                "message": "Maintenance completed successfully",
                "timestamp": get_current_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Maintenance error: {e}")
            return {
                "status": "error",
                "message": str(e),
                "timestamp": get_current_timestamp()
            }
    
    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS curated_memories (
                    memory_id TEXT PRIMARY KEY,
                    timestamp_created TEXT NOT NULL,
                    timestamp_updated TEXT NOT NULL,
                    source_conversation_id TEXT,
                    source_message_ids TEXT,
                    memory_type TEXT,
                    content TEXT NOT NULL,
                    importance_level INTEGER DEFAULT 5,
                    tags TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
    
    async def create_memory(self, content: str, memory_type: str = None, 
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> str:
        """Create a new curated memory"""
        
        memory_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO curated_memories 
               (memory_id, timestamp_created, timestamp_updated, source_conversation_id, 
                memory_type, content, importance_level, tags) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (memory_id, timestamp, timestamp, source_conversation_id, 
             memory_type, content, importance_level, 
             json.dumps(tags) if tags else None)
        )
        
        return memory_id


class ScheduleDatabase(DatabaseManager):
    """Manages appointments and reminders database"""
    
    def __init__(self, db_path: str = "schedule.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            # Appointments table migration
            appointments_expected = [
                'appointment_id', 'timestamp_created', 'scheduled_datetime', 'title', 'description',
                'location', 'source_conversation_id', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(appointments)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in appointments_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating appointments table to new schema!")
                old_rows = conn.execute("SELECT * FROM appointments").fetchall()
                conn.execute("DROP TABLE IF EXISTS appointments")
                conn.execute("""
                    CREATE TABLE appointments (
                        appointment_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        scheduled_datetime TEXT NOT NULL,
                        title TEXT NOT NULL,
                        description TEXT,
                        location TEXT,
                        source_conversation_id TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in appointments_expected:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO appointments ({', '.join(appointments_expected)}) VALUES ({', '.join(['?' for _ in appointments_expected])})",
                        tuple(row_dict[col] for col in appointments_expected)
                    )
                print(f"Restored {len(old_rows)} appointments after migration.")
            # Appointments table migration
            appointments_expected = [
                'appointment_id', 'timestamp_created', 'scheduled_datetime', 'title', 'description',
                'location', 'is_cancelled', 'source_conversation_id', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(appointments)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in appointments_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating appointments table to new schema!")
                old_rows = conn.execute("SELECT * FROM appointments").fetchall()
                conn.execute("DROP TABLE IF EXISTS appointments")
                conn.execute("""
                    CREATE TABLE appointments (
                        appointment_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        scheduled_datetime TEXT NOT NULL,
                        title TEXT NOT NULL,
                        description TEXT,
                        location TEXT,
                        is_cancelled INTEGER DEFAULT 0,
                        source_conversation_id TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in appointments_expected:
                        if col not in row_dict:
                            if col == 'is_cancelled':
                                row_dict[col] = 0
                            else:
                                row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO appointments ({', '.join(appointments_expected)}) VALUES ({', '.join(['?' for _ in appointments_expected])})",
                        [row_dict[col] for col in appointments_expected]
                    )
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS appointments (
                        appointment_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        scheduled_datetime TEXT NOT NULL,
                        title TEXT NOT NULL,
                        description TEXT,
                        location TEXT,
                        is_cancelled INTEGER DEFAULT 0,
                        source_conversation_id TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)

            # Reminders table migration
            reminders_expected = [
                'reminder_id', 'timestamp_created', 'due_datetime', 'content', 'priority_level',
                'completed', 'is_completed', 'completed_at', 'source_conversation_id', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(reminders)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in reminders_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating reminders table to new schema!")
                old_rows = conn.execute("SELECT * FROM reminders").fetchall()
                conn.execute("DROP TABLE IF EXISTS reminders")
                conn.execute("""
                    CREATE TABLE reminders (
                        reminder_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        due_datetime TEXT NOT NULL,
                        content TEXT NOT NULL,
                        priority_level INTEGER DEFAULT 5,
                        completed INTEGER DEFAULT 0,
                        is_completed INTEGER DEFAULT 0,
                        completed_at TEXT,
                        source_conversation_id TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in reminders_expected:
                        if col not in row_dict:
                            if col == 'is_completed':
                                row_dict[col] = row_dict.get('completed', 0)
                            else:
                                row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO reminders ({', '.join(reminders_expected)}) VALUES ({', '.join(['?' for _ in reminders_expected])})",
                        tuple(row_dict[col] for col in reminders_expected)
                    )
                print(f"Restored {len(old_rows)} reminders after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS reminders (
                        reminder_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        due_datetime TEXT NOT NULL,
                        content TEXT NOT NULL,
                        priority_level INTEGER DEFAULT 5,
                        completed INTEGER DEFAULT 0,
                        source_conversation_id TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            conn.commit()
    
    async def create_appointment(self, title: str, scheduled_datetime: str, 
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> str:
        """Create a new appointment with duplicate detection"""
        appointment_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing appointment with same title, datetime, location, and source
        existing = await self.execute_query(
            """SELECT appointment_id FROM appointments 
                   WHERE title = ? AND scheduled_datetime = ? AND location IS ? AND source_conversation_id IS ?""",
            (title, scheduled_datetime, location, source_conversation_id)
        )
        if existing:
            print("Skipping duplicate appointment entry.")
            return existing[0]["appointment_id"]

        await self.execute_update(
            """INSERT INTO appointments 
               (appointment_id, timestamp_created, scheduled_datetime, title, description, location, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (appointment_id, timestamp, scheduled_datetime, title, description, location, source_conversation_id)
        )
        return appointment_id
    
    async def create_reminder(self, content: str, due_datetime: str, 
                            priority_level: int = 5, source_conversation_id: str = None) -> str:
        """Create a new reminder with duplicate detection"""
        reminder_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing reminder with same content, due_datetime, and source
        existing = await self.execute_query(
            """SELECT reminder_id FROM reminders 
                   WHERE content = ? AND due_datetime = ? AND source_conversation_id IS ?""",
            (content, due_datetime, source_conversation_id)
        )
        if existing:
            print("Skipping duplicate reminder entry.")
            return existing[0]["reminder_id"]

        await self.execute_update(
            """INSERT INTO reminders 
               (reminder_id, timestamp_created, due_datetime, content, priority_level, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (reminder_id, timestamp, due_datetime, content, priority_level, source_conversation_id)
        )
        return reminder_id
    
    async def get_upcoming_appointments(self, days_ahead: int = 7) -> List[Dict]:
        """Get upcoming appointments within specified days"""
        
        future_date = datetime.now(get_local_timezone()) + timedelta(days=days_ahead)
        
        rows = await self.execute_query(
            """SELECT * FROM appointments 
               WHERE scheduled_datetime >= ? AND scheduled_datetime <= ?
               ORDER BY scheduled_datetime ASC""",
            (get_current_timestamp(), future_date.isoformat())
        )
        
        return [dict(row) for row in rows]
    
    async def get_active_reminders(self) -> List[Dict]:
        """Get all uncompleted reminders"""
        
        rows = await self.execute_query(
            "SELECT * FROM reminders WHERE completed = 0 ORDER BY due_datetime ASC"
        )
        
        return [dict(row) for row in rows]


class VSCodeProjectDatabase(DatabaseManager):
    """Manages VS Code project context and development sessions"""
    
    def __init__(self, db_path: str = "vscode_project.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            # Project sessions table migration
            sessions_expected = [
                'session_id', 'start_timestamp', 'end_timestamp', 'workspace_path', 'active_files',
                'git_branch', 'session_summary', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(project_sessions)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in sessions_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating project_sessions table to new schema!")
                old_rows = conn.execute("SELECT * FROM project_sessions").fetchall()
                conn.execute("DROP TABLE IF EXISTS project_sessions")
                conn.execute("""
                    CREATE TABLE project_sessions (
                        session_id TEXT PRIMARY KEY,
                        start_timestamp TEXT NOT NULL,
                        end_timestamp TEXT,
                        workspace_path TEXT NOT NULL,
                        active_files TEXT,
                        git_branch TEXT,
                        session_summary TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in sessions_expected:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO project_sessions ({', '.join(sessions_expected)}) VALUES ({', '.join(['?' for _ in sessions_expected])})",
                        tuple(row_dict[col] for col in sessions_expected)
                    )
                print(f"Restored {len(old_rows)} project sessions after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS project_sessions (
                        session_id TEXT PRIMARY KEY,
                        start_timestamp TEXT NOT NULL,
                        end_timestamp TEXT,
                        workspace_path TEXT NOT NULL,
                        active_files TEXT,
                        git_branch TEXT,
                        session_summary TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)

            # Project insights table migration
            insights_expected = [
                'insight_id', 'timestamp_created', 'timestamp_updated', 'insight_type', 'content',
                'related_files', 'source_conversation_id', 'importance_level', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(project_insights)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in insights_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating project_insights table to new schema!")
                old_rows = conn.execute("SELECT * FROM project_insights").fetchall()
                conn.execute("DROP TABLE IF EXISTS project_insights")
                conn.execute("""
                    CREATE TABLE project_insights (
                        insight_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        timestamp_updated TEXT NOT NULL,
                        insight_type TEXT,
                        content TEXT NOT NULL,
                        related_files TEXT,
                        source_conversation_id TEXT,
                        importance_level INTEGER DEFAULT 5,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in insights_expected:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO project_insights ({', '.join(insights_expected)}) VALUES ({', '.join(['?' for _ in insights_expected])})",
                        tuple(row_dict[col] for col in insights_expected)
                    )
                print(f"Restored {len(old_rows)} project insights after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS project_insights (
                        insight_id TEXT PRIMARY KEY,
                        timestamp_created TEXT NOT NULL,
                        timestamp_updated TEXT NOT NULL,
                        insight_type TEXT,
                        content TEXT NOT NULL,
                        related_files TEXT,
                        source_conversation_id TEXT,
                        importance_level INTEGER DEFAULT 5,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)

            # Code context table migration
            codectx_expected = [
                'context_id', 'timestamp', 'file_path', 'function_name', 'description', 'purpose',
                'related_insights', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(code_context)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in codectx_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating code_context table to new schema!")
                old_rows = conn.execute("SELECT * FROM code_context").fetchall()
                conn.execute("DROP TABLE IF EXISTS code_context")
                conn.execute("""
                    CREATE TABLE code_context (
                        context_id TEXT PRIMARY KEY,
                        timestamp TEXT NOT NULL,
                        file_path TEXT NOT NULL,
                        function_name TEXT,
                        description TEXT NOT NULL,
                        purpose TEXT,
                        related_insights TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in codectx_expected:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO code_context ({', '.join(codectx_expected)}) VALUES ({', '.join(['?' for _ in codectx_expected])})",
                        tuple(row_dict[col] for col in codectx_expected)
                    )
                print(f"Restored {len(old_rows)} code contexts after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS code_context (
                        context_id TEXT PRIMARY KEY,
                        timestamp TEXT NOT NULL,
                        file_path TEXT NOT NULL,
                        function_name TEXT,
                        description TEXT NOT NULL,
                        purpose TEXT,
                        related_insights TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)

            # Development conversations table migration
            devcon_expected = [
                'conversation_id', 'session_id', 'timestamp', 'chat_context_id', 'conversation_content',
                'decisions_made', 'code_changes', 'embedding', 'created_at'
            ]
            cur = conn.execute("PRAGMA table_info(development_conversations)")
            current_columns = [row[1] for row in cur.fetchall()]
            needs_migration = False
            if current_columns:
                for col in devcon_expected:
                    if col not in current_columns:
                        needs_migration = True
                        break
            if needs_migration:
                print("Migrating development_conversations table to new schema!")
                old_rows = conn.execute("SELECT * FROM development_conversations").fetchall()
                conn.execute("DROP TABLE IF EXISTS development_conversations")
                conn.execute("""
                    CREATE TABLE development_conversations (
                        conversation_id TEXT PRIMARY KEY,
                        session_id TEXT,
                        timestamp TEXT NOT NULL,
                        chat_context_id TEXT,
                        conversation_content TEXT NOT NULL,
                        decisions_made TEXT,
                        code_changes TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (session_id) REFERENCES project_sessions (session_id)
                    )
                """)
                for row in old_rows:
                    row_dict = dict(row)
                    for col in devcon_expected:
                        if col not in row_dict:
                            row_dict[col] = None
                    conn.execute(
                        f"INSERT INTO development_conversations ({', '.join(devcon_expected)}) VALUES ({', '.join(['?' for _ in devcon_expected])})",
                        tuple(row_dict[col] for col in devcon_expected)
                    )
                print(f"Restored {len(old_rows)} development conversations after migration.")
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS development_conversations (
                        conversation_id TEXT PRIMARY KEY,
                        session_id TEXT,
                        timestamp TEXT NOT NULL,
                        chat_context_id TEXT,
                        conversation_content TEXT NOT NULL,
                        decisions_made TEXT,
                        code_changes TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (session_id) REFERENCES project_sessions (session_id)
                    )
                """)

            conn.commit()
    
    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> str:
        """Save a development session"""
        
        session_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO project_sessions 
               (session_id, start_timestamp, workspace_path, active_files, git_branch, session_summary) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (session_id, timestamp, workspace_path, 
             json.dumps(active_files) if active_files else None,
             git_branch, session_summary)
        )
        
        return session_id
    
    async def store_development_conversation(self, content: str, session_id: str = None,
                                          chat_context_id: str = None, decisions_made: str = None,
                                          code_changes: Dict = None) -> str:
        """Store a development conversation from VS Code
        
        Args:
            content: The conversation content
            session_id: Optional project session ID (will create new if none)
            chat_context_id: Optional VS Code chat context ID
            decisions_made: Summary of decisions made in conversation
            code_changes: Dictionary of files changed and their changes
        """
        conversation_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        # Create session if none provided
        if not session_id:
            session_id = await self.save_development_session(
                workspace_path=os.getcwd(),  # Current workspace
                session_summary="Auto-created session for development conversation"
            )
        
        # Store conversation
        await self.execute_update(
            """INSERT INTO development_conversations 
               (conversation_id, session_id, timestamp, chat_context_id,
                conversation_content, decisions_made, code_changes)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (conversation_id, session_id, timestamp, chat_context_id,
             content, decisions_made, json.dumps(code_changes) if code_changes else None)
        )
        
        return conversation_id

    async def store_project_insight(self, content: str, insight_type: str = None,
                                  related_files: List[str] = None, importance_level: int = 5,
                                  source_conversation_id: str = None) -> str:
        """Store a project insight with duplicate detection"""
        insight_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing insight with same content, type, and source
        existing = await self.execute_query(
            """SELECT insight_id FROM project_insights 
                   WHERE content = ? AND insight_type IS ? AND source_conversation_id IS ?""",
            (content, insight_type, source_conversation_id)
        )
        if existing:
            print("Skipping duplicate project insight entry.")
            return existing[0]["insight_id"]

        await self.execute_update(
            """INSERT INTO project_insights 
               (insight_id, timestamp_created, timestamp_updated, insight_type, content, 
                related_files, source_conversation_id, importance_level) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (insight_id, timestamp, timestamp, insight_type, content,
             json.dumps(related_files) if related_files else None,
             source_conversation_id, importance_level)
        )
        return insight_id


class ConversationFileMonitor:
    def __init__(self, memory_system, watch_directories):
        self.memory_system = memory_system
        self.watch_directories = watch_directories
        self.vscode_db = memory_system.vscode_db
        self.conversations_db = memory_system.conversations_db  # Add this to maintain compatibility
        self.curated_db = memory_system.curated_db  # Add this to maintain compatibility
        # Do NOT start file monitoring or background tasks automatically here
        
    def _parse_character_ai_format(self, data: Dict) -> List[Dict]:
        """Parse Character.ai conversation format (list of messages under 'conversation')"""
        conversations = []
        try:
            messages = data.get('conversation', [])
            for msg in messages:
                if isinstance(msg, dict) and 'content' in msg:
                    conversations.append({
                        'role': msg.get('character', msg.get('role', 'unknown')),
                        'content': msg['content'],
                        'timestamp': msg.get('timestamp')
                    })
        except Exception as e:
            logger.error(f"Error parsing Character.ai format: {e}")
        return conversations

    def _parse_text_gen_format(self, data: Dict) -> List[Dict]:
        """Parse text-generation-webui format (list of messages under 'history')"""
        conversations = []
        try:
            history = data.get('history', [])
            for msg in history:
                if isinstance(msg, dict) and 'content' in msg:
                    conversations.append({
                        'role': msg.get('role', 'unknown'),
                        'content': msg['content'],
                        'timestamp': msg.get('timestamp')
                    })
        except Exception as e:
            logger.error(f"Error parsing text-generation-webui format: {e}")
        return conversations
    """Monitors files for conversation changes and auto-imports them.
    
    Features:
    - Automatic MCP server detection to avoid duplicate message processing
    - Real-time file monitoring with hash-based change detection
    - Support for VS Code, LM Studio, and Ollama chat files
    - Message deduplication across sources
    """
    
    def __init__(self, memory_system, watch_directories: List[str] = None, mcp_port: int = 1234):
        self.memory_system = memory_system
        self.watch_directories = watch_directories or []
        self.observer = None
        self.processed_files = set()  # Track processed files to avoid duplicates
        self.file_hashes = {}  # Track file content hashes to detect changes
        self.processed_messages = {}  # Track processed messages per file: {file_path: set(message_hashes)}
        self.mcp_port = mcp_port  # Port to check for MCP server
        self.mcp_server_running = False  # Will be updated periodically
        self.last_mcp_check = 0  # Timestamp of last MCP server check
        
    def _get_default_chat_directories(self) -> List[str]:
        """Get default chat storage directories for different platforms"""
        home = Path.home()
        documents = home / "Documents"
        downloads = home / "Downloads"
        directories = []
        
        # NOTE: ChatGPT and Claude desktop apps DO NOT store conversations locally
        # They are cloud-only applications. Removed these paths after verification.
        
        # LM Studio conversation directories
        lm_studio_paths = [
            home / ".lmstudio" / "conversations",  # Windows/Linux/macOS (new location)
            home / "AppData" / "Roaming" / "LM Studio" / "conversations",  # Windows (old location)
            home / ".config" / "lm-studio" / "conversations",  # Linux (old location)
            home / "Library" / "Application Support" / "LM Studio" / "conversations"  # macOS (old location)
        ]
        
        # Ollama database paths (SQLite database instead of files)
        ollama_db_paths = [
            home / "AppData" / "Local" / "Ollama" / "db.sqlite",  # Windows
            home / ".local" / "share" / "ollama" / "db.sqlite",  # Linux
            home / "Library" / "Application Support" / "Ollama" / "db.sqlite"  # macOS
        ]
        
        # Additional popular chat platforms
        perplexity_paths = [
            home / "AppData" / "Roaming" / "Perplexity" / "conversations",  # Windows
            home / ".config" / "perplexity" / "conversations",  # Linux
            home / "Library" / "Application Support" / "Perplexity" / "conversations"  # macOS
        ]
        
        jan_ai_paths = [
            home / "AppData" / "Roaming" / "Jan" / "conversations",  # Windows
            home / ".config" / "jan" / "conversations",  # Linux
            home / "Library" / "Application Support" / "Jan" / "conversations"  # macOS
        ]
        
        open_webui_paths = [
            home / ".open-webui" / "data" / "chats",  # All platforms
            home / "open-webui" / "data" / "chats",  # Alternative location
        ]
        
        # OpenWebUI database paths (SQLite database)
        open_webui_db_paths = [
            home / ".open-webui" / "data" / "webui.db",  # Default location
            home / "open-webui" / "data" / "webui.db",  # Alternative location
            home / "OpenWebUI" / "data" / "webui.db",  # Capitalized variant
            documents / "OpenWebUI" / "data" / "webui.db",  # Documents folder
            downloads / "OpenWebUI" / "data" / "webui.db",  # Downloads folder
        ]
        
        # Text generation WebUI paths  
        text_gen_webui_paths = [
            home / "text-generation-webui" / "logs",
            home / "text-generation-webui" / "characters",
            home / "Documents" / "text-generation-webui" / "logs"
        ]
        
        # SillyTavern paths (requested by Reddit community)
        sillytavern_paths = [
            home / "SillyTavern" / "data" / "chats",  # Default installation
            home / "AppData" / "Roaming" / "SillyTavern" / "chats",  # Windows
            home / ".config" / "sillytavern" / "chats",  # Linux
            home / "Library" / "Application Support" / "SillyTavern" / "chats",  # macOS
            documents / "SillyTavern" / "chats",  # User documents
            downloads / "SillyTavern" / "data" / "chats"  # Downloaded version
        ]
        
        # Gemini CLI paths (requested by Reddit community)
        gemini_cli_paths = [
            home / ".gemini" / "conversations",  # Linux/macOS
            home / "AppData" / "Roaming" / "gemini-cli" / "conversations",  # Windows
            home / ".config" / "gemini" / "conversations",  # Linux alternative
            home / "Library" / "Application Support" / "Gemini" / "conversations"  # macOS
        ]
        
        # VS Code workspace storage directories
        vscode_base_paths = [
            home / "AppData" / "Roaming" / "Code" / "User" / "workspaceStorage",  # Windows
            home / ".config" / "Code" / "User" / "workspaceStorage",  # Linux
            home / "Library" / "Application Support" / "Code" / "User" / "workspaceStorage"  # macOS
        ]
        
        # Helper function to add paths with logging
        def add_paths_if_exist(paths: List[Path], app_name: str):
            for path in paths:
                if path.exists():
                    directories.append(str(path))
                    logger.info(f"Found {app_name} conversations: {path}")
        
        # Add paths for each application (ChatGPT and Claude removed - cloud-only)
        add_paths_if_exist(lm_studio_paths, "LM Studio")
        add_paths_if_exist(perplexity_paths, "Perplexity")
        add_paths_if_exist(jan_ai_paths, "Jan AI")
        add_paths_if_exist(open_webui_paths, "Open WebUI")
        add_paths_if_exist(text_gen_webui_paths, "Text Generation WebUI")
        add_paths_if_exist(sillytavern_paths, "SillyTavern")
        add_paths_if_exist(gemini_cli_paths, "Gemini CLI")
        
        # Special handling for Ollama database
        for db_path in ollama_db_paths:
            if db_path.exists():
                directories.append(str(db_path))
                logger.info(f"Found Ollama database: {db_path}")
        
        # Special handling for OpenWebUI database
        for db_path in open_webui_db_paths:
            if db_path.exists():
                directories.append(str(db_path))
                logger.info(f"Found OpenWebUI database: {db_path}")
        
        # Add VS Code workspace storage paths - find specific workspace hashes
        for vscode_base in vscode_base_paths:
            if vscode_base.exists():
                try:
                    # Look for workspace hashes (directories with chatSessions folders)
                    for workspace_hash in vscode_base.iterdir():
                        if workspace_hash.is_dir():
                            chat_sessions_dir = workspace_hash / "chatSessions"
                            if chat_sessions_dir.exists():
                                directories.append(str(chat_sessions_dir))
                                logger.info(f"Found VS Code chat sessions: {chat_sessions_dir}")
                except Exception as e:
                    logger.error(f"Error scanning VS Code workspace storage: {e}")
        
        return directories

    def _check_mcp_server(self) -> bool:
        """Check if an MCP server is running by attempting a connection.
        
        Returns:
            bool: True if MCP server is running, False otherwise
        """
        # Only check every 60 seconds to avoid overhead
        current_time = time.time()
        if current_time - self.last_mcp_check < 60:
            return self.mcp_server_running
            
        try:
            # Try to connect to MCP server port
            with socket.create_connection(("localhost", self.mcp_port), timeout=1.0):
                self.mcp_server_running = True
        except (socket.timeout, ConnectionRefusedError):
            self.mcp_server_running = False
        
        self.last_mcp_check = current_time
        return self.mcp_server_running
        
    async def _is_message_in_mcp(self, msg_hash: str) -> bool:
        """Check if a message was manually stored through MCP server.
        
        Args:
            msg_hash: Hash of the message content to check
            
        Returns:
            bool: True if message exists in MCP storage, False otherwise
        """
        try:
            # Connect to MCP server
            reader, writer = await asyncio.open_connection('localhost', self.mcp_port)
            
            # Send check request
            request = json.dumps({
                'type': 'check_message',
                'hash': msg_hash
            }).encode() + b'\n'
            writer.write(request)
            await writer.drain()
            
            # Get response
            response = await reader.readline()
            writer.close()
            await writer.wait_closed()
            
            # Parse response
            result = json.loads(response.decode())
            return result.get('exists', False)
            
        except Exception as e:
            logger.debug(f"Failed to check message in MCP: {e}")
            return False  # If check fails, assume message doesn't exist
    
    def _get_mcp_start_time(self) -> Optional[datetime]:
        """Get the start time of the MCP server if running.
        
        Returns:
            Optional[datetime]: Server start time if available, None otherwise
        """
        if not self._check_mcp_server():
            return None
            
        try:
            with socket.create_connection(("localhost", self.mcp_port), timeout=1.0) as sock:
                sock.sendall(b"GET_START_TIME\n")
                response = sock.recv(1024).decode().strip()
                if response and response != "ERROR":
                    return datetime.fromisoformat(response)
        except Exception as e:
            logger.debug(f"Failed to get MCP start time: {e}")
        return None

    async def start_monitoring(self):
        """Start monitoring conversation files"""
        if not self.watch_directories:
            logger.info("No watch directories specified for file monitoring")
            return
            
        # Store reference to the current event loop
        self.loop = asyncio.get_running_loop()
        
        self.observer = Observer()
        
        for directory in self.watch_directories:
            if os.path.exists(directory):
                
                class ConversationFileHandler(FileSystemEventHandler):
                    def __init__(self, monitor):
                        self.monitor = monitor
                    
                    def on_modified(self, event):
                        if not event.is_directory:
                            try:
                                # Get the event loop from the main thread
                                loop = self.monitor.loop
                                if loop and loop.is_running():
                                    asyncio.run_coroutine_threadsafe(
                                        self.monitor._process_file_change(event.src_path), 
                                        loop
                                    )
                            except Exception as e:
                                print(f"Error scheduling file change processing: {e}")
                    
                    def on_created(self, event):
                        if not event.is_directory:
                            try:
                                # Get the event loop from the main thread
                                loop = self.monitor.loop
                                if loop and loop.is_running():
                                    asyncio.run_coroutine_threadsafe(
                                        self.monitor._process_file_change(event.src_path), 
                                        loop
                                    )
                            except Exception as e:
                                print(f"Error scheduling file change processing: {e}")
                
                handler = ConversationFileHandler(self)
                self.observer.schedule(handler, directory, recursive=True)
                logger.info(f"Started monitoring directory: {directory}")
        
        self.observer.start()
        logger.info("File monitoring started")
    
    async def stop_monitoring(self):
        """Stop monitoring conversation files"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            logger.info("File monitoring stopped")
    
    def add_watch_directory(self, directory: str):
        """Add a directory to monitor"""
        if directory not in self.watch_directories:
            self.watch_directories.append(directory)
            logger.info(f"Added watch directory: {directory}")
    
    async def _process_file_change(self, file_path: str):
        """Process a changed conversation file with MCP-aware deduplication"""
        try:
            # Check if file is a conversation file (JSON, txt, etc.)
            if not any(file_path.endswith(ext) for ext in ['.json', '.txt', '.md', '.log']):
                return
            
            # Calculate file hash to detect actual content changes
            with open(file_path, 'rb') as f:
                file_content = f.read()
                current_hash = hashlib.md5(file_content).hexdigest()
            
            # Skip if we've already processed this exact content
            if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:
                return
                
            self.file_hashes[file_path] = current_hash
            
            # Initialize message tracking for this file if needed
            if file_path not in self.processed_messages:
                self.processed_messages[file_path] = set()
            
            # Read and parse conversation content
            conversations = await self._extract_conversations(file_path)
            
            # Check with MCP server for manually stored messages
            if self._check_mcp_server():
                try:
                    filtered_conversations = []
                    for conv in conversations:
                        # Create a hash of the message content and metadata
                        msg_hash = hashlib.md5(
                            f"{conv['role']}:{conv['content']}".encode()
                        ).hexdigest()
                        
                        # Check if this exact message was manually stored
                        if not await self._is_message_in_mcp(msg_hash):
                            filtered_conversations.append(conv)
                    conversations = filtered_conversations
                except Exception as e:
                    logger.debug(f"Failed to check MCP messages: {e}")
                    # If we can't check MCP server, process all messages
            
            # For VS Code chat files, handle development conversations
            is_vscode_chat = 'vscode' in file_path.lower() or 'chatsessions' in file_path.lower()
            if is_vscode_chat:
                # Create development session
                dev_session_id = await self.memory_system.vscode_db.save_development_session(
                    workspace_path=os.path.dirname(file_path),
                    session_summary=f"Imported VS Code chat session from {os.path.basename(file_path)}"
                )
                full_conversation = []
            
            # Store conversations in database
            for conv in conversations:
                result = await self.memory_system.store_conversation(
                    content=conv['content'],
                    role=conv['role'],
                    metadata={'source_file': file_path, 'imported_at': get_current_timestamp()},
                    session_id=self._get_file_hash(file_path)  # Use file hash as session ID for grouping
                )
                
                if is_vscode_chat and not result.get("duplicate", False):
                    # Add to development conversation
                    full_conversation.append(f"{conv['role'].title()}: {conv['content']}")
            
            # Store development conversation if this is a VS Code chat
            if is_vscode_chat and full_conversation:
                await self.memory_system.vscode_db.store_development_conversation(
                    content="\n\n".join(full_conversation),
                    session_id=dev_session_id,
                    chat_context_id=self._get_file_hash(file_path)
                )
            
            logger.info(f"Imported {len(conversations)} conversations from {file_path}")
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """Generate hash of file content for duplicate detection"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return str(hash(file_path))
    
    async def _extract_conversations(self, file_path: str) -> List[Dict]:
        """Extract conversations from various file formats with timestamps, using registry-based extensibility and robust deduplication"""
        conversations = []
        try:
            # Special handling for Ollama SQLite database
            if file_path.lower().endswith('db.sqlite') and 'ollama' in file_path.lower():
                conversations.extend(self._extract_ollama_database(file_path))
                return conversations
            
            # Special handling for OpenWebUI SQLite database
            if (file_path.lower().endswith('webui.db') or 
                (file_path.lower().endswith('.db') and 'openwebui' in file_path.lower()) or
                (file_path.lower().endswith('.db') and 'open-webui' in file_path.lower())):
                conversations.extend(self._extract_openwebui_database(file_path))
                return conversations
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            fallback_time = datetime.fromtimestamp(
                os.path.getmtime(file_path),
                timezone.utc
            ).isoformat()

            # Registry of format handlers: (predicate, handler)
            format_handlers = [
                (lambda fn, _: fn.endswith('.json'), self._handle_json_formats),
                (lambda fn, _: fn.endswith(('.txt', '.md', '.log')), self._parse_text_format),
            ]

            handled = False
            for predicate, handler in format_handlers:
                if predicate(file_path, content):
                    if handler == self._handle_json_formats:
                        conversations.extend(handler(content))
                    else:
                        conversations.extend(handler(content))
                    handled = True
                    break

            if not handled:
                logger.warning(f"No format handler found for {file_path}")

            # Ensure all conversations have timestamps
            for conv in conversations:
                if 'timestamp' not in conv or not conv['timestamp']:
                    conv['timestamp'] = fallback_time

            # Robust deduplication: by id (if present), timestamp (if present), and content hash
            seen = set()
            deduped = []
            for conv in conversations:
                # Use id if present, else None
                cid = conv.get('id') or conv.get('message_id') or None
                ts = conv.get('timestamp') or None
                content_hash = hashlib.md5(conv.get('content', '').encode('utf-8')).hexdigest()
                dedup_key = (cid, ts, content_hash)
                if dedup_key not in seen:
                    seen.add(dedup_key)
                    deduped.append(conv)
            return deduped
        except Exception as e:
            logger.error(f"Error extracting conversations from {file_path}: {e}")
            return []

    def _handle_json_formats(self, content: str) -> List[Dict]:
        """Handle all supported JSON conversation formats (add new ones here)"""
        conversations = []
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if self._is_lmstudio_format(data):
                    conversations.extend(self._parse_lmstudio_format(data))
                elif ('messages' in data or 'chat' in data) and self._is_sillytavern_format(data):
                    conversations.extend(self._parse_sillytavern_format(data))
                elif ('conversation' in data or ('messages' in data and self._is_gemini_cli_format(data))):
                    conversations.extend(self._parse_gemini_cli_format(data))
            elif isinstance(data, list):
                conversations.extend(self._parse_simple_array(data))
        except Exception as e:
            logger.error(f"Error handling JSON formats: {e}")
        return conversations
    
    def _parse_simple_array(self, data: List) -> List[Dict]:
        """Parse simple conversation array format with timestamps"""
        conversations = []
        
        for item in data:
            if isinstance(item, dict) and 'content' in item:
                # Look for timestamp in various formats
                timestamp = None
                for key in ['timestamp', 'time', 'created_at', 'date']:
                    if key in item:
                        try:
                            # Handle both ISO format strings and Unix timestamps
                            if isinstance(item[key], (int, float)):
                                timestamp = datetime.fromtimestamp(item[key], timezone.utc).isoformat()
                            else:
                                timestamp = datetime.fromisoformat(str(item[key])).isoformat()
                            break
                        except (ValueError, TypeError):
                            continue
                
                conversations.append({
                    'role': item.get('role', 'user'),
                    'content': str(item['content']),
                    'timestamp': timestamp
                })
        
        return conversations
    
    def _is_lmstudio_format(self, data: Dict) -> bool:
        """Check if data is in LM Studio format (has messages with versions structure)"""
        try:
            messages = data.get('messages', [])
            if not messages:
                return False
            # Check if first message has the LM Studio structure
            first_msg = messages[0] if isinstance(messages, list) else None
            return (isinstance(first_msg, dict) and 
                    'versions' in first_msg and 
                    'currentlySelected' in first_msg)
        except:
            return False

    def _is_sillytavern_format(self, data: Dict) -> bool:
        """Check if data is in SillyTavern format"""
        if not isinstance(data, dict):
            return False
        
        # SillyTavern specific indicators
        if 'messages' in data:
            messages = data.get('messages', [])
            if isinstance(messages, list) and messages:
                first_msg = messages[0]
                if isinstance(first_msg, dict):
                    # SillyTavern specific fields
                    return 'is_user' in first_msg or 'mes' in first_msg or 'send_date' in first_msg
        
        # Alternative SillyTavern format
        if 'chat' in data:
            chat = data.get('chat', [])
            if isinstance(chat, list) and chat:
                first_msg = chat[0]
                if isinstance(first_msg, dict):
                    return 'is_user' in first_msg or 'mes' in first_msg
        
        return False

    def _is_gemini_cli_format(self, data: Dict) -> bool:
        """Check if data is in Gemini CLI format"""
        if not isinstance(data, dict):
            return False
        
        # Gemini CLI specific structure
        if 'conversation' in data:
            return True
        
        # Check for Gemini-specific message format
        if 'messages' in data:
            messages = data.get('messages', [])
            if isinstance(messages, list) and messages:
                first_msg = messages[0]
                if isinstance(first_msg, dict):
                    # Gemini uses 'parts' array or 'model' role
                    return ('parts' in first_msg or 
                           first_msg.get('role') == 'model' or
                           'response' in first_msg)
        
        return False

    def _parse_lmstudio_format(self, data: Dict) -> List[Dict]:
        """Parse LM Studio conversation format with versions and complex content structure"""
        conversations = []
        try:
            messages = data.get('messages', [])
            conversation_timestamp = data.get('createdAt')
            base_timestamp = None
            
            if conversation_timestamp:
                try:
                    # LM Studio uses millisecond timestamps
                    base_timestamp = datetime.fromtimestamp(conversation_timestamp / 1000, timezone.utc)
                except (ValueError, TypeError):
                    pass
            
            for i, msg in enumerate(messages):
                if not isinstance(msg, dict) or 'versions' not in msg:
                    continue
                
                versions = msg.get('versions', [])
                current_version = msg.get('currentlySelected', 0)
                
                if 0 <= current_version < len(versions):
                    version = versions[current_version]
                    
                    role = version.get('role', 'unknown')
                    content_parts = version.get('content', [])
                    
                    # Extract text content from LM Studio's complex content structure
                    text_content = []
                    for part in content_parts:
                        if isinstance(part, dict):
                            if part.get('type') == 'text':
                                text_content.append(part.get('text', ''))
                            elif part.get('type') == 'file':
                                # Handle file attachments
                                file_info = f"[File: {part.get('fileIdentifier', 'unknown')}]"
                                text_content.append(file_info)
                        elif isinstance(part, str):
                            text_content.append(part)
                    
                    # For assistant messages, handle multi-step responses
                    if version.get('type') == 'multiStep' and 'steps' in version:
                        for step in version.get('steps', []):
                            if step.get('type') == 'contentBlock':
                                step_content = step.get('content', [])
                                for step_part in step_content:
                                    if isinstance(step_part, dict) and step_part.get('type') == 'text':
                                        text_content.append(step_part.get('text', ''))
                    
                    final_content = ' '.join(text_content).strip()
                    if final_content:
                        # Calculate approximate timestamp for each message
                        timestamp = None
                        if base_timestamp:
                            # Spread messages over time based on their position
                            message_time = base_timestamp + timedelta(minutes=i * 2)
                            timestamp = message_time.isoformat()
                        
                        conversations.append({
                            'role': role,
                            'content': final_content,
                            'timestamp': timestamp,
                            'metadata': {
                                'source': 'LM_Studio',
                                'model': data.get('lastUsedModel', {}).get('name', 'unknown'),
                                'conversation_name': data.get('name', 'Unknown'),
                                'version_index': current_version,
                                'message_index': i
                            }
                        })
        except Exception as e:
            logger.error(f"Error parsing LM Studio format: {e}")
        
        return conversations
    
    def _extract_ollama_database(self, db_path: str) -> List[Dict]:
        """Extract conversations from Ollama SQLite database."""
        conversations = []
        try:
            import sqlite3
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Get all chats with their messages
            cursor.execute("""
                SELECT c.id, c.title, c.created_at,
                       m.role, m.content, m.model_name, m.created_at as message_created_at
                FROM chats c
                LEFT JOIN messages m ON c.id = m.chat_id
                ORDER BY c.created_at, m.created_at
            """)
            
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                logger.debug(f"No conversations found in Ollama database: {db_path}")
                return conversations
            
            # Group messages by chat and convert to conversation format
            chats = {}
            for row in rows:
                chat_id, title, chat_created_at, role, content, model_name, msg_created_at = row
                
                if chat_id not in chats:
                    chats[chat_id] = {
                        'title': title,
                        'created_at': chat_created_at,
                        'messages': []
                    }
                
                if role and content:  # Only add if message exists
                    # Convert timestamp if needed
                    timestamp = None
                    if msg_created_at:
                        try:
                            # Parse ISO format timestamp
                            timestamp = datetime.fromisoformat(msg_created_at.replace('Z', '+00:00')).isoformat()
                        except (ValueError, TypeError):
                            pass
                    
                    conversations.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp,
                        'metadata': {
                            'source': 'Ollama',
                            'model': model_name or 'unknown',
                            'chat_id': chat_id,
                            'chat_title': title
                        }
                    })
            
            logger.info(f"Extracted {len(conversations)} messages from {len(chats)} Ollama chats")
            
        except Exception as e:
            logger.error(f"Error extracting Ollama database {db_path}: {e}")
        
        return conversations
    
    def _extract_openwebui_database(self, db_path: str) -> List[Dict]:
        """Extract conversations from OpenWebUI SQLite database."""
        conversations = []
        try:
            import sqlite3
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Get all chats with their messages
            cursor.execute("""
                SELECT c.id, c.title, c.created_at, c.updated_at,
                       m.id as message_id, m.role, m.content, m.created_at as message_created_at
                FROM chat c
                LEFT JOIN message m ON c.id = m.chat_id
                ORDER BY c.created_at, m.created_at
            """)
            
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                logger.debug(f"No conversations found in OpenWebUI database: {db_path}")
                return conversations
            
            # Convert to conversation format
            for row in rows:
                chat_id, title, chat_created_at, chat_updated_at, message_id, role, content, msg_created_at = row
                
                if role and content:  # Only add if message exists
                    # Convert timestamp if needed
                    timestamp = None
                    if msg_created_at:
                        try:
                            # Parse timestamp (OpenWebUI typically uses ISO format)
                            if isinstance(msg_created_at, (int, float)):
                                timestamp = datetime.fromtimestamp(msg_created_at).isoformat()
                            else:
                                timestamp = datetime.fromisoformat(str(msg_created_at).replace('Z', '+00:00')).isoformat()
                        except (ValueError, TypeError):
                            pass
                    
                    conversations.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp,
                        'metadata': {
                            'source': 'OpenWebUI',
                            'chat_id': chat_id,
                            'chat_title': title or f'Chat {chat_id}',
                            'message_id': message_id
                        }
                    })
            
            logger.info(f"Extracted {len(conversations)} messages from OpenWebUI database")
            
        except Exception as e:
            logger.error(f"Error extracting OpenWebUI database {db_path}: {e}")
        
        return conversations
    
    def _parse_text_format(self, content: str) -> List[Dict]:
        """Parse text-based conversation formats with timestamp detection"""
        conversations = []
        lines = content.split('\n')
        
        current_role = 'user'
        current_content = []
        current_timestamp = None
        
        # Common timestamp patterns
        timestamp_patterns = [
            r'\[(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?)\]',  # ISO format
            r'\[(\d{2}:\d{2}(?::\d{2})?)\]',  # Time only
            r'\[(\d{4}-\d{2}-\d{2})\]',  # Date only
        ]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Try to extract timestamp
            for pattern in timestamp_patterns:
                match = re.match(pattern, line)
                if match:
                    try:
                        ts = match.group(1)
                        # Handle time-only format by adding today's date
                        if re.match(r'\d{2}:\d{2}(?::\d{2})?$', ts):
                            ts = f"{datetime.now().date()}T{ts}"
                        current_timestamp = datetime.fromisoformat(ts).isoformat()
                        line = line[match.end():].strip()
                        break
                    except (ValueError, TypeError):
                        continue
            
            # Detect role markers
            if line.lower().startswith(('user:', 'human:', 'me:')):
                if current_content:
                    conversations.append({
                        'role': current_role,
                        'content': '\n'.join(current_content),
                        'timestamp': current_timestamp
                    })
                    current_content = []
                current_role = 'user'
                content_part = line.split(':', 1)[1].strip() if ':' in line else line
                if content_part:
                    current_content.append(content_part)
                    
            elif line.lower().startswith(('assistant:', 'ai:', 'bot:', 'friday:')):
                if current_content:
                    conversations.append({
                        'role': current_role,
                        'content': '\n'.join(current_content)
                    })
                    current_content = []
                current_role = 'assistant'
                content_part = line.split(':', 1)[1].strip() if ':' in line else line
                if content_part:
                    current_content.append(content_part)
            else:
                current_content.append(line)
        
        # Add the last conversation
        if current_content:
            conversations.append({
                'role': current_role,
                'content': '\n'.join(current_content)
            })
        
        return conversations

    def _parse_sillytavern_format(self, data: Dict) -> List[Dict]:
        """Parse SillyTavern conversation format"""
        conversations = []
        
        try:
            # SillyTavern typically stores chats in nested format
            if 'messages' in data:
                for msg in data['messages']:
                    # SillyTavern message format
                    role = 'user' if msg.get('is_user', False) else 'assistant'
                    content = msg.get('mes', msg.get('message', ''))
                    timestamp = msg.get('send_date', msg.get('timestamp'))
                    
                    if content:
                        conversations.append({
                            'role': role,
                            'content': str(content),
                            'timestamp': self.parse_timestamp(timestamp),
                            'metadata': {'source_type': 'sillytavern'}
                        })
            
            # Alternative format for SillyTavern exports
            elif 'chat' in data and isinstance(data['chat'], list):
                for msg in data['chat']:
                    conversations.append({
                        'role': 'user' if msg.get('is_user') else 'assistant',
                        'content': str(msg.get('mes', '')),
                        'timestamp': self.parse_timestamp(msg.get('send_date')),
                        'metadata': {'source_type': 'sillytavern'}
                    })
                    
        except Exception as e:
            logger.warning(f"Error parsing SillyTavern format: {e}")
        
        return conversations

    def _parse_gemini_cli_format(self, data: Dict) -> List[Dict]:
        """Parse Gemini CLI conversation format"""
        conversations = []
        
        try:
            # Gemini CLI format (assuming similar to other CLI tools)
            if 'conversation' in data and isinstance(data['conversation'], list):
                for turn in data['conversation']:
                    # User input
                    if 'input' in turn:
                        conversations.append({
                            'role': 'user',
                            'content': str(turn['input']),
                            'timestamp': self.parse_timestamp(turn.get('timestamp')),
                            'metadata': {'source_type': 'gemini_cli'}
                        })
                    
                    # Assistant response
                    if 'response' in turn:
                        conversations.append({
                            'role': 'assistant',
                            'content': str(turn['response']),
                            'timestamp': self.parse_timestamp(turn.get('timestamp')),
                            'metadata': {'source_type': 'gemini_cli'}
                        })
            
            # Alternative format with messages array
            elif 'messages' in data:
                for msg in data['messages']:
                    role = msg.get('role', 'user')
                    if role == 'model':  # Gemini uses 'model' instead of 'assistant'
                        role = 'assistant'
                    
                    content = ''
                    if 'parts' in msg and isinstance(msg['parts'], list):
                        # Gemini format with parts array
                        content = ' '.join(str(part.get('text', part)) for part in msg['parts'])
                    else:
                        content = str(msg.get('content', msg.get('text', '')))
                    
                    if content:
                        conversations.append({
                            'role': role,
                            'content': content,
                            'timestamp': self.parse_timestamp(msg.get('timestamp')),
                            'metadata': {'source_type': 'gemini_cli'}
                        })
                        
        except Exception as e:
            logger.warning(f"Error parsing Gemini CLI format: {e}")
        
        return conversations


class EmbeddingService:
    """Intelligent embedding service that preserves existing embeddings while optimizing for quality"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize embedding service with intelligent configuration
        
        Args:
            config: Optional configuration dictionary. If None, loads from embedding_config.json
        """
        if config:
            self.primary_config = config
            self.fallback_config = config.get("fallback", {})
            self.full_config = {"primary": config, "fallback": self.fallback_config}
        else:
            self.full_config = self._load_full_config()
            self.primary_config = self.full_config.get("primary", {})
            self.fallback_config = self.full_config.get("fallback", {})
            
        self.provider_availability = {
            "lm_studio": None,  # Will be tested on first use
            "ollama": None,
            "openai": None
        }
        
        print("üîß Intelligent Embedding Service Configuration")
        primary_provider = self.primary_config.get('provider', 'lm_studio')
        primary_model = self.primary_config.get('model', 'text-embedding-nomic-embed-text-v1.5')
        fallback_provider = self.fallback_config.get('provider', 'ollama')
        fallback_model = self.fallback_config.get('model', 'nomic-embed-text')
        
        print(f"‚úÖ Primary: {primary_provider} ({primary_model})")
        print(f"‚ö° Fallback: {fallback_provider} ({fallback_model})")
        print(f"üíæ Preserving existing 768D embeddings, using best available for new ones")
        print("To customize, edit embedding_config.json in the project directory")
        # Provide a simple embeddings endpoint value for health reporting
        self.embeddings_endpoint = self.primary_config.get("base_url") or f"local://{self.primary_config.get('model', 'local')}"
    
    @property
    def config(self) -> Dict[str, Any]:
        """Backward compatibility property - returns primary config as expected format"""
        return {
            "provider": self.primary_config.get("provider"),
            "model": self.primary_config.get("model"),
            "base_url": self.primary_config.get("base_url"),
            "api_key": self.primary_config.get("api_key"),
            "fallback_provider": self.fallback_config.get("provider"),
            "fallback_model": self.fallback_config.get("model"),
            "fallback_base_url": self.fallback_config.get("base_url"),
            "fallback_api_key": self.fallback_config.get("api_key")
        }
    
    def _load_full_config(self) -> dict:
        """Load complete embedding configuration from JSON file"""
        try:
            config_path = Path(__file__).parent / "embedding_config.json"
            if config_path.exists():
                with open(config_path, 'r') as f:
                    config_data = json.load(f)
                    return config_data.get("embedding_configuration", {})
        except Exception as e:
            logger.warning(f"Failed to load embedding config: {e}, using defaults")
        
        # Return default configuration
        return {
            "primary": {
                "provider": "lm_studio",
                "model": "text-embedding-nomic-embed-text-v1.5",
                "base_url": "http://localhost:1234",
                "description": "High-quality LM Studio embeddings for semantic search"
            },
            "fallback": {
                "provider": "ollama",
                "model": "nomic-embed-text", 
                "base_url": "http://localhost:11434",
                "description": "Fast local Ollama embeddings"
            }
        }
    
    @classmethod
    def create_with_user_config(cls) -> 'EmbeddingService':
        """Create embedding service with user configuration prompt"""
        try:
            print("üîß Embedding Service Configuration")
            print("Loading configuration from embedding_config.json...")
            return cls()  # Use config file
            
        except Exception as e:
            logger.warning(f"Failed to get user config, using defaults: {e}")
            return cls()  # Fallback to defaults

    async def generate_embedding(self, text: str, model: str = None) -> List[float]:
        """Generate embedding using intelligent provider selection with preservation strategy"""
        
        # Try primary provider first
        primary_provider = self.primary_config.get("provider", "lm_studio")
        
        try:
            if primary_provider == "lm_studio":
                result = await self._generate_lm_studio_embedding(text)
                if result:
                    self.provider_availability["lm_studio"] = True
                    return result
                else:
                    self.provider_availability["lm_studio"] = False
                    logger.warning("LM Studio unavailable, trying fallback")
                    
            elif primary_provider == "ollama":
                result = await self._generate_ollama_embedding(text)
                if result:
                    self.provider_availability["ollama"] = True
                    return result
                else:
                    self.provider_availability["ollama"] = False
                    logger.warning("Ollama unavailable, trying fallback")
                    
            elif primary_provider == "openai":
                result = await self._generate_openai_embedding(text)
                if result:
                    self.provider_availability["openai"] = True
                    return result
                else:
                    self.provider_availability["openai"] = False
                    logger.warning("OpenAI unavailable, trying fallback")

            elif primary_provider == "local":
                # Deterministic local embeddings for offline/test environments
                result = await self._generate_local_embedding(text)
                if result:
                    self.provider_availability["local"] = True
                    return result
                else:
                    self.provider_availability["local"] = False
                    logger.warning("Local embedding provider failed, trying fallback")
                    
        except Exception as e:
            logger.warning(f"Primary provider {primary_provider} failed: {e}")
        
        # Try fallback provider
        fallback_provider = self.fallback_config.get("provider")
        if fallback_provider and fallback_provider != primary_provider:
            try:
                if fallback_provider == "local":
                    result = await self._generate_local_embedding(text)
                    if result:
                        self.provider_availability["local"] = True
                        logger.info("Using local fallback for embedding")
                        return result
                    else:
                        self.provider_availability["local"] = False
                        logger.warning("Local fallback failed")

                if fallback_provider == "lm_studio":
                    result = await self._generate_lm_studio_embedding(text, fallback=True)
                    if result:
                        self.provider_availability["lm_studio"] = True
                        logger.info("Using LM Studio fallback for embedding")
                        return result
                        
                elif fallback_provider == "ollama":
                    result = await self._generate_ollama_embedding(text, fallback=True)
                    if result:
                        self.provider_availability["ollama"] = True
                        logger.info("Using Ollama fallback for embedding")
                        return result
                        
                elif fallback_provider == "openai":
                    result = await self._generate_openai_embedding(text, fallback=True)
                    if result:
                        self.provider_availability["openai"] = True
                        logger.info("Using OpenAI fallback for embedding")
                        return result
                        
            except Exception as e:
                logger.error(f"Fallback provider {fallback_provider} also failed: {e}")
        
        # If both primary and fallback fail, log the issue
        logger.error("All embedding providers failed - semantic search will be unavailable")
        return []
    
    async def _generate_ollama_embedding(self, text: str, fallback: bool = False) -> List[float]:
        """Generate embedding using Ollama"""
        if fallback:
            config = self.fallback_config
        else:
            config = self.primary_config if self.primary_config.get("provider") == "ollama" else self.fallback_config
            
        base_url = config.get("base_url", "http://localhost:11434")
        model = config.get("model", "nomic-embed-text")
        
        async with aiohttp.ClientSession() as session:
            payload = {"model": model, "prompt": text}
            async with session.post(f"{base_url}/api/embeddings", json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("embedding")
                else:
                    error_text = await response.text()
                    logger.error(f"Ollama API error {response.status}: {error_text}")
                    return None
    
    async def _generate_lm_studio_embedding(self, text: str, fallback: bool = False) -> List[float]:
        """Generate embedding using LM Studio"""
        if fallback:
            config = self.fallback_config
        else:
            config = self.primary_config if self.primary_config.get("provider") == "lm_studio" else self.fallback_config
            
        base_url = config.get("base_url", "http://localhost:1234")
        model = config.get("model", "text-embedding-nomic-embed-text-v1.5")
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {"model": model, "input": text}
                async with session.post(f"{base_url}/v1/embeddings", json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data and "data" in data and len(data["data"]) > 0:
                            embedding = data["data"][0].get("embedding")
                            if embedding:
                                return embedding
                        logger.error(f"Invalid LM Studio response format: {data}")
                        return None
                    else:
                        error_text = await response.text()
                        logger.error(f"LM Studio API error {response.status}: {error_text}")
                        return None
        except Exception as e:
            logger.error(f"LM Studio embedding error: {e}")
            return None
    
    async def _generate_openai_embedding(self, text: str, fallback: bool = False) -> List[float]:
        """Generate embedding using OpenAI"""
        if fallback:
            config = self.fallback_config
        else:
            config = self.primary_config if self.primary_config.get("provider") == "openai" else self.fallback_config
            
        api_key = config.get("api_key")
        if not api_key or api_key == "your-openai-api-key-here":
            logger.error("OpenAI API key not configured")
            return None
            
        model = config.get("model", "text-embedding-3-small")
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {"Authorization": f"Bearer {api_key}"}
                payload = {"model": model, "input": text}
                async with session.post("https://api.openai.com/v1/embeddings", 
                                        json=payload, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data and "data" in data and len(data["data"]) > 0:
                            return data["data"][0].get("embedding")
                        return None
                    else:
                        error_text = await response.text()
                        logger.error(f"OpenAI API error {response.status}: {error_text}")
                        return None
        except Exception as e:
            logger.error(f"OpenAI embedding error: {e}")
            return None

    async def _generate_local_embedding(self, text: str) -> List[float]:
        """Generate a deterministic local embedding for offline testing.

        This uses a simple hash-based approach to produce a fixed-size
        float vector so semantic-search flows and health checks remain functional
        during tests without external services.
        """
        try:
            import hashlib

            # Create a reproducible 128-dimension embedding from SHA256
            h = hashlib.sha256(text.encode('utf-8')).digest()
            # Expand to 128 floats by repeating the digest if necessary
            vec = []
            while len(vec) < 128:
                for i in range(0, len(h), 4):
                    if len(vec) >= 128:
                        break
                    chunk = h[i:i+4]
                    # Convert 4 bytes to uint32 and normalize
                    val = int.from_bytes(chunk, 'big', signed=False)
                    vec.append(((val % 100000) / 100000.0) - 0.5)
                # Rehash to get more entropy
                h = hashlib.sha256(h).digest()

            return vec[:128]
        except Exception as e:
            logger.error(f"Local embedding generator error: {e}")
            return None


class PersistentAIMemorySystem:
    """Main memory system that coordinates all databases and operations - FULL FEATURED VERSION"""
    
    def __init__(self, data_dir: str = "memory_data", enable_file_monitoring: bool = True, 
                 watch_directories: List[str] = None):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # Initialize all 5 databases
        self.conversations_db = ConversationDatabase(str(self.data_dir / "conversations.db"))
        self.ai_memory_db = AIMemoryDatabase(str(self.data_dir / "ai_memories.db"))
        self.schedule_db = ScheduleDatabase(str(self.data_dir / "schedule.db"))
        self.vscode_db = VSCodeProjectDatabase(str(self.data_dir / "vscode_project.db"))
        self.mcp_db = MCPToolCallDatabase(str(self.data_dir / "mcp_tool_calls.db"))
        
        # Initialize embedding service with user-configurable options
        self.embedding_service = EmbeddingService.create_with_user_config()
        
        # Initialize file monitoring
        self.file_monitor = None
        if enable_file_monitoring:
            self.file_monitor = ConversationFileMonitor(self, watch_directories)
        # Compatibility alias used in some tests/examples
        self.conversation_monitor = self.file_monitor
    
    async def start_file_monitoring(self):
        """Start monitoring conversation files"""
        if self.file_monitor:
            await self.file_monitor.start_monitoring()
            logger.info("File monitoring started")
    
    async def stop_file_monitoring(self):
        """Stop monitoring conversation files"""
        if self.file_monitor:
            await self.file_monitor.stop_monitoring()
            logger.info("File monitoring stopped")
    
    def add_watch_directory(self, directory: str):
        """Add a directory to monitor for conversation files"""
        if self.file_monitor:
            self.file_monitor.add_watch_directory(directory)

    # =============================================================================
    # CONVERSATION OPERATIONS
    # =============================================================================
    
    async def store_conversation(self, content: str, role: str, session_id: str = None,
                               conversation_id: str = None, metadata: Dict = None) -> Dict:
        """Store a conversation message with automatic embedding generation"""
        
        result = await self.conversations_db.store_message(
            content, role, session_id, conversation_id, metadata
        )
        
        # Generate and store embedding asynchronously
        asyncio.create_task(self._add_embedding_to_message(result["message_id"], content))
        
        return {
            "status": "success",
            "message_id": result["message_id"],
            "conversation_id": result["conversation_id"],
            "session_id": result["session_id"]
        }
    
    async def get_conversation_history(self, limit: int = 20, session_id: str = None) -> List[Dict]:
        """Get recent conversation history"""
        
        messages = await self.conversations_db.get_recent_messages(limit, session_id)
        return [dict(msg) for msg in messages]

    async def get_recent_context(self, limit: int = 10, session_id: str = None) -> Dict:
        """Retrieve recent conversation context, optionally filtered by session."""
        try:
            if session_id:
                query = """
                    SELECT m.*, c.session_id 
                    FROM messages m 
                    JOIN conversations c ON m.conversation_id = c.conversation_id
                    WHERE c.session_id = ?
                    ORDER BY m.timestamp DESC 
                    LIMIT ?
                """
                params = (session_id, limit)
            else:
                query = """
                    SELECT m.*, c.session_id 
                    FROM messages m 
                    JOIN conversations c ON m.conversation_id = c.conversation_id
                    ORDER BY m.timestamp DESC 
                    LIMIT ?
                """
                params = (limit,)

            rows = await self.conversations_db.execute_query(query, params)
            return {
                "status": "success",
                "recent_context": [dict(row) for row in rows]
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e)
            }

    # =============================================================================
    # AI MEMORY OPERATIONS
    # =============================================================================
    
    async def create_memory(self, content: str, memory_type: str = None,
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> Dict:
        """Create a curated AI memory with automatic embedding generation"""
        
        memory_id = await self.ai_memory_db.create_memory(
            content, memory_type, importance_level, tags, source_conversation_id
        )
        
        # Generate and store embedding asynchronously
        asyncio.create_task(self._add_embedding_to_memory(memory_id, content))
        
        return {
            "status": "success",
            "memory_id": memory_id
        }

    # =============================================================================
    # SCHEDULE OPERATIONS
    # =============================================================================
    
    async def create_appointment(self, title: str, scheduled_datetime: str, 
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> Dict:
        """Create an appointment with automatic embedding generation"""
        
        appointment_id = await self.schedule_db.create_appointment(
            title, scheduled_datetime, description, location, source_conversation_id
        )
        
        # Generate embedding for search (combine title and description)
        content_for_embedding = f"{title}"
        if description:
            content_for_embedding += f" {description}"
        
        asyncio.create_task(self._add_embedding_to_appointment(appointment_id, content_for_embedding))
        
        return {
            "status": "success",
            "appointment_id": appointment_id
        }
    
    async def create_reminder(self, content: str, due_datetime: str, 
                            priority_level: int = 5, source_conversation_id: str = None) -> Dict:
        """Create a reminder with automatic embedding generation"""
        
        reminder_id = await self.schedule_db.create_reminder(
            content, due_datetime, priority_level, source_conversation_id
        )
        
        # Generate and store embedding for the reminder content
        asyncio.create_task(self._add_embedding_to_reminder(reminder_id, content))
        
        return {
            "status": "success",
            "reminder_id": reminder_id
        }
    
    async def get_upcoming_schedule(self, days_ahead: int = 7) -> Dict:
        """Get upcoming appointments and reminders"""
        
        appointments = await self.schedule_db.get_upcoming_appointments(days_ahead)
        reminders = await self.schedule_db.get_active_reminders()
        
        return {
            "status": "success",
            "appointments": appointments,
            "active_reminders": reminders,
            "period_days": days_ahead
        }

    # =============================================================================
    # VSCODE PROJECT OPERATIONS
    # =============================================================================
    
    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> Dict:
        """Save development session"""
        
        session_id = await self.vscode_db.save_development_session(
            workspace_path, active_files, git_branch, session_summary
        )
        
        return {
            "status": "success",
            "session_id": session_id
        }
    
    async def store_project_insight(self, content: str, insight_type: str = None,
                                  related_files: List[str] = None, importance_level: int = 5,
                                  source_conversation_id: str = None) -> Dict:
        """Store project insight with automatic embedding generation"""
        
        insight_id = await self.vscode_db.store_project_insight(
            content, insight_type, related_files, importance_level, source_conversation_id
        )
        
        # Generate and store embedding for the insight content
        asyncio.create_task(self._add_embedding_to_project_insight(insight_id, content))
        
        return {
            "status": "success",
            "insight_id": insight_id
        }

    # =============================================================================
    # MCP TOOL CALL OPERATIONS
    # =============================================================================
    
    async def log_tool_call(self, tool_name: str, parameters: Dict = None,
                          execution_time_ms: float = None, status: str = "success",
                          result: Any = None, error_message: str = None, client_id: str = None) -> str:
        """Log an MCP tool call for analysis and debugging"""
        
        return await self.mcp_db.log_tool_call(
            tool_name, parameters, result, status, execution_time_ms, error_message, client_id
        )
    
    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """Get comprehensive tool usage summary"""
        
        return await self.mcp_db.get_tool_usage_summary(days)

    # =============================================================================
    # ADVANCED SEARCH OPERATIONS
    # =============================================================================
    
    async def search_project_history(self, query: str, limit: int = 10) -> Dict:
        """Search project development history including conversations and insights.
        
        Args:
            query: Search query string
            limit: Maximum number of results
            
        Returns:
            Dict containing search results from project context
        """
        query_embedding = await self.embedding_service.generate_embedding(query)
        if not query_embedding:
            return await self._text_based_project_search(query, limit)
            
        results = []
        
        # Search development conversations
        conv_results = await self._search_development_conversations(query_embedding, limit)
        results.extend(conv_results)
        
        # Search project insights
        insight_results = await self._search_project_insights(query_embedding, limit)
        results.extend(insight_results)
        
        # Search code context
        context_results = await self._search_code_context(query_embedding, limit)
        results.extend(context_results)
        
        # Sort by relevance and return
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return {
            "status": "success",
            "query": query,
            "results": results[:limit],
            "count": len(results[:limit])
        }
        
    async def link_code_context(self, file_path: str, description: str,
                              function_name: str = None, conversation_id: str = None) -> Dict:
        """Link conversation context to specific code location.
        
        Args:
            file_path: Path to the code file
            description: Description of the code context
            function_name: Optional function/method name
            conversation_id: Optional related conversation ID
            
        Returns:
            Dict containing the created context link
        """
        context_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.vscode_db.execute_update(
            """INSERT INTO code_context 
               (context_id, timestamp, file_path, function_name, description)
               VALUES (?, ?, ?, ?, ?)""",
            (context_id, timestamp, file_path, function_name, description)
        )
        
        if conversation_id:
            await self.vscode_db.execute_update(
                """UPDATE development_conversations
                   SET chat_context_id = ?
                   WHERE conversation_id = ?""",
                (context_id, conversation_id)
            )
            
        # Generate embedding for search
        asyncio.create_task(self._add_embedding_to_code_context(context_id, description))
        
        return {
            "status": "success",
            "context_id": context_id
        }
        
    async def get_project_continuity(self, workspace_path: str = None, limit: int = 5) -> Dict:
        """Get context for continuing development work.
        
        Args:
            workspace_path: Optional workspace path filter
            limit: Maximum number of context items
            
        Returns:
            Dict containing recent development context
        """
        # Get recent development sessions
        sessions_query = """
            SELECT * FROM project_sessions
            WHERE end_timestamp IS NULL
        """
        if workspace_path:
            sessions_query += " AND workspace_path = ?"
            sessions = await self.vscode_db.execute_query(
                sessions_query + " ORDER BY start_timestamp DESC LIMIT ?",
                (workspace_path, limit)
            )
        else:
            sessions = await self.vscode_db.execute_query(
                sessions_query + " ORDER BY start_timestamp DESC LIMIT ?",
                (limit,)
            )
            
        # Get associated conversations and insights
        context = {
            "active_sessions": [dict(session) for session in sessions],
            "recent_conversations": [],
            "relevant_insights": []
        }
        
        for session in sessions:
            # Get conversations for this session
            convs = await self.vscode_db.execute_query(
                """SELECT * FROM development_conversations
                   WHERE session_id = ?
                   ORDER BY timestamp DESC LIMIT ?""",
                (session["session_id"], limit)
            )
            context["recent_conversations"].extend([dict(conv) for conv in convs])
            
            # Get insights mentioning active files
            if session["active_files"]:
                active_files = json.loads(session["active_files"])
                for file in active_files:
                    insights = await self.vscode_db.execute_query(
                        """SELECT * FROM project_insights
                           WHERE related_files LIKE ?
                           ORDER BY timestamp_created DESC LIMIT ?""",
                        (f"%{file}%", limit)
                    )
                    context["relevant_insights"].extend([dict(insight) for insight in insights])
        
        return {
            "status": "success",
            "context": context
        }
            
    async def search_memories(self, query: str, limit: int = 10, 
                            min_importance: int = None, max_importance: int = None,
                            memory_type: str = None, database_filter: str = "all") -> Dict:
        """Advanced semantic search across all databases with filtering"""
        
        # Generate embedding for the search query
        query_embedding = await self.embedding_service.generate_embedding(query)
        if not query_embedding:
            # Fallback to text-based search if embedding fails
            return await self._text_based_search(query, limit, database_filter, min_importance, max_importance, memory_type)
        
        all_results = []
        
        # Search AI memories
        if database_filter in ["all", "ai_memories"]:
            memory_results = await self._search_ai_memories(query_embedding, limit, min_importance, max_importance, memory_type)
            all_results.extend(memory_results)
        
        # Search conversations
        if database_filter in ["all", "conversations"]:
            conversation_results = await self._search_conversations(query_embedding, limit)
            all_results.extend(conversation_results)
        
        # Search schedule items
        if database_filter in ["all", "schedule"]:
            schedule_results = await self._search_schedule(query_embedding, limit)
            all_results.extend(schedule_results)
        
        # Search project insights
        if database_filter in ["all", "projects"]:
            project_results = await self._search_project_insights(query_embedding, limit)
            all_results.extend(project_results)
        
        # Sort all results by similarity score and return top results
        all_results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        return {
            "status": "success",
            "query": query,
            "results": all_results[:limit],
            "count": len(all_results[:limit]),
            "search_type": "semantic" if query_embedding else "text_based"
        }

    # =============================================================================
    # SYSTEM HEALTH AND MONITORING
    # =============================================================================
    
    async def get_system_health(self) -> Dict:
        """Get comprehensive system health and statistics"""
        health_data = {
            "status": "healthy",
            "timestamp": get_current_timestamp(),
            "databases": {},
            "file_monitoring": {},
            "embedding_service": {}
        }
        
        try:
            # Check conversations database
            conversations_count = await self.conversations_db.execute_query(
                "SELECT COUNT(*) as count FROM messages"
            )
            sessions_count = await self.conversations_db.execute_query(
                "SELECT COUNT(*) as count FROM sessions"
            )
            health_data["databases"]["conversations"] = {
                "status": "healthy",
                "message_count": conversations_count[0]["count"] if conversations_count else 0,
                "session_count": sessions_count[0]["count"] if sessions_count else 0,
                "database_path": self.conversations_db.db_path
            }
            
            # Check AI memories database
            memories_count = await self.ai_memory_db.execute_query(
                "SELECT COUNT(*) as count FROM curated_memories"
            )
            high_importance_count = await self.ai_memory_db.execute_query(
                "SELECT COUNT(*) as count FROM curated_memories WHERE importance_level >= 7"
            )
            health_data["databases"]["ai_memories"] = {
                "status": "healthy",
                "memory_count": memories_count[0]["count"] if memories_count else 0,
                "high_importance_count": high_importance_count[0]["count"] if high_importance_count else 0,
                "database_path": self.ai_memory_db.db_path
            }
            
            # Check schedule database
            appointments_count = await self.schedule_db.execute_query(
                "SELECT COUNT(*) as count FROM appointments"
            )
            reminders_count = await self.schedule_db.execute_query(
                "SELECT COUNT(*) as count FROM reminders"
            )
            health_data["databases"]["schedule"] = {
                "status": "healthy",
                "appointment_count": appointments_count[0]["count"] if appointments_count else 0,
                "reminder_count": reminders_count[0]["count"] if reminders_count else 0,
                "database_path": self.schedule_db.db_path
            }
            
            # Check VS Code project database
            project_sessions_count = await self.vscode_db.execute_query(
                "SELECT COUNT(*) as count FROM project_sessions"
            )
            insights_count = await self.vscode_db.execute_query(
                "SELECT COUNT(*) as count FROM project_insights"
            )
            health_data["databases"]["vscode_project"] = {
                "status": "healthy",
                "session_count": project_sessions_count[0]["count"] if project_sessions_count else 0,
                "insight_count": insights_count[0]["count"] if insights_count else 0,
                "database_path": self.vscode_db.db_path
            }
            
            # Check MCP tool calls database
            tool_calls_count = await self.mcp_db.execute_query(
                "SELECT COUNT(*) as count FROM tool_calls"
            )
            health_data["databases"]["mcp_tool_calls"] = {
                "status": "healthy",
                "total_tool_calls": tool_calls_count[0]["count"] if tool_calls_count else 0,
                "database_path": self.mcp_db.db_path
            }
            
            # Check file monitoring status
            if self.file_monitor:
                health_data["file_monitoring"] = {
                    "status": "enabled",
                    "watch_directories": len(self.file_monitor.watch_directories),
                    "directories": self.file_monitor.watch_directories
                }
            else:
                health_data["file_monitoring"] = {
                    "status": "disabled",
                    "message": "File monitoring is not enabled"
                }
            
            # Check embedding service
            try:
                # Try a simple ping to the embedding service
                test_embedding = await self.embedding_service.generate_embedding("test")
                if test_embedding:
                    health_data["embedding_service"] = {
                        "status": "healthy",
                        "endpoint": self.embedding_service.embeddings_endpoint,
                        "embedding_dimensions": len(test_embedding)
                    }
                else:
                    health_data["embedding_service"] = {
                        "status": "unhealthy",
                        "endpoint": self.embedding_service.embeddings_endpoint,
                        "error": "Failed to generate test embedding"
                    }
            except Exception as e:
                health_data["embedding_service"] = {
                    "status": "unhealthy",
                    "endpoint": self.embedding_service.embeddings_endpoint,
                    "error": str(e)
                }
            
            # Overall system status
            unhealthy_components = []
            if health_data["embedding_service"]["status"] == "unhealthy":
                unhealthy_components.append("embedding_service")
            
            if unhealthy_components:
                health_data["status"] = "degraded"
                health_data["issues"] = unhealthy_components
            
        except Exception as e:
            health_data["status"] = "error"
            health_data["error"] = str(e)
            logger.error(f"Error getting system health: {e}")
        
        return health_data

    # =============================================================================
    # INTERNAL HELPER METHODS
    # =============================================================================
    
    async def _search_ai_memories(self, query_embedding: List[float], limit: int,
                                min_importance: int = None, max_importance: int = None,
                                memory_type: str = None) -> List[Dict]:
        """Search AI curated memories using semantic similarity"""
        
        # Build SQL query with optional filters
        sql = "SELECT * FROM curated_memories WHERE embedding IS NOT NULL"
        params = []
        
        if min_importance is not None:
            sql += " AND importance_level >= ?"
            params.append(min_importance)
            
        if max_importance is not None:
            sql += " AND importance_level <= ?"
            params.append(max_importance)
            
        if memory_type is not None:
            sql += " AND memory_type = ?"
            params.append(memory_type)
        
        rows = await self.ai_memory_db.execute_query(sql, params)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:  # Threshold for relevance
                    result = {
                        "type": "ai_memory",
                        "similarity_score": similarity,
                        "data": {
                            "memory_id": row["memory_id"],
                            "content": row["content"],
                            "importance_level": row["importance_level"],
                            "memory_type": row["memory_type"],
                            "timestamp_created": row["timestamp_created"],
                            "tags": json.loads(row["tags"]) if row["tags"] else []
                        }
                    }
                    results.append(result)
        
        # Boost results based on importance level
        for result in results:
            importance_boost = result["data"]["importance_level"] / 10.0 * 0.1
            result["similarity_score"] += importance_boost
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_conversations(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search conversation messages using semantic similarity"""
        
        query = """
            SELECT message_id, conversation_id, timestamp, role, content, metadata, embedding
            FROM messages 
            WHERE embedding IS NOT NULL
            ORDER BY timestamp DESC
            LIMIT 1000
        """
        
        rows = await self.conversations_db.execute_query(query)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "conversation",
                        "similarity_score": similarity,
                        "data": {
                            "message_id": row["message_id"],
                            "conversation_id": row["conversation_id"],
                            "timestamp": row["timestamp"],
                            "role": row["role"],
                            "content": row["content"],
                            "metadata": json.loads(row["metadata"]) if row["metadata"] else None
                        }
                    }
                    results.append(result)
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_schedule(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search appointments and reminders using semantic similarity"""
        
        results = []
        
        # Search appointments
        appointment_query = """
            SELECT appointment_id, timestamp_created, scheduled_datetime, title, 
                   description, location, source_conversation_id, embedding
            FROM appointments 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.schedule_db.execute_query(appointment_query)
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "appointment",
                        "similarity_score": similarity,
                        "data": {
                            "appointment_id": row["appointment_id"],
                            "scheduled_datetime": row["scheduled_datetime"],
                            "title": row["title"],
                            "description": row["description"],
                            "location": row["location"]
                        }
                    }
                    results.append(result)
        
        # Search reminders
        reminder_query = """
            SELECT reminder_id, timestamp_created, due_datetime, content, 
                   priority_level, completed, source_conversation_id, embedding
            FROM reminders 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.schedule_db.execute_query(reminder_query)
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "reminder",
                        "similarity_score": similarity,
                        "data": {
                            "reminder_id": row["reminder_id"],
                            "due_datetime": row["due_datetime"],
                            "content": row["content"],
                            "priority_level": row["priority_level"],
                            "completed": bool(row["completed"])
                        }
                    }
                    results.append(result)
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_project_insights(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search project insights using semantic similarity"""
        
        query = """
            SELECT insight_id, timestamp_created, insight_type, content, 
                   related_files, importance_level, embedding
            FROM project_insights 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.vscode_db.execute_query(query)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "project_insight",
                        "similarity_score": similarity,
                        "data": {
                            "insight_id": row["insight_id"],
                            "timestamp_created": row["timestamp_created"],
                            "insight_type": row["insight_type"],
                            "content": row["content"],
                            "related_files": json.loads(row["related_files"]) if row["related_files"] else None,
                            "importance_level": row["importance_level"]
                        }
                    }
                    results.append(result)
        
        # Boost results based on importance level
        for result in results:
            importance_boost = result["data"]["importance_level"] / 10.0 * 0.15
            result["similarity_score"] += importance_boost
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    def _calculate_cosine_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings"""
        
        try:
            # Convert to numpy arrays
            vec1 = np.array(embedding1, dtype=np.float32)
            vec2 = np.array(embedding2, dtype=np.float32)
            
            # Calculate cosine similarity
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {e}")
            return 0.0
    
    async def _text_based_search(self, query: str, limit: int, database_filter: str,
                               min_importance: int = None, max_importance: int = None,
                               memory_type: str = None) -> Dict:
        """Fallback text-based search when embeddings are unavailable"""
        
        query_words = query.lower().split()
        results = []
        
        if database_filter in ["all", "ai_memories"]:
            # Search AI memories with text matching and filters
            sql = "SELECT * FROM curated_memories WHERE 1=1"
            params = []
            
            # Add content search
            content_conditions = []
            for word in query_words:
                content_conditions.append("LOWER(content) LIKE ?")
                params.append(f"%{word}%")
            
            if content_conditions:
                sql += f" AND ({' OR '.join(content_conditions)})"
            
            # Add importance filters
            if min_importance is not None:
                sql += " AND importance_level >= ?"
                params.append(min_importance)
                
            if max_importance is not None:
                sql += " AND importance_level <= ?"
                params.append(max_importance)
                
            if memory_type is not None:
                sql += " AND memory_type = ?"
                params.append(memory_type)
            
            sql += " ORDER BY importance_level DESC LIMIT ?"
            params.append(limit)
            
            rows = await self.ai_memory_db.execute_query(sql, params)
            for row in rows:
                results.append({
                    "type": "ai_memory",
                    "similarity_score": 0.5,
                    "data": dict(row)
                })
        
        if database_filter in ["all", "conversations"]:
            # Search conversations with text matching
            for word in query_words:
                rows = await self.conversations_db.execute_query(
                    "SELECT * FROM messages WHERE LOWER(content) LIKE ? ORDER BY timestamp DESC LIMIT ?",
                    (f"%{word}%", limit)
                )
                for row in rows:
                    results.append({
                        "type": "conversation",
                        "similarity_score": 0.5,
                        "data": dict(row)
                    })
        
        # Remove duplicates and limit results
        seen = set()
        unique_results = []
        for result in results:
            key = f"{result['type']}_{result['data'].get('memory_id', result['data'].get('message_id', ''))}"
            if key not in seen:
                seen.add(key)
                unique_results.append(result)
        
        return {
            "status": "success",
            "query": query,
            "results": unique_results[:limit],
            "count": len(unique_results[:limit]),
            "search_type": "text_based",
            "note": "Used text-based search (embeddings unavailable)"
        }
    
    # SillyTavern-specific methods
    async def get_character_context(self, character_name: str, context_type: str = None, limit: int = 5) -> Dict:
        """Get relevant context about characters from memory"""
        try:
            # Search for character-specific memories
            query = f"character {character_name}"
            if context_type:
                query += f" {context_type}"
            
            # Search memories and conversations
            memories = await self.search_memories(query, limit=limit)
            
            # Filter and format results
            character_context = {
                "character_name": character_name,
                "context_type": context_type,
                "memories": memories.get("results", []),
                "total_found": len(memories.get("results", [])),
                "timestamp": get_current_timestamp()
            }
            
            logger.info(f"Retrieved {len(memories.get('results', []))} memories for character: {character_name}")
            return character_context
            
        except Exception as e:
            logger.error(f"Error getting character context: {e}")
            return {"error": str(e), "character_name": character_name}

    async def store_roleplay_memory(self, character_name: str, event_description: str, 
                                    importance_level: int = 5, tags: List[str] = None) -> Dict:
        """Store important roleplay moments or character developments"""
        try:
            # Format the memory content
            content = f"Character: {character_name}\nEvent: {event_description}"
            
            # Add roleplay-specific tags
            if tags is None:
                tags = []
            tags.extend(["roleplay", "character", character_name.lower()])
            
            # Create the memory
            result = await self.create_memory(
                content=content,
                memory_type="roleplay",
                importance_level=importance_level,
                tags=tags
            )
            
            logger.info(f"Stored roleplay memory for character: {character_name}")
            return result
            
        except Exception as e:
            logger.error(f"Error storing roleplay memory: {e}")
            return {"error": str(e), "character_name": character_name}

    async def search_roleplay_history(self, query: str, character_name: str = None, limit: int = 10) -> Dict:
        """Search past roleplay interactions and character development"""
        try:
            # Build search query
            search_query = f"roleplay {query}"
            if character_name:
                search_query += f" character {character_name}"
            
            # Search with roleplay memory type filter
            results = await self.search_memories(
                query=search_query,
                limit=limit,
                memory_type="roleplay"
            )
            
            # Add roleplay-specific formatting
            results["search_type"] = "roleplay_history"
            results["character_filter"] = character_name
            results["original_query"] = query
            
            logger.info(f"Found {len(results.get('results', []))} roleplay history results")
            return results
            
        except Exception as e:
            logger.error(f"Error searching roleplay history: {e}")
            return {"error": str(e), "query": query}

    # System maintenance
    async def run_database_maintenance(self, force: bool = False) -> Dict:
        """Run maintenance on all databases.
        
        This includes:
        - Optimizing indexes
        - Cleaning up orphaned records
        - Updating statistics
        - Validating data consistency
        
        Args:
            force: Whether to force maintenance even if recent
            
        Returns:
            Dict containing maintenance results
        """
        results = {
            "status": "success",
            "databases": {},
            "timestamp": get_current_timestamp()
        }
        
        try:
            # Run maintenance on each database
            results["databases"]["ai_memories"] = await self.ai_memory_db.run_maintenance(force)
            results["databases"]["conversations"] = await self.conversations_db.run_maintenance(force)
            results["databases"]["schedule"] = await self.schedule_db.run_maintenance(force)
            results["databases"]["vscode"] = await self.vscode_db.run_maintenance(force)
            results["databases"]["mcp"] = await self.mcp_db.run_maintenance(force)
            
            # Check for failed maintenance
            failed = [db for db, result in results["databases"].items() 
                     if result["status"] == "error"]
            
            if failed:
                results["status"] = "partial"
                results["message"] = f"Maintenance failed for: {', '.join(failed)}"
            else:
                results["message"] = "All maintenance tasks completed successfully"
                
        except Exception as e:
            results["status"] = "error"
            results["message"] = str(e)
            logger.error(f"System maintenance error: {e}")
            
        return results
    
    # Embedding helper methods (async background tasks)
    async def _add_embedding_to_message(self, message_id: str, content: str):
        """Add embedding to a message (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.conversations_db.execute_update(
                    "UPDATE messages SET embedding = ? WHERE message_id = ?",
                    (embedding_blob, message_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to message {message_id}: {e}")
    
    async def _add_embedding_to_memory(self, memory_id: str, content: str):
        """Add embedding to a memory (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.ai_memory_db.execute_update(
                    "UPDATE curated_memories SET embedding = ? WHERE memory_id = ?",
                    (embedding_blob, memory_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to memory {memory_id}: {e}")
    
    async def _add_embedding_to_appointment(self, appointment_id: str, content: str):
        """Add embedding to an appointment (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.schedule_db.execute_update(
                    "UPDATE appointments SET embedding = ? WHERE appointment_id = ?",
                    (embedding_blob, appointment_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to appointment {appointment_id}: {e}")
    
    async def _add_embedding_to_reminder(self, reminder_id: str, content: str):
        """Add embedding to a reminder (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.schedule_db.execute_update(
                    "UPDATE reminders SET embedding = ? WHERE reminder_id = ?",
                    (embedding_blob, reminder_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to reminder {reminder_id}: {e}")
    
    async def _add_embedding_to_project_insight(self, insight_id: str, content: str):
        """Add embedding to a project insight (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.vscode_db.execute_update(
                    "UPDATE project_insights SET embedding = ? WHERE insight_id = ?",
                    (embedding_blob, insight_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to project insight {insight_id}: {e}")

    # =============================================================================
    # ADDITIONAL REMINDER AND APPOINTMENT METHODS
    # =============================================================================
    
    async def get_active_reminders(self, limit: int = 10, days_ahead: int = 30) -> List[Dict]:
        """Get active (not completed) reminders"""
        try:
            from datetime import datetime, timedelta
            end_date = (datetime.now() + timedelta(days=days_ahead)).isoformat()
            
            rows = await self.schedule_db.execute_query(
                """SELECT * FROM reminders 
                   WHERE is_completed = 0 AND due_datetime <= ? 
                   ORDER BY due_datetime ASC LIMIT ?""",
                (end_date, limit)
            )
            return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Error getting active reminders: {e}")
            return []
    
    async def get_completed_reminders(self, days: int = 7) -> List[Dict]:
        """Get recently completed reminders"""
        try:
            from datetime import datetime, timedelta
            start_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            rows = await self.schedule_db.execute_query(
                """SELECT * FROM reminders 
                   WHERE is_completed = 1 AND completed_at >= ? 
                   ORDER BY completed_at DESC""",
                (start_date,)
            )
            return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Error getting completed reminders: {e}")
            return []
    
    async def complete_reminder(self, reminder_id: str) -> Dict:
        """Mark a reminder as completed"""
        try:
            from datetime import datetime
            completed_at = datetime.now().isoformat()
            
            await self.schedule_db.execute_update(
                "UPDATE reminders SET is_completed = 1, completed_at = ? WHERE reminder_id = ?",
                (completed_at, reminder_id)
            )
            return {"status": "success", "reminder_id": reminder_id, "completed_at": completed_at}
        except Exception as e:
            logger.error(f"Error completing reminder {reminder_id}: {e}")
            return {"status": "error", "message": str(e)}
    
    async def reschedule_reminder(self, reminder_id: str, new_due_datetime: str) -> Dict:
        """Update the due date of a reminder"""
        try:
            await self.schedule_db.execute_update(
                "UPDATE reminders SET due_datetime = ? WHERE reminder_id = ?",
                (new_due_datetime, reminder_id)
            )
            return {"status": "success", "reminder_id": reminder_id, "new_due_datetime": new_due_datetime}
        except Exception as e:
            logger.error(f"Error rescheduling reminder {reminder_id}: {e}")
            return {"status": "error", "message": str(e)}
    
    async def delete_reminder(self, reminder_id: str) -> Dict:
        """Permanently delete a reminder"""
        try:
            await self.schedule_db.execute_update(
                "DELETE FROM reminders WHERE reminder_id = ?",
                (reminder_id,)
            )
            return {"status": "success", "reminder_id": reminder_id}
        except Exception as e:
            logger.error(f"Error deleting reminder {reminder_id}: {e}")
            return {"status": "error", "message": str(e)}
    
    async def cancel_appointment(self, appointment_id: str) -> Dict:
        """Cancel a scheduled appointment"""
        try:
            await self.schedule_db.execute_update(
                "UPDATE appointments SET is_cancelled = 1 WHERE appointment_id = ?",
                (appointment_id,)
            )
            return {"status": "success", "appointment_id": appointment_id}
        except Exception as e:
            logger.error(f"Error cancelling appointment {appointment_id}: {e}")
            return {"status": "error", "message": str(e)}
    
    async def get_upcoming_appointments(self, limit: int = 5, days_ahead: int = 30) -> List[Dict]:
        """Get upcoming appointments (not cancelled)"""
        try:
            from datetime import datetime, timedelta
            end_date = (datetime.now() + timedelta(days=days_ahead)).isoformat()
            
            rows = await self.schedule_db.execute_query(
                """SELECT * FROM appointments 
                   WHERE is_cancelled = 0 AND scheduled_datetime <= ? 
                   ORDER BY scheduled_datetime ASC LIMIT ?""",
                (end_date, limit)
            )
            return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Error getting upcoming appointments: {e}")
            return []
    
    async def get_appointments(self, limit: int = 5, days_ahead: int = 30) -> List[Dict]:
        """Get recent appointments, optionally filtered by date range"""
        try:
            from datetime import datetime, timedelta
            end_date = (datetime.now() + timedelta(days=days_ahead)).isoformat()
            
            rows = await self.schedule_db.execute_query(
                """SELECT * FROM appointments 
                   WHERE scheduled_datetime <= ? 
                   ORDER BY scheduled_datetime DESC LIMIT ?""",
                (end_date, limit)
            )
            return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Error getting appointments: {e}")
            return []
    
    async def store_ai_reflection(self, content: str, reflection_type: str = "general", 
                                insights: List[str] = None, recommendations: List[str] = None,
                                confidence_level: float = 0.7, source_period_days: int = None) -> Dict:
        """Store an AI self-reflection/insight record"""
        try:
            reflection_id = await self.mcp_db.store_ai_reflection(
                content, reflection_type, insights, recommendations, confidence_level, source_period_days
            )
            return {"status": "success", "reflection_id": reflection_id}
        except Exception as e:
            logger.error(f"Error storing AI reflection: {e}")
            return {"status": "error", "message": str(e)}
    
    async def get_current_time(self) -> Dict:
        """Get the current server time in ISO format (UTC and local)"""
        try:
            from datetime import datetime, timezone
            import time
            
            utc_time = datetime.now(timezone.utc)
            local_time = datetime.now()
            timezone_name = time.tzname[0]
            
            return {
                "utc_time": utc_time.isoformat(),
                "local_time": local_time.isoformat(),
                "timezone": timezone_name,
                "timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"Error getting current time: {e}")
            return {"status": "error", "message": str(e)}
    
    async def get_weather_open_meteo(self, latitude: float = None, longitude: float = None,
                                   timezone_str: str = None, force_refresh: bool = False,
                                   return_changes_only: bool = False, update_today: bool = True,
                                   severe_update: bool = False) -> Dict:
        """Open-Meteo forecast (no API key). Defaults to Motley, MN and caches once per local day."""
        try:
            import requests
            from datetime import datetime, timedelta
            import json
            import os
            
            # Default location (Motley, MN)
            lat = latitude if latitude is not None else 46.3436
            lon = longitude if longitude is not None else -94.6297
            tz = timezone_str if timezone_str is not None else "America/Chicago"
            
            # Create cache directory
            cache_dir = Path("weather_cache")
            cache_dir.mkdir(exist_ok=True)
            
            today = datetime.now().strftime("%Y-%m-%d")
            cache_file = cache_dir / f"weather_{today}.json"
            
            # Check cache unless forced refresh
            if not force_refresh and cache_file.exists():
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f)
                
                if return_changes_only:
                    return {"status": "cached", "message": "Using cached weather data"}
                else:
                    return cached_data
            
            # Fetch from Open-Meteo API
            url = "https://api.open-meteo.com/v1/forecast"
            params = {
                "latitude": lat,
                "longitude": lon,
                "timezone": tz,
                "current": ["temperature_2m", "relative_humidity_2m", "weather_code", "wind_speed_10m"],
                "daily": ["weather_code", "temperature_2m_max", "temperature_2m_min", "precipitation_sum"],
                "forecast_days": 7
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            weather_data = response.json()
            weather_data["cached_at"] = datetime.now().isoformat()
            weather_data["location"] = {"latitude": lat, "longitude": lon, "timezone": tz}
            
            # Save to cache
            with open(cache_file, 'w') as f:
                json.dump(weather_data, f, indent=2)
            
            return weather_data
            
        except Exception as e:
            logger.error(f"Error getting weather: {e}")
            return {"status": "error", "message": str(e)}


# =============================================================================
# MCP SERVER INTEGRATION (Optional - for Model Context Protocol support)
# =============================================================================

# The following code provides MCP server functionality when needed
# To use as MCP server, run: python ai_memory_core.py

async def main():
    """Main entry point - can be used for testing or as MCP server"""
    
    # Initialize the memory system
    memory = PersistentAIMemorySystem()
    
    # Example usage
    print("üß† Persistent AI Memory System - Enhanced Version")
    print("=" * 50)
    
    # Test system health
    health = await memory.get_system_health()
    print(f"System Status: {health['status']}")
    print(f"Databases: {len(health['databases'])} active")
    
    # Test memory creation
    result = await memory.create_memory(
        "This is a test memory with high importance",
        memory_type="test",
        importance_level=8,
        tags=["test", "demo"]
    )
    print(f"‚úÖ Created memory: {result['memory_id']}")
    
    # Test search
    search_results = await memory.search_memories("test memory", limit=5)
    print(f"üîç Found {search_results['count']} memories matching 'test memory'")
    
    print("\n‚ú® Memory system is ready for use!")
    print("üìö Features available:")
    print("   ‚Ä¢ 5 specialized databases")
    print("   ‚Ä¢ Vector semantic search")
    print("   ‚Ä¢ Real-time file monitoring")
    print("   ‚Ä¢ Schedule management")
    print("   ‚Ä¢ Project context tracking")
    print("   ‚Ä¢ MCP tool call logging")

if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/ai_memory_core.py.backup
[file content begin]
#!/usr/bin/env python3
"""
Persistent AI Memory System - Core Module

A comprehensive memory system designed for long-term persistence, semantic search,
and AI assistant augmentation. This standalone version includes all core functionality
with enhanced features for broader use.

Key Features:
- Specialized Database Architecture:
  * Conversations with automatic session management
  * AI-curated memories with importance levels and tags
  * Appointment and reminder scheduling
  * VS Code project context and development tracking
  * MCP tool call logging with AI self-reflection

- Advanced Search and Retrieval:
  * Vector-based semantic search across all databases
  * Project-specific search capabilities
  * Code context linking and retrieval
  * Importance-weighted memory search
  * Fallback text-based search when embeddings unavailable

- Enhanced AI Capabilities:
  * Automatic embedding generation
  * Usage pattern detection and analysis
  * AI self-reflection on tool usage
  * Pattern-based recommendations
  * Confidence scoring for insights

- Real-time Monitoring:
  * Conversation file monitoring
  * Multiple chat source support (VS Code, LM Studio, ChatGPT, etc.)
  * Deduplication across sources
  * MCP server integration
  
- System Management:
  * Comprehensive health monitoring
  * Automated database maintenance
  * Error tracking and logging
  * Performance optimization

- Development Tools:
  * Project continuity tracking
  * Code context management
  * Development session history
  * Insight storage and retrieval

All timestamps are stored in the local timezone using ISO format. This ensures
that timestamps are correctly displayed and interpreted in the local time context.

For usage examples and integration guides, see the documentation in /docs.
"""

import asyncio
import sqlite3
import json
import uuid
import logging
import aiohttp
import numpy as np
import hashlib
import os
import re
import time
import socket
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta, tzinfo
from pathlib import Path
from zoneinfo import ZoneInfo

# Get local timezone
def get_local_timezone() -> ZoneInfo:
    """Get local timezone based on system settings"""
    try:
        import time
        return ZoneInfo(time.tzname[0])
    except:
        # Fallback to a common timezone if detection fails
        return ZoneInfo("America/Chicago")  # Central Time fallback
    
def get_current_timestamp() -> str:
    """Get current timestamp in local timezone ISO format"""
    return datetime.now(get_local_timezone()).isoformat()
    
def datetime_to_local_isoformat(dt: datetime) -> str:
    """Convert any datetime to local timezone ISO format"""
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=get_local_timezone())
    return dt.astimezone(get_local_timezone()).isoformat()

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import hashlib
import sqlite3
import json
import uuid
import hashlib
import asyncio
import aiohttp
import logging
import os
import re
import time
import socket
import numpy as np
from typing import Any, Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone, timedelta
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Configure logging with minimal output
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
# Only show important messages and errors
logger.setLevel(logging.WARNING)


class DatabaseManager:
    """Base database manager for common operations"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.ensure_database_exists()
    
    def ensure_database_exists(self):
        """Ensure the database file and directory exist"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
    def get_connection(self) -> sqlite3.Connection:
        """Get a database connection with proper configuration"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # Enable dict-like access
        conn.execute("PRAGMA foreign_keys = ON")  # Enable foreign key constraints
        return conn
    
    async def execute_query(self, query: str, params: Tuple = ()) -> List[sqlite3.Row]:
        """Execute a SELECT query and return results"""
        with self.get_connection() as conn:
            cursor = conn.execute(query, params)
            return cursor.fetchall()
    
    async def execute_update(self, query: str, params: Tuple = ()) -> str:
        """Execute an INSERT/UPDATE/DELETE query and return last row ID"""
        with self.get_connection() as conn:
            try:
                cursor = conn.execute(query, params)
                conn.commit()
                return str(cursor.lastrowid)
            except sqlite3.Error as e:
                logger.error(f"Database error: {e}")
                logger.error(f"Query: {query}")
                logger.error(f"Params: {params}")
                raise
                
    def parse_timestamp(timestamp: Union[str, int, float, None], fallback: Optional[datetime] = None) -> str:
        """Parse various timestamp formats into ISO format string.
        
        Args:
            timestamp: Input timestamp (string, unix timestamp, or None)
            fallback: Optional fallback datetime if parsing fails
            
        Returns:
            ISO format datetime string
        """
        if not timestamp:
            return (fallback or datetime.now(get_local_timezone())).isoformat()
            
        try:
            if isinstance(timestamp, (int, float)):
                # Unix timestamp
                dt = datetime.fromtimestamp(timestamp, timezone.utc)
            elif isinstance(timestamp, str):
                # Try various string formats
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                except ValueError:
                    # Try parsing with dateutil as fallback
                    from dateutil import parser
                    dt = parser.parse(timestamp)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
            else:
                raise ValueError(f"Unsupported timestamp format: {type(timestamp)}")
                
            return dt.isoformat()
            
        except Exception as e:
            logger.warning(f"Error parsing timestamp {timestamp}: {e}")
            return (fallback or datetime.now(get_local_timezone())).isoformat()


class MCPToolCallDatabase(DatabaseManager):
    """üîß NEW: Tracks all MCP tool calls for reflection and debugging"""
    
    def __init__(self, db_path: str = "mcp_tool_calls.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tool call tracking tables"""
        with self.get_connection() as conn:
            # Tool calls table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_calls (
                    call_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    client_id TEXT,
                    tool_name TEXT NOT NULL,
                    parameters TEXT NOT NULL,
                    result TEXT,
                    status TEXT NOT NULL,
                    execution_time_ms INTEGER,
                    error_message TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Tool usage statistics
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tool_usage_stats (
                    stat_id TEXT PRIMARY KEY,
                    tool_name TEXT NOT NULL,
                    date TEXT NOT NULL,
                    call_count INTEGER DEFAULT 0,
                    success_count INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    avg_execution_time_ms REAL DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(tool_name, date)
                )
            """)
            
            # AI reflections table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS ai_reflections (
                    reflection_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    reflection_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    insights TEXT,
                    recommendations TEXT,
                    confidence_level REAL DEFAULT 0.5,
                    source_period_days INTEGER,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Usage patterns table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS usage_patterns (
                    pattern_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    pattern_type TEXT NOT NULL,
                    insight TEXT NOT NULL,
                    analysis_period_days INTEGER NOT NULL,
                    confidence_score REAL DEFAULT 0.5,
                    supporting_data TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
    
    async def log_tool_call(self, tool_name: str, parameters: Dict, result: Any = None, 
                           status: str = "success", execution_time_ms: float = None,
                           error_message: str = None, client_id: str = None) -> str:
        """Log a tool call with all relevant details"""
        
        call_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        # Store the tool call
        await self.execute_update(
            """INSERT INTO tool_calls 
               (call_id, timestamp, client_id, tool_name, parameters, result, 
                status, execution_time_ms, error_message) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (call_id, timestamp, client_id, tool_name, 
             json.dumps(parameters), json.dumps(result) if result else None,
             status, int(execution_time_ms) if execution_time_ms else None, error_message)
        )
        
        # Update daily statistics
        await self._update_tool_stats(tool_name, status, execution_time_ms)
        
        return call_id
    
    async def _update_tool_stats(self, tool_name: str, status: str, execution_time_ms: float):
        """Update daily usage statistics for a tool"""
        today = datetime.now(get_local_timezone()).date().isoformat()
        
        # Check if stat record exists for today
        existing = await self.execute_query(
            "SELECT * FROM tool_usage_stats WHERE tool_name = ? AND date = ?",
            (tool_name, today)
        )
        
        if existing:
            # Update existing record
            stat = existing[0]
            new_call_count = stat["call_count"] + 1
            new_success_count = stat["success_count"] + (1 if status == "success" else 0)
            new_error_count = stat["error_count"] + (1 if status == "error" else 0)
            
            # Calculate new average execution time
            if execution_time_ms and stat["avg_execution_time_ms"]:
                new_avg = ((stat["avg_execution_time_ms"] * stat["call_count"]) + execution_time_ms) / new_call_count
            elif execution_time_ms:
                new_avg = execution_time_ms
            else:
                new_avg = stat["avg_execution_time_ms"]
            
            await self.execute_update(
                """UPDATE tool_usage_stats 
                   SET call_count = ?, success_count = ?, error_count = ?, avg_execution_time_ms = ?
                   WHERE tool_name = ? AND date = ?""",
                (new_call_count, new_success_count, new_error_count, new_avg, tool_name, today)
            )
        else:
            # Create new record
            stat_id = str(uuid.uuid4())
            await self.execute_update(
                """INSERT INTO tool_usage_stats 
                   (stat_id, tool_name, date, call_count, success_count, error_count, avg_execution_time_ms)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (stat_id, tool_name, today, 1,
                 1 if status == "success" else 0,
                 1 if status == "error" else 0,
                 execution_time_ms or 0)
            )
    
    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """Get tool usage summary for the last N days"""
        
        # Get recent tool calls
        recent_calls = await self.execute_query(
            """SELECT tool_name, status, COUNT(*) as count
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name, status
               ORDER BY count DESC""".format(days)
        )
        
        # Get daily stats
        daily_stats = await self.execute_query(
            """SELECT * FROM tool_usage_stats 
               WHERE date >= date('now', '-{} days')
               ORDER BY date DESC, call_count DESC""".format(days)
        )
        
        # Get most used tools
        most_used = await self.execute_query(
            """SELECT tool_name, COUNT(*) as total_calls
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name
               ORDER BY total_calls DESC
               LIMIT 10""".format(days)
        )
        
        return {
            "recent_calls": [dict(row) for row in recent_calls],
            "daily_stats": [dict(row) for row in daily_stats],
            "most_used_tools": [dict(row) for row in most_used],
            "period_days": days
        }
    
    async def get_tool_call_history(self, tool_name: str = None, limit: int = 50) -> List[Dict]:
        """Get recent tool call history, optionally filtered by tool name"""
        
        if tool_name:
            query = """SELECT * FROM tool_calls 
                      WHERE tool_name = ? 
                      ORDER BY timestamp DESC 
                      LIMIT ?"""
            params = (tool_name, limit)
        else:
            query = """SELECT * FROM tool_calls 
                      ORDER BY timestamp DESC 
                      LIMIT ?"""
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]
        
    async def store_ai_reflection(self, reflection_type: str, content: str,
                                insights: List[str] = None, recommendations: List[str] = None,
                                confidence_level: float = 0.5, source_period_days: int = None) -> str:
        """Store AI self-reflection on tool usage and patterns.
        
        Args:
            reflection_type: Type of reflection (e.g., usage_patterns, performance, suggestions)
            content: Main reflection content
            insights: List of specific insights gained
            recommendations: List of action recommendations
            confidence_level: Confidence in the reflection (0-1)
            source_period_days: Period of data analyzed
            
        Returns:
            str: Reflection ID
        """
        reflection_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO ai_reflections 
               (reflection_id, timestamp, reflection_type, content, insights, 
                recommendations, confidence_level, source_period_days)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (reflection_id, timestamp, reflection_type, content,
             json.dumps(insights) if insights else None,
             json.dumps(recommendations) if recommendations else None,
             confidence_level, source_period_days)
        )
        
        return reflection_id
        
    async def store_usage_pattern(self, pattern_type: str, insight: str, 
                                analysis_period_days: int, confidence_score: float = 0.5,
                                supporting_data: Dict = None) -> str:
        """Store identified usage pattern from AI analysis.
        
        Args:
            pattern_type: Type of usage pattern
            insight: Description of the pattern
            analysis_period_days: Period analyzed to identify pattern
            confidence_score: Confidence in pattern (0-1)
            supporting_data: Additional data supporting the pattern
            
        Returns:
            str: Pattern ID
        """
        pattern_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO usage_patterns
               (pattern_id, timestamp, pattern_type, insight, analysis_period_days,
                confidence_score, supporting_data)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (pattern_id, timestamp, pattern_type, insight, analysis_period_days,
             confidence_score, json.dumps(supporting_data) if supporting_data else None)
        )
        
        return pattern_id
        
    async def get_recent_reflections(self, limit: int = 5, reflection_type: str = None) -> List[Dict]:
        """Get recent AI reflections, optionally filtered by type.
        
        Args:
            limit: Maximum number of reflections to return
            reflection_type: Optional filter by reflection type
            
        Returns:
            List of reflection entries
        """
        if reflection_type:
            query = """SELECT * FROM ai_reflections
                      WHERE reflection_type = ?
                      ORDER BY timestamp DESC
                      LIMIT ?"""
            params = (reflection_type, limit)
        else:
            query = """SELECT * FROM ai_reflections
                      ORDER BY timestamp DESC
                      LIMIT ?"""
            params = (limit,)
            
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]


class ConversationDatabase(DatabaseManager):
    """Manages conversation auto-save database"""
    
    def __init__(self, db_path: str = "conversations.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            # Sessions table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    start_timestamp TEXT NOT NULL,
                    end_timestamp TEXT,
                    context TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Conversations table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversations (
                    conversation_id TEXT PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    start_timestamp TEXT NOT NULL,
                    end_timestamp TEXT,
                    topic_summary TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES sessions (session_id)
                )
            """)
            
            # Messages table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS messages (
                    message_id TEXT PRIMARY KEY,
                    conversation_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    role TEXT NOT NULL,
                    content TEXT NOT NULL,
                    source_type TEXT NOT NULL,  -- chatgpt, claude, vscode, etc.
                    source_id TEXT,  -- Original ID from source system
                    source_url TEXT,  -- URL or path to original content
                    source_metadata TEXT,  -- Source-specific metadata
                    sync_status TEXT,  -- pending, synced, error
                    last_sync TEXT,  -- Last sync timestamp
                    metadata TEXT,  -- General metadata
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
                )
            """)
            
            # Source metadata table for tracking chat sources
            conn.execute("""
                CREATE TABLE IF NOT EXISTS source_tracking (
                    source_id TEXT PRIMARY KEY,
                    source_type TEXT NOT NULL,
                    source_name TEXT NOT NULL,
                    source_path TEXT,
                    last_check TEXT NOT NULL,
                    last_sync TEXT,
                    status TEXT NOT NULL,
                    error_count INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Cross-source relationships table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversation_relationships (
                    relationship_id TEXT PRIMARY KEY,
                    source_conversation_id TEXT NOT NULL,
                    related_conversation_id TEXT NOT NULL,
                    relationship_type TEXT NOT NULL,  -- continuation, reference, fork, etc.
                    metadata TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (source_conversation_id) REFERENCES conversations (conversation_id),
                    FOREIGN KEY (related_conversation_id) REFERENCES conversations (conversation_id)
                )
            """)
            
            conn.commit()
    
    async def store_message(self, content: str, role: str, session_id: str = None, 
                          conversation_id: str = None, metadata: Dict = None) -> Dict[str, str]:
        """Store a message and auto-manage sessions/conversations with duplicate detection"""
        
        timestamp = get_current_timestamp()
        message_id = str(uuid.uuid4())
        
        # Check for duplicate messages (same content, role, and session within recent time)
        if session_id:
            # Check if we already have this exact message in this session recently
            existing = await self.execute_query(
                """SELECT message_id FROM messages 
                   WHERE conversation_id IN (
                       SELECT conversation_id FROM conversations WHERE session_id = ?
                   ) AND role = ? AND content = ? 
                   AND datetime(timestamp) > datetime('now', '-1 hour')""",
                (session_id, role, content)
            )
            
            if existing:
                logger.debug(f"Skipping duplicate message in session {session_id}")
                return {
                    "message_id": existing[0]["message_id"],
                    "conversation_id": None,  # Don't return conversation_id for duplicates
                    "session_id": session_id,
                    "duplicate": True
                }
        
        # Auto-create session if not provided
        if not session_id:
            session_id = str(uuid.uuid4())
            await self.execute_update(
                "INSERT INTO sessions (session_id, start_timestamp, context) VALUES (?, ?, ?)",
                (session_id, timestamp, "auto-created")
            )
        
        # Auto-create conversation if not provided
        if not conversation_id:
            conversation_id = str(uuid.uuid4())
            await self.execute_update(
                "INSERT INTO conversations (conversation_id, session_id, start_timestamp) VALUES (?, ?, ?)",
                (conversation_id, session_id, timestamp)
            )
        
        # Store the message
        await self.execute_update(
            """INSERT INTO messages 
               (message_id, conversation_id, timestamp, role, content, metadata) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (message_id, conversation_id, timestamp, role, content, 
             json.dumps(metadata) if metadata else None)
        )
        
        return {
            "message_id": message_id,
            "conversation_id": conversation_id,
            "session_id": session_id,
            "duplicate": False
        }
    
    async def get_recent_messages(self, limit: int = 10, session_id: str = None) -> List[Dict]:
        """Get recent messages, optionally filtered by session"""
        
        if session_id:
            query = """
                SELECT m.*, c.session_id 
                FROM messages m 
                JOIN conversations c ON m.conversation_id = c.conversation_id
                WHERE c.session_id = ?
                ORDER BY m.timestamp DESC 
                LIMIT ?
            """
            params = (session_id, limit)
        else:
            query = """
                SELECT m.*, c.session_id 
                FROM messages m 
                JOIN conversations c ON m.conversation_id = c.conversation_id
                ORDER BY m.timestamp DESC 
                LIMIT ?
            """
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]


class AIMemoryDatabase(DatabaseManager):
    """Manages AI-curated memories database with enhanced operations"""
    
    def __init__(self, db_path: str = "ai_memories.db"):
        super().__init__(db_path)
        self.initialize_tables()
        
    async def run_maintenance(self, force: bool = False) -> Dict:
        """Run database maintenance tasks.
        
        Args:
            force: Whether to force maintenance even if recent
            
        Returns:
            Dict containing maintenance results
        """
        try:
            # Check last maintenance
            last_maintenance = await self.execute_query(
                "SELECT value FROM metadata WHERE key = 'last_maintenance'"
            )
            
            if not force and last_maintenance:
                last_time = datetime.fromisoformat(last_maintenance[0]["value"])
                if datetime.now(get_local_timezone()) - last_time < timedelta(days=7):
                    return {
                        "status": "skipped",
                        "message": "Maintenance ran recently",
                        "last_run": last_time.isoformat()
                    }
            
            with self.get_connection() as conn:
                # Optimize indexes
                conn.execute("ANALYZE")
                
                # Clean up any orphaned records
                conn.execute("""
                    DELETE FROM curated_memories 
                    WHERE source_conversation_id NOT IN (
                        SELECT conversation_id FROM conversations
                    ) AND source_conversation_id IS NOT NULL
                """)
                
                # Update metadata
                conn.execute(
                    "INSERT OR REPLACE INTO metadata (key, value) VALUES (?, ?)",
                    ("last_maintenance", get_current_timestamp())
                )
                
                conn.commit()
                
            return {
                "status": "success",
                "message": "Maintenance completed successfully",
                "timestamp": get_current_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Maintenance error: {e}")
            return {
                "status": "error",
                "message": str(e),
                "timestamp": get_current_timestamp()
            }
    
    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS curated_memories (
                    memory_id TEXT PRIMARY KEY,
                    timestamp_created TEXT NOT NULL,
                    timestamp_updated TEXT NOT NULL,
                    source_conversation_id TEXT,
                    source_message_ids TEXT,
                    memory_type TEXT,
                    content TEXT NOT NULL,
                    importance_level INTEGER DEFAULT 5,
                    tags TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
    
    async def create_memory(self, content: str, memory_type: str = None, 
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> str:
        """Create a new curated memory"""
        
        memory_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO curated_memories 
               (memory_id, timestamp_created, timestamp_updated, source_conversation_id, 
                memory_type, content, importance_level, tags) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (memory_id, timestamp, timestamp, source_conversation_id, 
             memory_type, content, importance_level, 
             json.dumps(tags) if tags else None)
        )
        
        return memory_id


class ScheduleDatabase(DatabaseManager):
    """Manages appointments and reminders database"""
    
    def __init__(self, db_path: str = "schedule.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            # Appointments table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS appointments (
                    appointment_id TEXT PRIMARY KEY,
                    timestamp_created TEXT NOT NULL,
                    scheduled_datetime TEXT NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    location TEXT,
                    source_conversation_id TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Reminders table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS reminders (
                    reminder_id TEXT PRIMARY KEY,
                    timestamp_created TEXT NOT NULL,
                    due_datetime TEXT NOT NULL,
                    content TEXT NOT NULL,
                    priority_level INTEGER DEFAULT 5,
                    completed INTEGER DEFAULT 0,
                    source_conversation_id TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
    
    async def create_appointment(self, title: str, scheduled_datetime: str, 
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> str:
        """Create a new appointment"""
        
        appointment_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO appointments 
               (appointment_id, timestamp_created, scheduled_datetime, title, description, location, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (appointment_id, timestamp, scheduled_datetime, title, description, location, source_conversation_id)
        )
        
        return appointment_id
    
    async def create_reminder(self, content: str, due_datetime: str, 
                            priority_level: int = 5, source_conversation_id: str = None) -> str:
        """Create a new reminder"""
        
        reminder_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO reminders 
               (reminder_id, timestamp_created, due_datetime, content, priority_level, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (reminder_id, timestamp, due_datetime, content, priority_level, source_conversation_id)
        )
        
        return reminder_id
    
    async def get_upcoming_appointments(self, days_ahead: int = 7) -> List[Dict]:
        """Get upcoming appointments within specified days"""
        
        future_date = datetime.now(get_local_timezone()) + timedelta(days=days_ahead)
        
        rows = await self.execute_query(
            """SELECT * FROM appointments 
               WHERE scheduled_datetime >= ? AND scheduled_datetime <= ?
               ORDER BY scheduled_datetime ASC""",
            (get_current_timestamp(), future_date.isoformat())
        )
        
        return [dict(row) for row in rows]
    
    async def get_active_reminders(self) -> List[Dict]:
        """Get all uncompleted reminders"""
        
        rows = await self.execute_query(
            "SELECT * FROM reminders WHERE completed = 0 ORDER BY due_datetime ASC"
        )
        
        return [dict(row) for row in rows]


class VSCodeProjectDatabase(DatabaseManager):
    """Manages VS Code project context and development sessions"""
    
    def __init__(self, db_path: str = "vscode_project.db"):
        super().__init__(db_path)
        self.initialize_tables()
    
    def initialize_tables(self):
        """Create tables if they don't exist"""
        with self.get_connection() as conn:
            # Project sessions table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS project_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_timestamp TEXT NOT NULL,
                    end_timestamp TEXT,
                    workspace_path TEXT NOT NULL,
                    active_files TEXT,
                    git_branch TEXT,
                    session_summary TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Project insights table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS project_insights (
                    insight_id TEXT PRIMARY KEY,
                    timestamp_created TEXT NOT NULL,
                    timestamp_updated TEXT NOT NULL,
                    insight_type TEXT,
                    content TEXT NOT NULL,
                    related_files TEXT,
                    source_conversation_id TEXT,
                    importance_level INTEGER DEFAULT 5,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Code context table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS code_context (
                    context_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    function_name TEXT,
                    description TEXT NOT NULL,
                    purpose TEXT,
                    related_insights TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Development conversations table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS development_conversations (
                    conversation_id TEXT PRIMARY KEY,
                    session_id TEXT,
                    timestamp TEXT NOT NULL,
                    chat_context_id TEXT,
                    conversation_content TEXT NOT NULL,
                    decisions_made TEXT,
                    code_changes TEXT,
                    embedding BLOB,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES project_sessions (session_id)
                )
            """)
            
            conn.commit()
    
    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> str:
        """Save a development session"""
        
        session_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO project_sessions 
               (session_id, start_timestamp, workspace_path, active_files, git_branch, session_summary) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (session_id, timestamp, workspace_path, 
             json.dumps(active_files) if active_files else None,
             git_branch, session_summary)
        )
        
        return session_id
    
    async def store_development_conversation(self, content: str, session_id: str = None,
                                          chat_context_id: str = None, decisions_made: str = None,
                                          code_changes: Dict = None) -> str:
        """Store a development conversation from VS Code
        
        Args:
            content: The conversation content
            session_id: Optional project session ID (will create new if none)
            chat_context_id: Optional VS Code chat context ID
            decisions_made: Summary of decisions made in conversation
            code_changes: Dictionary of files changed and their changes
        """
        conversation_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        # Create session if none provided
        if not session_id:
            session_id = await self.save_development_session(
                workspace_path=os.getcwd(),  # Current workspace
                session_summary="Auto-created session for development conversation"
            )
        
        # Store conversation
        await self.execute_update(
            """INSERT INTO development_conversations 
               (conversation_id, session_id, timestamp, chat_context_id,
                conversation_content, decisions_made, code_changes)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (conversation_id, session_id, timestamp, chat_context_id,
             content, decisions_made, json.dumps(code_changes) if code_changes else None)
        )
        
        return conversation_id

    async def store_project_insight(self, content: str, insight_type: str = None,
                                  related_files: List[str] = None, importance_level: int = 5,
                                  source_conversation_id: str = None) -> str:
        """Store a project insight"""
        
        insight_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO project_insights 
               (insight_id, timestamp_created, timestamp_updated, insight_type, content, 
                related_files, source_conversation_id, importance_level) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (insight_id, timestamp, timestamp, insight_type, content,
             json.dumps(related_files) if related_files else None,
             source_conversation_id, importance_level)
        )
        
        return insight_id


class ConversationFileMonitor:
    def __init__(self, memory_system, watch_directories):
        self.memory_system = memory_system
        self.watch_directories = watch_directories
        self.vscode_db = memory_system.vscode_db
        self.conversations_db = memory_system.conversations_db  # Add this to maintain compatibility
        self.curated_db = memory_system.curated_db  # Add this to maintain compatibility
        
    def _parse_character_ai_format(self, data: Dict) -> List[Dict]:
        """Parse Character.ai conversation format (list of messages under 'conversation')"""
        conversations = []
        try:
            messages = data.get('conversation', [])
            for msg in messages:
                if isinstance(msg, dict) and 'content' in msg:
                    conversations.append({
                        'role': msg.get('character', msg.get('role', 'unknown')),
                        'content': msg['content'],
                        'timestamp': msg.get('timestamp')
                    })
        except Exception as e:
            logger.error(f"Error parsing Character.ai format: {e}")
        return conversations

    def _parse_text_gen_format(self, data: Dict) -> List[Dict]:
        """Parse text-generation-webui format (list of messages under 'history')"""
        conversations = []
        try:
            history = data.get('history', [])
            for msg in history:
                if isinstance(msg, dict) and 'content' in msg:
                    conversations.append({
                        'role': msg.get('role', 'unknown'),
                        'content': msg['content'],
                        'timestamp': msg.get('timestamp')
                    })
        except Exception as e:
            logger.error(f"Error parsing text-generation-webui format: {e}")
        return conversations
    """Monitors files for conversation changes and auto-imports them.
    
    Features:
    - Automatic MCP server detection to avoid duplicate message processing
    - Real-time file monitoring with hash-based change detection
    - Support for VS Code, LM Studio, and Ollama chat files
    - Message deduplication across sources
    """
    
    def __init__(self, memory_system, watch_directories: List[str] = None, mcp_port: int = 1234):
        self.memory_system = memory_system
        self.watch_directories = watch_directories or []
        self.observer = None
        self.processed_files = set()  # Track processed files to avoid duplicates
        self.file_hashes = {}  # Track file content hashes to detect changes
        self.processed_messages = {}  # Track processed messages per file: {file_path: set(message_hashes)}
        self.mcp_port = mcp_port  # Port to check for MCP server
        self.mcp_server_running = False  # Will be updated periodically
        self.last_mcp_check = 0  # Timestamp of last MCP server check
        
    def _get_default_chat_directories(self) -> List[str]:
        """Get default chat storage directories for different platforms"""
        home = Path.home()
        documents = home / "Documents"
        downloads = home / "Downloads"
        directories = []
        
        # ChatGPT desktop app directories
        chatgpt_paths = [
            home / "AppData" / "Roaming" / "ChatGPT" / "chats",  # Windows
            home / ".config" / "ChatGPT" / "chats",  # Linux
            home / "Library" / "Application Support" / "ChatGPT" / "chats"  # macOS
        ]
        
        # Claude desktop app directories
        claude_paths = [
            home / "AppData" / "Roaming" / "Anthropic" / "Claude" / "conversations",  # Windows
            home / ".config" / "anthropic-claude" / "conversations",  # Linux
            home / "Library" / "Application Support" / "Claude" / "conversations"  # macOS
        ]
        
        # LM Studio conversation directories
        lm_studio_paths = [
            home / ".lmstudio" / "conversations",  # Windows/Linux/macOS (new location)
            home / "AppData" / "Roaming" / "LM Studio" / "conversations",  # Windows (old location)
            home / ".config" / "lm-studio" / "conversations",  # Linux (old location)
            home / "Library" / "Application Support" / "LM Studio" / "conversations"  # macOS (old location)
        ]
        
        # Ollama chat directories
        ollama_paths = [
            home / ".ollama" / "chats",  # Windows/Linux/macOS (main location)
            home / "AppData" / "Roaming" / "Ollama" / "chats",  # Windows (alternative)
            home / ".config" / "ollama" / "chats",  # Linux (alternative)
            home / "Library" / "Application Support" / "Ollama" / "chats"  # macOS (alternative)
        ]
        
        # VS Code workspace storage directories
        vscode_base_paths = [
            home / "AppData" / "Roaming" / "Code" / "User" / "workspaceStorage",  # Windows
            home / ".config" / "Code" / "User" / "workspaceStorage",  # Linux
            home / "Library" / "Application Support" / "Code" / "User" / "workspaceStorage"  # macOS
        ]
        
        # Helper function to add paths with logging
        def add_paths_if_exist(paths: List[Path], app_name: str):
            for path in paths:
                if path.exists():
                    directories.append(str(path))
                    logger.info(f"Found {app_name} conversations: {path}")
        
        # Add paths for each application
        add_paths_if_exist(lm_studio_paths, "LM Studio")
        add_paths_if_exist(ollama_paths, "Ollama")
        add_paths_if_exist(chatgpt_paths, "ChatGPT")
        add_paths_if_exist(claude_paths, "Claude")
        
        # Add VS Code workspace storage paths - find specific workspace hashes
        for vscode_base in vscode_base_paths:
            if vscode_base.exists():
                try:
                    # Look for workspace hashes (directories with chatSessions folders)
                    for workspace_hash in vscode_base.iterdir():
                        if workspace_hash.is_dir():
                            chat_sessions_dir = workspace_hash / "chatSessions"
                            if chat_sessions_dir.exists():
                                directories.append(str(chat_sessions_dir))
                                logger.info(f"Found VS Code chat sessions: {chat_sessions_dir}")
                except Exception as e:
                    logger.error(f"Error scanning VS Code workspace storage: {e}")
        
        return directories

    def _check_mcp_server(self) -> bool:
        """Check if an MCP server is running by attempting a connection.
        
        Returns:
            bool: True if MCP server is running, False otherwise
        """
        # Only check every 60 seconds to avoid overhead
        current_time = time.time()
        if current_time - self.last_mcp_check < 60:
            return self.mcp_server_running
            
        try:
            # Try to connect to MCP server port
            with socket.create_connection(("localhost", self.mcp_port), timeout=1.0):
                self.mcp_server_running = True
        except (socket.timeout, ConnectionRefusedError):
            self.mcp_server_running = False
        
        self.last_mcp_check = current_time
        return self.mcp_server_running
        
    async def _is_message_in_mcp(self, msg_hash: str) -> bool:
        """Check if a message was manually stored through MCP server.
        
        Args:
            msg_hash: Hash of the message content to check
            
        Returns:
            bool: True if message exists in MCP storage, False otherwise
        """
        try:
            # Connect to MCP server
            reader, writer = await asyncio.open_connection('localhost', self.mcp_port)
            
            # Send check request
            request = json.dumps({
                'type': 'check_message',
                'hash': msg_hash
            }).encode() + b'\n'
            writer.write(request)
            await writer.drain()
            
            # Get response
            response = await reader.readline()
            writer.close()
            await writer.wait_closed()
            
            # Parse response
            result = json.loads(response.decode())
            return result.get('exists', False)
            
        except Exception as e:
            logger.debug(f"Failed to check message in MCP: {e}")
            return False  # If check fails, assume message doesn't exist
    
    def _get_mcp_start_time(self) -> Optional[datetime]:
        """Get the start time of the MCP server if running.
        
        Returns:
            Optional[datetime]: Server start time if available, None otherwise
        """
        if not self._check_mcp_server():
            return None
            
        try:
            with socket.create_connection(("localhost", self.mcp_port), timeout=1.0) as sock:
                sock.sendall(b"GET_START_TIME\n")
                response = sock.recv(1024).decode().strip()
                if response and response != "ERROR":
                    return datetime.fromisoformat(response)
        except Exception as e:
            logger.debug(f"Failed to get MCP start time: {e}")
        return None

    async def start_monitoring(self):
        """Start monitoring conversation files"""
        if not self.watch_directories:
            logger.info("No watch directories specified for file monitoring")
            return
            
        # Store reference to the current event loop
        self.loop = asyncio.get_running_loop()
        
        self.observer = Observer()
        
        for directory in self.watch_directories:
            if os.path.exists(directory):
                
                class ConversationFileHandler(FileSystemEventHandler):
                    def __init__(self, monitor):
                        self.monitor = monitor
                    
                    def on_modified(self, event):
                        if not event.is_directory:
                            try:
                                # Get the event loop from the main thread
                                loop = self.monitor.loop
                                if loop and loop.is_running():
                                    asyncio.run_coroutine_threadsafe(
                                        self.monitor._process_file_change(event.src_path), 
                                        loop
                                    )
                            except Exception as e:
                                print(f"Error scheduling file change processing: {e}")
                    
                    def on_created(self, event):
                        if not event.is_directory:
                            try:
                                # Get the event loop from the main thread
                                loop = self.monitor.loop
                                if loop and loop.is_running():
                                    asyncio.run_coroutine_threadsafe(
                                        self.monitor._process_file_change(event.src_path), 
                                        loop
                                    )
                            except Exception as e:
                                print(f"Error scheduling file change processing: {e}")
                
                handler = ConversationFileHandler(self)
                self.observer.schedule(handler, directory, recursive=True)
                logger.info(f"Started monitoring directory: {directory}")
        
        self.observer.start()
        logger.info("File monitoring started")
    
    async def stop_monitoring(self):
        """Stop monitoring conversation files"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            logger.info("File monitoring stopped")
    
    def add_watch_directory(self, directory: str):
        """Add a directory to monitor"""
        if directory not in self.watch_directories:
            self.watch_directories.append(directory)
            logger.info(f"Added watch directory: {directory}")
    
    async def _process_file_change(self, file_path: str):
        """Process a changed conversation file with MCP-aware deduplication"""
        try:
            # Check if file is a conversation file (JSON, txt, etc.)
            if not any(file_path.endswith(ext) for ext in ['.json', '.txt', '.md', '.log']):
                return
            
            # Calculate file hash to detect actual content changes
            with open(file_path, 'rb') as f:
                file_content = f.read()
                current_hash = hashlib.md5(file_content).hexdigest()
            
            # Skip if we've already processed this exact content
            if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:
                return
                
            self.file_hashes[file_path] = current_hash
            
            # Initialize message tracking for this file if needed
            if file_path not in self.processed_messages:
                self.processed_messages[file_path] = set()
            
            # Read and parse conversation content
            conversations = await self._extract_conversations(file_path)
            
            # Check with MCP server for manually stored messages
            if self._check_mcp_server():
                try:
                    filtered_conversations = []
                    for conv in conversations:
                        # Create a hash of the message content and metadata
                        msg_hash = hashlib.md5(
                            f"{conv['role']}:{conv['content']}".encode()
                        ).hexdigest()
                        
                        # Check if this exact message was manually stored
                        if not await self._is_message_in_mcp(msg_hash):
                            filtered_conversations.append(conv)
                    conversations = filtered_conversations
                except Exception as e:
                    logger.debug(f"Failed to check MCP messages: {e}")
                    # If we can't check MCP server, process all messages
            
            # For VS Code chat files, handle development conversations
            is_vscode_chat = 'vscode' in file_path.lower() or 'chatsessions' in file_path.lower()
            if is_vscode_chat:
                # Create development session
                dev_session_id = await self.memory_system.vscode_db.save_development_session(
                    workspace_path=os.path.dirname(file_path),
                    session_summary=f"Imported VS Code chat session from {os.path.basename(file_path)}"
                )
                full_conversation = []
            
            # Store conversations in database
            for conv in conversations:
                result = await self.memory_system.store_conversation(
                    content=conv['content'],
                    role=conv['role'],
                    metadata={'source_file': file_path, 'imported_at': get_current_timestamp()},
                    session_id=self._get_file_hash(file_path)  # Use file hash as session ID for grouping
                )
                
                if is_vscode_chat and not result.get("duplicate", False):
                    # Add to development conversation
                    full_conversation.append(f"{conv['role'].title()}: {conv['content']}")
            
            # Store development conversation if this is a VS Code chat
            if is_vscode_chat and full_conversation:
                await self.memory_system.vscode_db.store_development_conversation(
                    content="\n\n".join(full_conversation),
                    session_id=dev_session_id,
                    chat_context_id=self._get_file_hash(file_path)
                )
            
            logger.info(f"Imported {len(conversations)} conversations from {file_path}")
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """Generate hash of file content for duplicate detection"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return str(hash(file_path))
    
    async def _extract_conversations(self, file_path: str) -> List[Dict]:
        """Extract conversations from various file formats with timestamps, using registry-based extensibility and robust deduplication"""
        conversations = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            fallback_time = datetime.fromtimestamp(
                os.path.getmtime(file_path),
                timezone.utc
            ).isoformat()

            # Registry of format handlers: (predicate, handler)
            format_handlers = [
                (lambda fn, _: fn.endswith('.json'), self._handle_json_formats),
                (lambda fn, _: fn.endswith(('.txt', '.md', '.log')), self._parse_text_format),
            ]

            handled = False
            for predicate, handler in format_handlers:
                if predicate(file_path, content):
                    if handler == self._handle_json_formats:
                        conversations.extend(handler(content))
                    else:
                        conversations.extend(handler(content))
                    handled = True
                    break

            if not handled:
                logger.warning(f"No format handler found for {file_path}")

            # Ensure all conversations have timestamps
            for conv in conversations:
                if 'timestamp' not in conv or not conv['timestamp']:
                    conv['timestamp'] = fallback_time

            # Robust deduplication: by id (if present), timestamp (if present), and content hash
            seen = set()
            deduped = []
            for conv in conversations:
                # Use id if present, else None
                cid = conv.get('id') or conv.get('message_id') or None
                ts = conv.get('timestamp') or None
                content_hash = hashlib.md5(conv.get('content', '').encode('utf-8')).hexdigest()
                dedup_key = (cid, ts, content_hash)
                if dedup_key not in seen:
                    seen.add(dedup_key)
                    deduped.append(conv)
            return deduped
        except Exception as e:
            logger.error(f"Error extracting conversations from {file_path}: {e}")
            return []

    def _handle_json_formats(self, content: str) -> List[Dict]:
        """Handle all supported JSON conversation formats (add new ones here)"""
        conversations = []
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if 'mapping' in data:
                    conversations.extend(self._parse_chatgpt_format(data))
                elif 'messages' in data:
                    conversations.extend(self._parse_claude_format(data))
            elif isinstance(data, list):
                conversations.extend(self._parse_simple_array(data))
        except Exception as e:
            logger.error(f"Error handling JSON formats: {e}")
        return conversations
    
    def _parse_chatgpt_format(self, data: Dict) -> List[Dict]:
        """Parse ChatGPT export format with timestamps"""
        conversations = []
        
        try:
            if 'mapping' in data:
                for node_id, node in data['mapping'].items():
                    if node.get('message') and node['message'].get('content'):
                        content_parts = node['message']['content'].get('parts', [])
                        if content_parts:
                            # Try to get create_time from message
                            timestamp = None
                            if 'create_time' in node['message']:
                                try:
                                    timestamp = datetime.fromtimestamp(
                                        int(node['message']['create_time']),
                                        timezone.utc
                                    ).isoformat()
                                except (ValueError, TypeError):
                                    pass
                            
                            conversations.append({
                                'role': node['message'].get('author', {}).get('role', 'unknown'),
                                'content': ' '.join(str(part) for part in content_parts if part),
                                'timestamp': timestamp
                            })
        except Exception as e:
            logger.error(f"Error parsing ChatGPT format: {e}")
        
        return conversations
    
    def _parse_simple_array(self, data: List) -> List[Dict]:
        """Parse simple conversation array format with timestamps"""
        conversations = []
        
        for item in data:
            if isinstance(item, dict) and 'content' in item:
                # Look for timestamp in various formats
                timestamp = None
                for key in ['timestamp', 'time', 'created_at', 'date']:
                    if key in item:
                        try:
                            # Handle both ISO format strings and Unix timestamps
                            if isinstance(item[key], (int, float)):
                                timestamp = datetime.fromtimestamp(item[key], timezone.utc).isoformat()
                            else:
                                timestamp = datetime.fromisoformat(str(item[key])).isoformat()
                            break
                        except (ValueError, TypeError):
                            continue
                
                conversations.append({
                    'role': item.get('role', 'user'),
                    'content': str(item['content']),
                    'timestamp': timestamp
                })
        
        return conversations
    
    def _parse_claude_format(self, data: Dict) -> List[Dict]:
        """Parse Claude/Anthropic conversation format"""
        conversations = []
        
        try:
            # Handle both array and object formats
            messages = data.get('messages', [])
            if isinstance(messages, dict):
                messages = messages.values()
            
            for msg in messages:
                if isinstance(msg, dict) and 'content' in msg:
                    # Try to extract timestamp
                    timestamp = None
                    if 'timestamp' in msg:
                        try:
                            timestamp = datetime.fromisoformat(msg['timestamp'])
                        except (ValueError, TypeError):
                            pass
                    
                    conversations.append({
                        'role': msg.get('role', 'unknown'),
                        'content': msg['content'],
                        'timestamp': timestamp.isoformat() if timestamp else None,
                        'metadata': {
                            'source': 'Claude',
                            'model': data.get('model', 'claude'),
                            'conversation_id': data.get('conversation_id'),
                            'message_id': msg.get('id'),
                            'parent_id': msg.get('parent')
                        }
                    })
        except Exception as e:
            logger.error(f"Error parsing Claude format: {e}")
        
        return conversations
    
    def _parse_text_format(self, content: str) -> List[Dict]:
        """Parse text-based conversation formats with timestamp detection"""
        conversations = []
        lines = content.split('\n')
        
        current_role = 'user'
        current_content = []
        current_timestamp = None
        
        # Common timestamp patterns
        timestamp_patterns = [
            r'\[(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?)\]',  # ISO format
            r'\[(\d{2}:\d{2}(?::\d{2})?)\]',  # Time only
            r'\[(\d{4}-\d{2}-\d{2})\]',  # Date only
        ]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Try to extract timestamp
            for pattern in timestamp_patterns:
                match = re.match(pattern, line)
                if match:
                    try:
                        ts = match.group(1)
                        # Handle time-only format by adding today's date
                        if re.match(r'\d{2}:\d{2}(?::\d{2})?$', ts):
                            ts = f"{datetime.now().date()}T{ts}"
                        current_timestamp = datetime.fromisoformat(ts).isoformat()
                        line = line[match.end():].strip()
                        break
                    except (ValueError, TypeError):
                        continue
            
            # Detect role markers
            if line.lower().startswith(('user:', 'human:', 'me:')):
                if current_content:
                    conversations.append({
                        'role': current_role,
                        'content': '\n'.join(current_content),
                        'timestamp': current_timestamp
                    })
                    current_content = []
                current_role = 'user'
                content_part = line.split(':', 1)[1].strip() if ':' in line else line
                if content_part:
                    current_content.append(content_part)
                    
            elif line.lower().startswith(('assistant:', 'ai:', 'bot:', 'friday:')):
                if current_content:
                    conversations.append({
                        'role': current_role,
                        'content': '\n'.join(current_content)
                    })
                    current_content = []
                current_role = 'assistant'
                content_part = line.split(':', 1)[1].strip() if ':' in line else line
                if content_part:
                    current_content.append(content_part)
            else:
                current_content.append(line)
        
        # Add the last conversation
        if current_content:
            conversations.append({
                'role': current_role,
                'content': '\n'.join(current_content)
            })
        
        return conversations


class EmbeddingService:
    """Handles embedding generation via LM Studio"""
    
    def __init__(self, base_url: str = "http://localhost:1234"):
        """Initialize embedding service with LM Studio URL
        
        Args:
            base_url: LM Studio API URL. Defaults to localhost:1234 which is LM Studio's default.
                     For Ollama, use "http://localhost:11434"
                     For local LM Studio, keep as is
                     For other embedding services, provide full base URL
        """
        self.base_url = base_url
        self.embeddings_endpoint = f"{base_url}/v1/embeddings"
    
    async def generate_embedding(self, text: str, model: str = "text-embedding-nomic-embed-text-v1.5") -> List[float]:
        """Generate embedding for text using LM Studio"""
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": model,
                    "input": text,
                }
                
                async with session.post(self.embeddings_endpoint, json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data and "data" in data and len(data["data"]) > 0:
                            return data["data"][0].get("embedding")
                        else:
                            logger.error(f"Invalid response format: {data}")
                            return None
                    else:
                        error_text = await response.text()
                        logger.error(f"Embedding API error {response.status}: {error_text}")
                        return None
        
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return None


class PersistentAIMemorySystem:
    """Main memory system that coordinates all databases and operations - FULL FEATURED VERSION"""
    
    def __init__(self, data_dir: str = "memory_data", enable_file_monitoring: bool = True, 
                 watch_directories: List[str] = None):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # Initialize all 5 databases
        self.conversations_db = ConversationDatabase(str(self.data_dir / "conversations.db"))
        self.ai_memory_db = AIMemoryDatabase(str(self.data_dir / "ai_memories.db"))
        self.schedule_db = ScheduleDatabase(str(self.data_dir / "schedule.db"))
        self.vscode_db = VSCodeProjectDatabase(str(self.data_dir / "vscode_project.db"))
        self.mcp_db = MCPToolCallDatabase(str(self.data_dir / "mcp_tool_calls.db"))
        
        # Initialize embedding service
        self.embedding_service = EmbeddingService()
        
        # Initialize file monitoring
        self.file_monitor = None
        if enable_file_monitoring:
            self.file_monitor = ConversationFileMonitor(self, watch_directories)
    
    async def start_file_monitoring(self):
        """Start monitoring conversation files"""
        if self.file_monitor:
            await self.file_monitor.start_monitoring()
            logger.info("File monitoring started")
    
    async def stop_file_monitoring(self):
        """Stop monitoring conversation files"""
        if self.file_monitor:
            await self.file_monitor.stop_monitoring()
            logger.info("File monitoring stopped")
    
    def add_watch_directory(self, directory: str):
        """Add a directory to monitor for conversation files"""
        if self.file_monitor:
            self.file_monitor.add_watch_directory(directory)

    # =============================================================================
    # CONVERSATION OPERATIONS
    # =============================================================================
    
    async def store_conversation(self, content: str, role: str, session_id: str = None,
                               conversation_id: str = None, metadata: Dict = None) -> Dict:
        """Store a conversation message with automatic embedding generation"""
        
        result = await self.conversations_db.store_message(
            content, role, session_id, conversation_id, metadata
        )
        
        # Generate and store embedding asynchronously
        asyncio.create_task(self._add_embedding_to_message(result["message_id"], content))
        
        return {
            "status": "success",
            "message_id": result["message_id"],
            "conversation_id": result["conversation_id"],
            "session_id": result["session_id"]
        }
    
    async def get_conversation_history(self, limit: int = 20, session_id: str = None) -> List[Dict]:
        """Get recent conversation history"""
        
        messages = await self.conversations_db.get_recent_messages(limit, session_id)
        return [dict(msg) for msg in messages]

    # =============================================================================
    # AI MEMORY OPERATIONS
    # =============================================================================
    
    async def create_memory(self, content: str, memory_type: str = None,
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> Dict:
        """Create a curated AI memory with automatic embedding generation"""
        
        memory_id = await self.ai_memory_db.create_memory(
            content, memory_type, importance_level, tags, source_conversation_id
        )
        
        # Generate and store embedding asynchronously
        asyncio.create_task(self._add_embedding_to_memory(memory_id, content))
        
        return {
            "status": "success",
            "memory_id": memory_id
        }

    # =============================================================================
    # SCHEDULE OPERATIONS
    # =============================================================================
    
    async def create_appointment(self, title: str, scheduled_datetime: str,
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> Dict:
        """Create an appointment with automatic embedding generation"""
        
        appointment_id = await self.schedule_db.create_appointment(
            title, scheduled_datetime, description, location, source_conversation_id
        )
        
        # Generate embedding for search (combine title and description)
        content_for_embedding = f"{title}"
        if description:
            content_for_embedding += f" {description}"
        
        asyncio.create_task(self._add_embedding_to_appointment(appointment_id, content_for_embedding))
        
        return {
            "status": "success",
            "appointment_id": appointment_id
        }
    
    async def create_reminder(self, content: str, due_datetime: str,
                            priority_level: int = 5, source_conversation_id: str = None) -> Dict:
        """Create a reminder with automatic embedding generation"""
        
        reminder_id = await self.schedule_db.create_reminder(
            content, due_datetime, priority_level, source_conversation_id
        )
        
        # Generate and store embedding for the reminder content
        asyncio.create_task(self._add_embedding_to_reminder(reminder_id, content))
        
        return {
            "status": "success",
            "reminder_id": reminder_id
        }
    
    async def get_upcoming_schedule(self, days_ahead: int = 7) -> Dict:
        """Get upcoming appointments and reminders"""
        
        appointments = await self.schedule_db.get_upcoming_appointments(days_ahead)
        reminders = await self.schedule_db.get_active_reminders()
        
        return {
            "status": "success",
            "appointments": appointments,
            "active_reminders": reminders,
            "period_days": days_ahead
        }

    # =============================================================================
    # VSCODE PROJECT OPERATIONS
    # =============================================================================
    
    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> Dict:
        """Save development session"""
        
        session_id = await self.vscode_db.save_development_session(
            workspace_path, active_files, git_branch, session_summary
        )
        
        return {
            "status": "success",
            "session_id": session_id
        }
    
    async def store_project_insight(self, content: str, insight_type: str = None,
                                  related_files: List[str] = None, importance_level: int = 5,
                                  source_conversation_id: str = None) -> Dict:
        """Store project insight with automatic embedding generation"""
        
        insight_id = await self.vscode_db.store_project_insight(
            content, insight_type, related_files, importance_level, source_conversation_id
        )
        
        # Generate and store embedding for the insight content
        asyncio.create_task(self._add_embedding_to_project_insight(insight_id, content))
        
        return {
            "status": "success",
            "insight_id": insight_id
        }

    # =============================================================================
    # MCP TOOL CALL OPERATIONS
    # =============================================================================
    
    async def log_tool_call(self, tool_name: str, parameters: Dict = None,
                          execution_time_ms: float = None, status: str = "success",
                          result: Any = None, error_message: str = None, client_id: str = None) -> str:
        """Log an MCP tool call for analysis and debugging"""
        
        return await self.mcp_db.log_tool_call(
            tool_name, parameters, result, status, execution_time_ms, error_message, client_id
        )
    
    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """Get comprehensive tool usage summary"""
        
        return await self.mcp_db.get_tool_usage_summary(days)

    # =============================================================================
    # ADVANCED SEARCH OPERATIONS
    # =============================================================================
    
    async def search_project_history(self, query: str, limit: int = 10) -> Dict:
        """Search project development history including conversations and insights.
        
        Args:
            query: Search query string
            limit: Maximum number of results
            
        Returns:
            Dict containing search results from project context
        """
        query_embedding = await self.embedding_service.generate_embedding(query)
        if not query_embedding:
            return await self._text_based_project_search(query, limit)
            
        results = []
        
        # Search development conversations
        conv_results = await self._search_development_conversations(query_embedding, limit)
        results.extend(conv_results)
        
        # Search project insights
        insight_results = await self._search_project_insights(query_embedding, limit)
        results.extend(insight_results)
        
        # Search code context
        context_results = await self._search_code_context(query_embedding, limit)
        results.extend(context_results)
        
        # Sort by relevance and return
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return {
            "status": "success",
            "query": query,
            "results": results[:limit],
            "count": len(results[:limit])
        }
        
    async def link_code_context(self, file_path: str, description: str,
                              function_name: str = None, conversation_id: str = None) -> Dict:
        """Link conversation context to specific code location.
        
        Args:
            file_path: Path to the code file
            description: Description of the code context
            function_name: Optional function/method name
            conversation_id: Optional related conversation ID
            
        Returns:
            Dict containing the created context link
        """
        context_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.vscode_db.execute_update(
            """INSERT INTO code_context 
               (context_id, timestamp, file_path, function_name, description)
               VALUES (?, ?, ?, ?, ?)""",
            (context_id, timestamp, file_path, function_name, description)
        )
        
        if conversation_id:
            await self.vscode_db.execute_update(
                """UPDATE development_conversations
                   SET chat_context_id = ?
                   WHERE conversation_id = ?""",
                (context_id, conversation_id)
            )
            
        # Generate embedding for search
        asyncio.create_task(self._add_embedding_to_code_context(context_id, description))
        
        return {
            "status": "success",
            "context_id": context_id
        }
        
    async def get_project_continuity(self, workspace_path: str = None, limit: int = 5) -> Dict:
        """Get context for continuing development work.
        
        Args:
            workspace_path: Optional workspace path filter
            limit: Maximum number of context items
            
        Returns:
            Dict containing recent development context
        """
        # Get recent development sessions
        sessions_query = """
            SELECT * FROM project_sessions
            WHERE end_timestamp IS NULL
        """
        if workspace_path:
            sessions_query += " AND workspace_path = ?"
            sessions = await self.vscode_db.execute_query(
                sessions_query + " ORDER BY start_timestamp DESC LIMIT ?",
                (workspace_path, limit)
            )
        else:
            sessions = await self.vscode_db.execute_query(
                sessions_query + " ORDER BY start_timestamp DESC LIMIT ?",
                (limit,)
            )
            
        # Get associated conversations and insights
        context = {
            "active_sessions": [dict(session) for session in sessions],
            "recent_conversations": [],
            "relevant_insights": []
        }
        
        for session in sessions:
            # Get conversations for this session
            convs = await self.vscode_db.execute_query(
                """SELECT * FROM development_conversations
                   WHERE session_id = ?
                   ORDER BY timestamp DESC LIMIT ?""",
                (session["session_id"], limit)
            )
            context["recent_conversations"].extend([dict(conv) for conv in convs])
            
            # Get insights mentioning active files
            if session["active_files"]:
                active_files = json.loads(session["active_files"])
                for file in active_files:
                    insights = await self.vscode_db.execute_query(
                        """SELECT * FROM project_insights
                           WHERE related_files LIKE ?
                           ORDER BY timestamp_created DESC LIMIT ?""",
                        (f"%{file}%", limit)
                    )
                    context["relevant_insights"].extend([dict(insight) for insight in insights])
        
        return {
            "status": "success",
            "context": context
        }
            
    async def search_memories(self, query: str, limit: int = 10, 
                            min_importance: int = None, max_importance: int = None,
                            memory_type: str = None, database_filter: str = "all") -> Dict:
        """Advanced semantic search across all databases with filtering"""
        
        # Generate embedding for the search query
        query_embedding = await self.embedding_service.generate_embedding(query)
        if not query_embedding:
            # Fallback to text-based search if embedding fails
            return await self._text_based_search(query, limit, database_filter, min_importance, max_importance, memory_type)
        
        all_results = []
        
        # Search AI memories
        if database_filter in ["all", "ai_memories"]:
            memory_results = await self._search_ai_memories(query_embedding, limit, min_importance, max_importance, memory_type)
            all_results.extend(memory_results)
        
        # Search conversations
        if database_filter in ["all", "conversations"]:
            conversation_results = await self._search_conversations(query_embedding, limit)
            all_results.extend(conversation_results)
        
        # Search schedule items
        if database_filter in ["all", "schedule"]:
            schedule_results = await self._search_schedule(query_embedding, limit)
            all_results.extend(schedule_results)
        
        # Search project insights
        if database_filter in ["all", "projects"]:
            project_results = await self._search_project_insights(query_embedding, limit)
            all_results.extend(project_results)
        
        # Sort all results by similarity score and return top results
        all_results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        return {
            "status": "success",
            "query": query,
            "results": all_results[:limit],
            "count": len(all_results[:limit]),
            "search_type": "semantic" if query_embedding else "text_based"
        }

    # =============================================================================
    # SYSTEM HEALTH AND MONITORING
    # =============================================================================
    
    async def get_system_health(self) -> Dict:
        """Get comprehensive system health and statistics"""
        health_data = {
            "status": "healthy",
            "timestamp": get_current_timestamp(),
            "databases": {},
            "file_monitoring": {},
            "embedding_service": {}
        }
        
        try:
            # Check conversations database
            conversations_count = await self.conversations_db.execute_query(
                "SELECT COUNT(*) as count FROM messages"
            )
            sessions_count = await self.conversations_db.execute_query(
                "SELECT COUNT(*) as count FROM sessions"
            )
            health_data["databases"]["conversations"] = {
                "status": "healthy",
                "message_count": conversations_count[0]["count"] if conversations_count else 0,
                "session_count": sessions_count[0]["count"] if sessions_count else 0,
                "database_path": self.conversations_db.db_path
            }
            
            # Check AI memories database
            memories_count = await self.ai_memory_db.execute_query(
                "SELECT COUNT(*) as count FROM curated_memories"
            )
            high_importance_count = await self.ai_memory_db.execute_query(
                "SELECT COUNT(*) as count FROM curated_memories WHERE importance_level >= 7"
            )
            health_data["databases"]["ai_memories"] = {
                "status": "healthy",
                "memory_count": memories_count[0]["count"] if memories_count else 0,
                "high_importance_count": high_importance_count[0]["count"] if high_importance_count else 0,
                "database_path": self.ai_memory_db.db_path
            }
            
            # Check schedule database
            appointments_count = await self.schedule_db.execute_query(
                "SELECT COUNT(*) as count FROM appointments"
            )
            reminders_count = await self.schedule_db.execute_query(
                "SELECT COUNT(*) as count FROM reminders"
            )
            health_data["databases"]["schedule"] = {
                "status": "healthy",
                "appointment_count": appointments_count[0]["count"] if appointments_count else 0,
                "reminder_count": reminders_count[0]["count"] if reminders_count else 0,
                "database_path": self.schedule_db.db_path
            }
            
            # Check VS Code project database
            project_sessions_count = await self.vscode_db.execute_query(
                "SELECT COUNT(*) as count FROM project_sessions"
            )
            insights_count = await self.vscode_db.execute_query(
                "SELECT COUNT(*) as count FROM project_insights"
            )
            health_data["databases"]["vscode_project"] = {
                "status": "healthy",
                "session_count": project_sessions_count[0]["count"] if project_sessions_count else 0,
                "insight_count": insights_count[0]["count"] if insights_count else 0,
                "database_path": self.vscode_db.db_path
            }
            
            # Check MCP tool calls database
            tool_calls_count = await self.mcp_db.execute_query(
                "SELECT COUNT(*) as count FROM tool_calls"
            )
            health_data["databases"]["mcp_tool_calls"] = {
                "status": "healthy",
                "total_tool_calls": tool_calls_count[0]["count"] if tool_calls_count else 0,
                "database_path": self.mcp_db.db_path
            }
            
            # Check file monitoring status
            if self.file_monitor:
                health_data["file_monitoring"] = {
                    "status": "enabled",
                    "watch_directories": len(self.file_monitor.watch_directories),
                    "directories": self.file_monitor.watch_directories
                }
            else:
                health_data["file_monitoring"] = {
                    "status": "disabled",
                    "message": "File monitoring is not enabled"
                }
            
            # Check embedding service
            try:
                # Try a simple ping to the embedding service
                test_embedding = await self.embedding_service.generate_embedding("test")
                if test_embedding:
                    health_data["embedding_service"] = {
                        "status": "healthy",
                        "endpoint": self.embedding_service.embeddings_endpoint,
                        "embedding_dimensions": len(test_embedding)
                    }
                else:
                    health_data["embedding_service"] = {
                        "status": "unhealthy",
                        "endpoint": self.embedding_service.embeddings_endpoint,
                        "error": "Failed to generate test embedding"
                    }
            except Exception as e:
                health_data["embedding_service"] = {
                    "status": "unhealthy",
                    "endpoint": self.embedding_service.embeddings_endpoint,
                    "error": str(e)
                }
            
            # Overall system status
            unhealthy_components = []
            if health_data["embedding_service"]["status"] == "unhealthy":
                unhealthy_components.append("embedding_service")
            
            if unhealthy_components:
                health_data["status"] = "degraded"
                health_data["issues"] = unhealthy_components
            
        except Exception as e:
            health_data["status"] = "error"
            health_data["error"] = str(e)
            logger.error(f"Error getting system health: {e}")
        
        return health_data

    # =============================================================================
    # INTERNAL HELPER METHODS
    # =============================================================================
    
    async def _search_ai_memories(self, query_embedding: List[float], limit: int,
                                min_importance: int = None, max_importance: int = None,
                                memory_type: str = None) -> List[Dict]:
        """Search AI curated memories using semantic similarity"""
        
        # Build SQL query with optional filters
        sql = "SELECT * FROM curated_memories WHERE embedding IS NOT NULL"
        params = []
        
        if min_importance is not None:
            sql += " AND importance_level >= ?"
            params.append(min_importance)
            
        if max_importance is not None:
            sql += " AND importance_level <= ?"
            params.append(max_importance)
            
        if memory_type is not None:
            sql += " AND memory_type = ?"
            params.append(memory_type)
        
        rows = await self.ai_memory_db.execute_query(sql, params)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:  # Threshold for relevance
                    result = {
                        "type": "ai_memory",
                        "similarity_score": similarity,
                        "data": {
                            "memory_id": row["memory_id"],
                            "content": row["content"],
                            "importance_level": row["importance_level"],
                            "memory_type": row["memory_type"],
                            "timestamp_created": row["timestamp_created"],
                            "tags": json.loads(row["tags"]) if row["tags"] else []
                        }
                    }
                    results.append(result)
        
        # Boost results based on importance level
        for result in results:
            importance_boost = result["data"]["importance_level"] / 10.0 * 0.1
            result["similarity_score"] += importance_boost
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_conversations(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search conversation messages using semantic similarity"""
        
        query = """
            SELECT message_id, conversation_id, timestamp, role, content, metadata, embedding
            FROM messages 
            WHERE embedding IS NOT NULL
            ORDER BY timestamp DESC
            LIMIT 1000
        """
        
        rows = await self.conversations_db.execute_query(query)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "conversation",
                        "similarity_score": similarity,
                        "data": {
                            "message_id": row["message_id"],
                            "conversation_id": row["conversation_id"],
                            "timestamp": row["timestamp"],
                            "role": row["role"],
                            "content": row["content"],
                            "metadata": json.loads(row["metadata"]) if row["metadata"] else None
                        }
                    }
                    results.append(result)
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_schedule(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search appointments and reminders using semantic similarity"""
        
        results = []
        
        # Search appointments
        appointment_query = """
            SELECT appointment_id, timestamp_created, scheduled_datetime, title, 
                   description, location, source_conversation_id, embedding
            FROM appointments 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.schedule_db.execute_query(appointment_query)
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "appointment",
                        "similarity_score": similarity,
                        "data": {
                            "appointment_id": row["appointment_id"],
                            "scheduled_datetime": row["scheduled_datetime"],
                            "title": row["title"],
                            "description": row["description"],
                            "location": row["location"]
                        }
                    }
                    results.append(result)
        
        # Search reminders
        reminder_query = """
            SELECT reminder_id, timestamp_created, due_datetime, content, 
                   priority_level, completed, source_conversation_id, embedding
            FROM reminders 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.schedule_db.execute_query(reminder_query)
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "reminder",
                        "similarity_score": similarity,
                        "data": {
                            "reminder_id": row["reminder_id"],
                            "due_datetime": row["due_datetime"],
                            "content": row["content"],
                            "priority_level": row["priority_level"],
                            "completed": bool(row["completed"])
                        }
                    }
                    results.append(result)
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    async def _search_project_insights(self, query_embedding: List[float], limit: int) -> List[Dict]:
        """Search project insights using semantic similarity"""
        
        query = """
            SELECT insight_id, timestamp_created, insight_type, content, 
                   related_files, importance_level, embedding
            FROM project_insights 
            WHERE embedding IS NOT NULL
        """
        
        rows = await self.vscode_db.execute_query(query)
        results = []
        
        for row in rows:
            if row["embedding"]:
                stored_embedding = np.frombuffer(row["embedding"], dtype=np.float32).tolist()
                similarity = self._calculate_cosine_similarity(query_embedding, stored_embedding)
                
                if similarity > 0.3:
                    result = {
                        "type": "project_insight",
                        "similarity_score": similarity,
                        "data": {
                            "insight_id": row["insight_id"],
                            "timestamp_created": row["timestamp_created"],
                            "insight_type": row["insight_type"],
                            "content": row["content"],
                            "related_files": json.loads(row["related_files"]) if row["related_files"] else None,
                            "importance_level": row["importance_level"]
                        }
                    }
                    results.append(result)
        
        # Boost results based on importance level
        for result in results:
            importance_boost = result["data"]["importance_level"] / 10.0 * 0.15
            result["similarity_score"] += importance_boost
        
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return results[:limit]
    
    def _calculate_cosine_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings"""
        
        try:
            # Convert to numpy arrays
            vec1 = np.array(embedding1, dtype=np.float32)
            vec2 = np.array(embedding2, dtype=np.float32)
            
            # Calculate cosine similarity
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {e}")
            return 0.0
    
    async def _text_based_search(self, query: str, limit: int, database_filter: str,
                               min_importance: int = None, max_importance: int = None,
                               memory_type: str = None) -> Dict:
        """Fallback text-based search when embeddings are unavailable"""
        
        query_words = query.lower().split()
        results = []
        
        if database_filter in ["all", "ai_memories"]:
            # Search AI memories with text matching and filters
            sql = "SELECT * FROM curated_memories WHERE 1=1"
            params = []
            
            # Add content search
            content_conditions = []
            for word in query_words:
                content_conditions.append("LOWER(content) LIKE ?")
                params.append(f"%{word}%")
            
            if content_conditions:
                sql += f" AND ({' OR '.join(content_conditions)})"
            
            # Add importance filters
            if min_importance is not None:
                sql += " AND importance_level >= ?"
                params.append(min_importance)
                
            if max_importance is not None:
                sql += " AND importance_level <= ?"
                params.append(max_importance)
                
            if memory_type is not None:
                sql += " AND memory_type = ?"
                params.append(memory_type)
            
            sql += " ORDER BY importance_level DESC LIMIT ?"
            params.append(limit)
            
            rows = await self.ai_memory_db.execute_query(sql, params)
            for row in rows:
                results.append({
                    "type": "ai_memory",
                    "similarity_score": 0.5,
                    "data": dict(row)
                })
        
        if database_filter in ["all", "conversations"]:
            # Search conversations with text matching
            for word in query_words:
                rows = await self.conversations_db.execute_query(
                    "SELECT * FROM messages WHERE LOWER(content) LIKE ? ORDER BY timestamp DESC LIMIT ?",
                    (f"%{word}%", limit)
                )
                for row in rows:
                    results.append({
                        "type": "conversation",
                        "similarity_score": 0.5,
                        "data": dict(row)
                    })
        
        # Remove duplicates and limit results
        seen = set()
        unique_results = []
        for result in results:
            key = f"{result['type']}_{result['data'].get('memory_id', result['data'].get('message_id', ''))}"
            if key not in seen:
                seen.add(key)
                unique_results.append(result)
        
        return {
            "status": "success",
            "query": query,
            "results": unique_results[:limit],
            "count": len(unique_results[:limit]),
            "search_type": "text_based",
            "note": "Used text-based search (embeddings unavailable)"
        }
    
    # System maintenance
    async def run_database_maintenance(self, force: bool = False) -> Dict:
        """Run maintenance on all databases.
        
        This includes:
        - Optimizing indexes
        - Cleaning up orphaned records
        - Updating statistics
        - Validating data consistency
        
        Args:
            force: Whether to force maintenance even if recent
            
        Returns:
            Dict containing maintenance results
        """
        results = {
            "status": "success",
            "databases": {},
            "timestamp": get_current_timestamp()
        }
        
        try:
            # Run maintenance on each database
            results["databases"]["ai_memories"] = await self.ai_memory_db.run_maintenance(force)
            results["databases"]["conversations"] = await self.conversations_db.run_maintenance(force)
            results["databases"]["schedule"] = await self.schedule_db.run_maintenance(force)
            results["databases"]["vscode"] = await self.vscode_db.run_maintenance(force)
            results["databases"]["mcp"] = await self.mcp_db.run_maintenance(force)
            
            # Check for failed maintenance
            failed = [db for db, result in results["databases"].items() 
                     if result["status"] == "error"]
            
            if failed:
                results["status"] = "partial"
                results["message"] = f"Maintenance failed for: {', '.join(failed)}"
            else:
                results["message"] = "All maintenance tasks completed successfully"
                
        except Exception as e:
            results["status"] = "error"
            results["message"] = str(e)
            logger.error(f"System maintenance error: {e}")
            
        return results
    
    # Embedding helper methods (async background tasks)
    async def _add_embedding_to_message(self, message_id: str, content: str):
        """Add embedding to a message (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.conversations_db.execute_update(
                    "UPDATE messages SET embedding = ? WHERE message_id = ?",
                    (embedding_blob, message_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to message {message_id}: {e}")
    
    async def _add_embedding_to_memory(self, memory_id: str, content: str):
        """Add embedding to a memory (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.ai_memory_db.execute_update(
                    "UPDATE curated_memories SET embedding = ? WHERE memory_id = ?",
                    (embedding_blob, memory_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to memory {memory_id}: {e}")
    
    async def _add_embedding_to_appointment(self, appointment_id: str, content: str):
        """Add embedding to an appointment (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.schedule_db.execute_update(
                    "UPDATE appointments SET embedding = ? WHERE appointment_id = ?",
                    (embedding_blob, appointment_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to appointment {appointment_id}: {e}")
    
    async def _add_embedding_to_reminder(self, reminder_id: str, content: str):
        """Add embedding to a reminder (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.schedule_db.execute_update(
                    "UPDATE reminders SET embedding = ? WHERE reminder_id = ?",
                    (embedding_blob, reminder_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to reminder {reminder_id}: {e}")
    
    async def _add_embedding_to_project_insight(self, insight_id: str, content: str):
        """Add embedding to a project insight (background task)"""
        try:
            embedding = await self.embedding_service.generate_embedding(content)
            if embedding:
                embedding_blob = np.array(embedding, dtype=np.float32).tobytes()
                await self.vscode_db.execute_update(
                    "UPDATE project_insights SET embedding = ? WHERE insight_id = ?",
                    (embedding_blob, insight_id)
                )
        except Exception as e:
            logger.error(f"Error adding embedding to project insight {insight_id}: {e}")


# =============================================================================
# MCP SERVER INTEGRATION (Optional - for Model Context Protocol support)
# =============================================================================

# The following code provides MCP server functionality when needed
# To use as MCP server, run: python ai_memory_core.py

async def main():
    """Main entry point - can be used for testing or as MCP server"""
    
    # Initialize the memory system
    memory = PersistentAIMemorySystem()
    
    # Example usage
    print("üß† Persistent AI Memory System - Enhanced Version")
    print("=" * 50)
    
    # Test system health
    health = await memory.get_system_health()
    print(f"System Status: {health['status']}")
    print(f"Databases: {len(health['databases'])} active")
    
    # Test memory creation
    result = await memory.create_memory(
        "This is a test memory with high importance",
        memory_type="test",
        importance_level=8,
        tags=["test", "demo"]
    )
    print(f"‚úÖ Created memory: {result['memory_id']}")
    
    # Test search
    search_results = await memory.search_memories("test memory", limit=5)
    print(f"üîç Found {search_results['count']} memories matching 'test memory'")
    
    print("\n‚ú® Memory system is ready for use!")
    print("üìö Features available:")
    print("   ‚Ä¢ 5 specialized databases")
    print("   ‚Ä¢ Vector semantic search")
    print("   ‚Ä¢ Real-time file monitoring")
    print("   ‚Ä¢ Schedule management")
    print("   ‚Ä¢ Project context tracking")
    print("   ‚Ä¢ MCP tool call logging")

if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/check_db.py
[file content begin]

[file content end]

/soulforge/persistent-ai-memory-new/database_maintenance.py
[file content begin]
#!/usr/bin/env python3
"""
Friday Memory System - Database Maintenance Module

Provides automated cleanup, optimization, and retention policies for the memory system.
"""

import asyncio
import sqlite3
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class DatabaseMaintenance:
    """Handles automated database cleanup and optimization"""
    
    def __init__(self, memory_system):
        self.memory_system = memory_system
        self.retention_policies = {
            "conversations": {
                "max_age_days": 90,  # Keep conversations for 3 months
                "max_count": 10000,  # Keep max 10k conversations
                "preserve_important": True  # Keep high-importance items
            },
            "curated_memories": {
                "max_age_days": 365,  # Keep memories for 1 year
                "max_count": 5000,   # Keep max 5k memories
                "preserve_important": True
            },
            "schedule": {
                "max_age_days": 30,  # Keep old appointments/reminders for 1 month
                "cleanup_completed": True  # Remove completed items
            },
            "mcp_tool_calls": {
                "max_age_days": 30,  # Keep tool call logs for 1 month
                "max_count": 50000   # Keep max 50k tool calls
            }
        }
    
    async def run_maintenance(self, force: bool = False) -> Dict:
        """Run full database maintenance"""
        logger.info("üßπ Starting database maintenance...")
        
        results = {
            "maintenance_timestamp": datetime.now(timezone.utc).isoformat(),
            "cleanup_results": {},
            "optimization_results": {},
            "statistics": {},
            "schema_upgrades": []
        }
        
        try:
            # 0. Apply any needed schema upgrades
            logger.info("üîÑ Checking and applying schema upgrades...")
            schema_upgrades = await self._upgrade_schemas()
            results["schema_upgrades"] = schema_upgrades
            
            # 1. Clean up old data based on retention policies
            logger.info("üìÖ Applying retention policies...")
            results["cleanup_results"] = await self._apply_retention_policies(force)
            
            # 2. Remove duplicate entries (shouldn't be many with our new system)
            logger.info("üîç Removing any remaining duplicates...")
            results["cleanup_results"]["duplicates"] = await self._remove_duplicates()
            
            # 3. Optimize database performance
            logger.info("‚ö° Optimizing database performance...")
            results["optimization_results"] = await self._optimize_databases()
            
            # 4. Collect post-cleanup statistics
            logger.info("üìä Collecting statistics...")
            results["statistics"] = await self._collect_statistics()
            
            logger.info("‚úÖ Database maintenance completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Database maintenance failed: {e}")
            results["error"] = str(e)
        
        return results
    
    async def _apply_retention_policies(self, force: bool = False) -> Dict:
        """Apply retention policies to remove old data"""
        cleanup_results = {}
        
        # Clean conversations
        cleanup_results["conversations"] = await self._cleanup_conversations()
        
        # Clean AI memories (more conservative)
        cleanup_results["curated_memories"] = await self._cleanup_ai_memories()
        
        # Clean old schedule items
        cleanup_results["schedule"] = await self._cleanup_schedule()
        
        # Clean old tool call logs
        cleanup_results["mcp_tool_calls"] = await self._cleanup_tool_calls()
        
        return cleanup_results
    
    async def _cleanup_conversations(self) -> Dict:
        """Clean up old conversation data"""
        policy = self.retention_policies["conversations"]
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=policy["max_age_days"])
        
        # Get conversation statistics before cleanup
        before_stats = await self._get_conversation_stats()
        
        # Delete old conversations (but preserve important ones)
        if policy.get("preserve_important"):
            # Keep conversations with high engagement or marked as important
            delete_query = """
                DELETE FROM conversations 
                WHERE start_timestamp < ? 
                AND conversation_id NOT IN (
                    SELECT DISTINCT conversation_id FROM messages 
                    WHERE json_extract(metadata, '$.importance_level') >= 7
                    OR json_extract(metadata, '$.preserve') = 'true'
                )
                AND conversation_id NOT IN (
                    SELECT conversation_id FROM conversations c
                    WHERE (
                        SELECT COUNT(*) FROM messages m 
                        WHERE m.conversation_id = c.conversation_id
                    ) >= 10  -- Keep conversations with 10+ messages
                )
            """
        else:
            delete_query = "DELETE FROM conversations WHERE start_timestamp < ?"
        
        # Execute cleanup
        deleted_conversations = await self.memory_system.conversations_db.execute_update(
            delete_query, (cutoff_date.isoformat(),)
        )
        
        # Clean up orphaned messages
        await self.memory_system.conversations_db.execute_update(
            "DELETE FROM messages WHERE conversation_id NOT IN (SELECT conversation_id FROM conversations)"
        )
        
        # Get statistics after cleanup
        after_stats = await self._get_conversation_stats()
        
        return {
            "policy_applied": policy,
            "cutoff_date": cutoff_date.isoformat(),
            "conversations_before": before_stats["conversation_count"],
            "conversations_after": after_stats["conversation_count"],
            "conversations_deleted": before_stats["conversation_count"] - after_stats["conversation_count"],
            "messages_before": before_stats["message_count"],
            "messages_after": after_stats["message_count"],
            "messages_deleted": before_stats["message_count"] - after_stats["message_count"]
        }
    
    async def _cleanup_ai_memories(self) -> Dict:
        """Clean up old AI memory data (more conservative)"""
        policy = self.retention_policies["curated_memories"]
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=policy["max_age_days"])
        
        before_count = len(await self.memory_system.ai_memory_db.execute_query(
            "SELECT memory_id FROM curated_memories", ()
        ))
        
        # Only delete low-importance, old memories
        deleted = await self.memory_system.ai_memory_db.execute_update(
            """DELETE FROM curated_memories 
               WHERE created_at < ? 
               AND importance_level < 5 
               AND memory_type NOT IN ('safety', 'critical', 'preference')""",
            (cutoff_date.isoformat(),)
        )
        
        after_count = len(await self.memory_system.ai_memory_db.execute_query(
            "SELECT memory_id FROM curated_memories", ()
        ))
        
        return {
            "policy_applied": policy,
            "cutoff_date": cutoff_date.isoformat(),
            "memories_before": before_count,
            "memories_after": after_count,
            "memories_deleted": before_count - after_count
        }
    
    async def _cleanup_schedule(self) -> Dict:
        """Clean up completed and old schedule items"""
        policy = self.retention_policies["schedule"]
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=policy["max_age_days"])
        now = datetime.now(timezone.utc).isoformat()
        
        # Auto-complete overdue reminders (assume they're done) - with 24 hour grace period
        grace_period_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        overdue_completed = await self.memory_system.schedule_db.execute_update(
            "UPDATE reminders SET completed = 1, completed_at = ? WHERE due_datetime < ? AND completed = 0",
            (now, grace_period_cutoff.isoformat())
        )
        
        # Clean old completed appointments
        old_appointments = await self.memory_system.schedule_db.execute_update(
            "DELETE FROM appointments WHERE scheduled_datetime < ?",
            (cutoff_date.isoformat(),)
        )
        
        # Clean old completed reminders
        old_reminders = await self.memory_system.schedule_db.execute_update(
            "DELETE FROM reminders WHERE due_datetime < ? AND completed = 1",
            (cutoff_date.isoformat(),)
        )
        
        return {
            "policy_applied": policy,
            "cutoff_date": cutoff_date.isoformat(),
            "overdue_reminders_auto_completed": overdue_completed,
            "old_appointments_deleted": old_appointments,
            "old_reminders_deleted": old_reminders
        }
    
    async def _cleanup_tool_calls(self) -> Dict:
        """Clean up old tool call logs"""
        policy = self.retention_policies["mcp_tool_calls"]
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=policy["max_age_days"])
        
        before_count = len(await self.memory_system.mcp_db.execute_query(
            "SELECT call_id FROM tool_calls", ()
        ))
        
        # Delete old tool calls
        deleted = await self.memory_system.mcp_db.execute_update(
            "DELETE FROM tool_calls WHERE timestamp < ?",
            (cutoff_date.isoformat(),)
        )
        
        after_count = len(await self.memory_system.mcp_db.execute_query(
            "SELECT call_id FROM tool_calls", ()
        ))
        
        return {
            "policy_applied": policy,
            "cutoff_date": cutoff_date.isoformat(),
            "tool_calls_before": before_count,
            "tool_calls_after": after_count,
            "tool_calls_deleted": before_count - after_count
        }
    
    async def _remove_duplicates(self) -> Dict:
        """Remove any remaining duplicate entries with strict deduplication"""
        results = {}

        # Deduplicate messages by (content, role, conversation_id), keep entry with earliest timestamp
        dedup_query_messages = '''
            DELETE FROM messages
            WHERE message_id NOT IN (
                SELECT message_id FROM (
                    SELECT message_id
                    FROM messages m1
                    WHERE timestamp = (
                        SELECT MIN(timestamp)
                        FROM messages m2
                        WHERE m2.content = m1.content
                          AND m2.role = m1.role
                          AND m2.conversation_id = m1.conversation_id
                    )
                )
            )
        '''
        duplicate_messages = await self.memory_system.conversations_db.execute_update(dedup_query_messages)
        results["duplicate_messages_removed"] = duplicate_messages

        # Deduplicate curated memories by (content, memory_type, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_memories = '''
            DELETE FROM curated_memories
            WHERE memory_id NOT IN (
                SELECT memory_id FROM (
                    SELECT memory_id
                    FROM curated_memories m1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM curated_memories m2
                        WHERE m2.content = m1.content
                          AND m2.memory_type = m1.memory_type
                          AND (m2.source_conversation_id IS m1.source_conversation_id OR (m2.source_conversation_id IS NULL AND m1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_memories = await self.memory_system.ai_memory_db.execute_update(dedup_query_memories)
        results["duplicate_memories_removed"] = duplicate_memories

        # Deduplicate reminders by (content, due_datetime, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_reminders = '''
            DELETE FROM reminders
            WHERE reminder_id NOT IN (
                SELECT reminder_id FROM (
                    SELECT reminder_id
                    FROM reminders r1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM reminders r2
                        WHERE r2.content = r1.content
                          AND r2.due_datetime = r1.due_datetime
                          AND (r2.source_conversation_id IS r1.source_conversation_id OR (r2.source_conversation_id IS NULL AND r1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_reminders = await self.memory_system.schedule_db.execute_update(dedup_query_reminders)
        results["duplicate_reminders_removed"] = duplicate_reminders

        # Deduplicate appointments by (title, scheduled_datetime, location, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_appointments = '''
            DELETE FROM appointments
            WHERE appointment_id NOT IN (
                SELECT appointment_id FROM (
                    SELECT appointment_id
                    FROM appointments a1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM appointments a2
                        WHERE a2.title = a1.title
                          AND a2.scheduled_datetime = a1.scheduled_datetime
                          AND (a2.location IS a1.location OR (a2.location IS NULL AND a1.location IS NULL))
                          AND (a2.source_conversation_id IS a1.source_conversation_id OR (a2.source_conversation_id IS NULL AND a1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_appointments = await self.memory_system.schedule_db.execute_update(dedup_query_appointments)
        results["duplicate_appointments_removed"] = duplicate_appointments

        return results
    
    async def _upgrade_messages_schema(self) -> List[str]:
        """Upgrade messages table schema if needed"""
        upgrades_applied = []
        
        try:
            # Use raw connection for schema modification
            conn = self.memory_system.conversations_db.get_connection()
            try:
                # First check if we need to modify the source_type column
                cursor = conn.execute("""SELECT sql FROM sqlite_master 
                                      WHERE type='table' AND name='messages'""")
                table_sql = cursor.fetchone()[0]
                
                if 'source_type TEXT NOT NULL' in table_sql:
                    # We need to modify the constraint
                    logger.info("Updating messages table source_type constraint")
                    
                    # Create new table with modified schema
                    conn.execute("""CREATE TABLE messages_new (
                        message_id TEXT PRIMARY KEY,
                        conversation_id TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        role TEXT NOT NULL,
                        content TEXT NOT NULL,
                        source_type TEXT DEFAULT 'unknown',
                        source_id TEXT,
                        source_url TEXT,
                        source_metadata TEXT,
                        sync_status TEXT,
                        last_sync TEXT,
                        metadata TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
                    )""")
                    
                    # Copy data
                    conn.execute("""INSERT INTO messages_new 
                                  SELECT * FROM messages""")
                    
                    # Drop old table and rename new one
                    conn.execute("DROP TABLE messages")
                    conn.execute("ALTER TABLE messages_new RENAME TO messages")
                    conn.commit()
                    
                    upgrades_applied.append("updated_messages_source_type")
                    logger.info("Successfully updated messages table schema")
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"Error upgrading messages schema: {e}")
            
        return upgrades_applied

    async def _optimize_databases(self) -> Dict:
        """Optimize database performance"""
        results = {}
        
        # List of database paths to optimize
        db_paths = [
            self.memory_system.conversations_db.db_path,
            self.memory_system.ai_memory_db.db_path,
            self.memory_system.schedule_db.db_path,
            self.memory_system.vscode_db.db_path,
            self.memory_system.mcp_db.db_path
        ]
        
        for db_path in db_paths:
            db_name = Path(db_path).stem
            try:
                # Get database size before optimization
                size_before = Path(db_path).stat().st_size if Path(db_path).exists() else 0
                
                # Optimize database
                conn = sqlite3.connect(db_path)
                conn.execute("VACUUM")  # Reclaim space and defragment
                conn.execute("REINDEX")  # Rebuild indexes for better performance
                conn.execute("ANALYZE")  # Update query planner statistics
                conn.close()
                
                # Get size after optimization
                size_after = Path(db_path).stat().st_size if Path(db_path).exists() else 0
                
                results[db_name] = {
                    "size_before_mb": round(size_before / 1024 / 1024, 2),
                    "size_after_mb": round(size_after / 1024 / 1024, 2),
                    "space_saved_mb": round((size_before - size_after) / 1024 / 1024, 2),
                    "optimized": True
                }
                
            except Exception as e:
                results[db_name] = {"error": str(e), "optimized": False}
        
        return results
    
    async def _upgrade_schemas(self) -> List[str]:
        """Apply any needed schema upgrades"""
        upgrades_applied = []
        
        try:
            # 1. Upgrade development_conversations table
            logger.info("Checking development_conversations schema...")
            conn = self.memory_system.vscode_db.get_connection()
            try:
                # Check if column exists
                cursor = conn.execute("""SELECT COUNT(*) 
                                      FROM pragma_table_info('development_conversations') 
                                      WHERE name='source_metadata'""")
                has_column = cursor.fetchone()[0] > 0
                
                if not has_column:
                    logger.info("Adding source_metadata column to development_conversations table")
                    # Add the column
                    conn.execute("""ALTER TABLE development_conversations 
                                  ADD COLUMN source_metadata TEXT""")
                    conn.commit()
                    upgrades_applied.append("added_source_metadata_column")
                    logger.info("Successfully added source_metadata column")
            finally:
                conn.close()
                
            # 2. Upgrade messages table schema
            logger.info("Checking messages table schema...")
            messages_upgrades = await self._upgrade_messages_schema()
            upgrades_applied.extend(messages_upgrades)
                
        except Exception as e:
            logger.error(f"Error during schema upgrades: {e}")
            
        return upgrades_applied

    async def _get_conversation_stats(self) -> Dict:
        """Get current conversation statistics"""
        conversations = await self.memory_system.conversations_db.execute_query(
            "SELECT COUNT(*) as count FROM conversations", ()
        )
        messages = await self.memory_system.conversations_db.execute_query(
            "SELECT COUNT(*) as count FROM messages", ()
        )
        
        return {
            "conversation_count": conversations[0]["count"],
            "message_count": messages[0]["count"]
        }
    
    async def _collect_statistics(self) -> Dict:
        """Collect database statistics after maintenance"""
        stats = {}
        
        # Get system health for current statistics
        health = await self.memory_system.get_system_health()
        
        for db_name, db_info in health["databases"].items():
            if "count" in str(db_info):
                # Extract counts from the database info
                stats[db_name] = {
                    key: value for key, value in db_info.items() 
                    if "count" in key or "size" in key
                }
        
        return stats


# Integration function to add to PersistentAIMemorySystem
async def run_database_maintenance(memory_system, force: bool = False) -> Dict:
    """Convenience function to run database maintenance"""
    maintenance = DatabaseMaintenance(memory_system)
    return await maintenance.run_maintenance(force)
[file content end]

/soulforge/persistent-ai-memory-new/embedding_config.json
[file content begin]
{
  "embedding_configuration": {
    "primary": {
      "provider": "local",
      "model": "local-deterministic-128",
      "base_url": "",
      "description": "Local deterministic embeddings for offline testing"
    },
    "fallback": {
      "provider": "local",
      "model": "local-deterministic-128",
      "base_url": "",
      "description": "Local deterministic embeddings"
    },
    "options": {
      "openai": {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "base_url": "https://api.openai.com/v1",
        "api_key": "your-openai-api-key-here",
        "description": "OpenAI embeddings (requires API key)"
      },
      "custom": {
        "provider": "custom",
        "model": "your-model-name",
        "base_url": "http://your-server:port",
        "description": "Custom embedding server"
      }
    }
  },
  "instructions": {
    "setup": [
      "1. Edit this file to configure your preferred embedding providers",
      "2. Set 'primary' to your main embedding service",
      "3. Set 'fallback' to a backup service",
      "4. For Ollama: Make sure qwen2.5:1.5b is pulled (ollama pull qwen2.5:1.5b)",
      "5. For LM Studio: Load an embedding model",
      "6. For OpenAI: Add your API key",
      "7. Restart the memory system to apply changes"
    ],
    "providers": {
      "ollama": "Fastest and most efficient for local use",
      "lm_studio": "Good balance of speed and quality", 
      "openai": "High quality but requires internet and API costs",
      "custom": "For custom embedding servers"
    }
  }
}
[file content end]

/soulforge/persistent-ai-memory-new/examples/README.md
[file content begin]
# Examples for Persistent AI Memory System

This directory contains practical examples demonstrating how to use the Persistent AI Memory System in real-world scenarios. These examples show the full potential of AI assistants with persistent memory and self-reflection capabilities.

## Example Files Overview

### Getting Started

#### `basic_usage.py`
- **Purpose**: Introduction to core functionality
- **Demonstrates**: Memory storage, search, conversation handling, file monitoring
- **Best for**: New users, understanding basic concepts
- **Key features**: Step-by-step examples with explanations

### Advanced Features

#### `advanced_usage.py`
- **Purpose**: Complex scenarios and advanced features
- **Demonstrates**: MCP tool integration, AI self-reflection, strategic memory management
- **Best for**: Power users, production implementations
- **Key features**: Real-world workflows, optimization strategies

### Performance and Testing

#### `performance_tests.py`
- **Purpose**: Benchmarking and performance validation
- **Demonstrates**: Load testing, concurrent operations, performance metrics
- **Best for**: System optimization, capacity planning
- **Key features**: Detailed performance analysis, bottleneck identification

## Use Case Examples

### ü§ñ AI Assistant Memory
```python
# Store user preferences
await store_memory(
    content="User prefers detailed explanations with examples",
    memory_type="user_preference",
    importance_level=8,
    tags=["communication", "style", "preference"]
)

# Later, AI retrieves context
memories = await search_memories("user communication style")
# AI now knows to provide detailed explanations!
```

### üí¨ Conversation Context
```python
# Automatic conversation storage
await store_conversation(
    user_message="Help me debug this Python error",
    assistant_response="I see the issue. Let me check similar problems you've had...",
    session_id="debugging_session_001"
)

# AI learns from conversation history
history = await get_conversation_history("debugging_session_001")
# AI provides context-aware help based on past interactions
```

### üß† AI Self-Reflection
```python
# AI analyzes its own tool usage
reflection = await reflect_on_tool_usage(days=7)
print(reflection['insights'])
# "I notice I'm frequently searching for database-related memories. 
#  The user seems to be working on database optimization projects."

# AI adapts behavior based on self-analysis
recommendations = reflection['recommendations']
# ["Consider storing more detailed database performance memories",
#  "Increase importance level for SQL optimization insights"]
```

### üîÑ Learning from Patterns
```python
# AI discovers patterns in user behavior
patterns = await get_tool_usage_summary(days=30)
insights = patterns['insights']

if insights['most_used_tool'] == 'search_memories':
    # AI realizes user relies heavily on memory search
    # Could optimize search algorithms or suggest better tagging
    pass
```

## Real-World Scenarios

### Scenario 1: Software Development Assistant

An AI assistant helping with coding projects:

1. **Learns coding style**: Stores preferences for code formatting, naming conventions
2. **Remembers project context**: Tracks architecture decisions, design patterns used
3. **Builds debugging knowledge**: Stores solutions to specific errors encountered
4. **Reflects on effectiveness**: Analyzes which types of help are most useful

Example workflow:
```python
# Day 1: Store project context
await store_memory(
    content="Project uses microservices architecture with FastAPI and PostgreSQL",
    memory_type="project_context",
    importance_level=9,
    tags=["architecture", "fastapi", "postgresql", "microservices"]
)

# Day 5: AI uses context to provide relevant help
context_memories = await search_memories("project architecture database")
# AI suggests PostgreSQL-specific optimizations for FastAPI project
```

### Scenario 2: Research Assistant

An AI helping with academic research:

1. **Tracks research topics**: Stores paper summaries, key findings
2. **Connects related concepts**: Uses semantic search to find connections
3. **Monitors progress**: Reflects on research direction and productivity
4. **Suggests next steps**: Based on gaps in current knowledge

### Scenario 3: Personal Productivity Assistant

An AI managing daily tasks and learning user habits:

1. **Learns work patterns**: Stores peak productivity times, preferred task types
2. **Adapts scheduling**: Uses memory to suggest optimal task timing
3. **Tracks goal progress**: Monitors long-term objectives and milestones
4. **Provides contextual reminders**: Based on current projects and deadlines

## Implementation Patterns

### Pattern 1: Importance-Based Memory Management
```python
# Critical information gets high importance
await store_memory(
    content="Emergency contact: John Doe - 555-0123",
    importance_level=10,  # Maximum importance
    memory_type="emergency_contact"
)

# Regular preferences get medium importance
await store_memory(
    content="Prefers coffee over tea in meetings",
    importance_level=5,  # Medium importance
    memory_type="personal_preference"
)
```

### Pattern 2: Strategic Tagging
```python
# Use hierarchical tags for better organization
tags = [
    "project:friday",           # Project identifier
    "component:memory_system",  # System component
    "issue:performance",        # Issue type
    "status:resolved"          # Current status
]

await store_memory(content="...", tags=tags)
```

### Pattern 3: Context-Aware Search
```python
# Combine multiple search terms for better context
query = f"user preferences {current_project} {current_task_type}"
relevant_memories = await search_memories(query, limit=5)

# Use memory importance for prioritization
high_priority = [m for m in relevant_memories if m['importance_level'] >= 8]
```

## Performance Optimization Examples

### Batch Operations
```python
# Store multiple memories efficiently
memories_to_store = [
    {"content": "Memory 1", "memory_type": "batch_test"},
    {"content": "Memory 2", "memory_type": "batch_test"},
    {"content": "Memory 3", "memory_type": "batch_test"},
]

# Process in batches for better performance
for memory_data in memories_to_store:
    await store_memory(**memory_data)
```

### Optimized Search
```python
# Use specific queries for faster results
specific_query = "Python database optimization SQLite"  # Good
vague_query = "help with code"  # Less effective

# Limit results to what you actually need
results = await search_memories(specific_query, limit=3)  # Faster
results = await search_memories(specific_query, limit=100)  # Slower
```

## Error Handling Examples

### Graceful Degradation
```python
try:
    memories = await search_memories(user_query)
    if not memories:
        # Fallback to broader search
        memories = await search_memories(f"general {user_query}")
except Exception as e:
    # Log error but continue operation
    print(f"Memory search failed: {e}")
    memories = []  # Empty list allows system to continue

# Always provide helpful response, even without memory context
if memories:
    response = f"Based on what I remember about {user_query}..."
else:
    response = f"I'll help you with {user_query}..."
```

### Retry Logic
```python
import asyncio

async def robust_memory_operation(operation_func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await operation_func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

## Integration Examples

### With Web Applications
```python
# FastAPI integration
from fastapi import FastAPI
from mcp_server import PersistentAIMemoryMCPServer

app = FastAPI()
memory_server = PersistentAIMemoryMCPServer()

@app.post("/chat")
async def chat_endpoint(message: str, session_id: str):
    # Search for relevant context
    context = await memory_server.handle_mcp_request({
        "tool": "search_memories",
        "parameters": {"query": message, "limit": 5}
    }, client_id=session_id)
    
    # Generate AI response using context
    ai_response = generate_response(message, context)
    
    # Store conversation for future context
    await memory_server.handle_mcp_request({
        "tool": "store_conversation",
        "parameters": {
            "user_message": message,
            "assistant_response": ai_response,
            "session_id": session_id
        }
    }, client_id=session_id)
    
    return {"response": ai_response}
```

### With CLI Applications
```python
# Command-line AI assistant
import sys
from mcp_server import PersistentAIMemoryMCPServer

async def cli_assistant():
    memory_server = PersistentAIMemoryMCPServer()
    session_id = "cli_session"
    
    print("AI Assistant with Memory - Type 'quit' to exit")
    
    while True:
        user_input = input("\nYou: ")
        if user_input.lower() == 'quit':
            break
            
        # Search for relevant memories
        context_request = {
            "tool": "search_memories",
            "parameters": {"query": user_input, "limit": 3}
        }
        context = await memory_server.handle_mcp_request(context_request, session_id)
        
        # Generate response (implement your AI logic here)
        ai_response = f"Based on my memory of similar topics: {user_input}"
        
        # Store conversation
        conv_request = {
            "tool": "store_conversation",
            "parameters": {
                "user_message": user_input,
                "assistant_response": ai_response,
                "session_id": session_id
            }
        }
        await memory_server.handle_mcp_request(conv_request, session_id)
        
        print(f"AI: {ai_response}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(cli_assistant())
```

## Running Examples

### Prerequisites
1. Ensure the system is properly installed
2. LM Studio running (for embedding examples)
3. Database initialized (run a health check first)

### Execute Examples
```bash
# Start with basic usage
python basic_usage.py

# Explore advanced features
python advanced_usage.py

# Test performance
python performance_tests.py
```

### Customization
All examples can be modified for your specific use case:
- Change embedding service URLs
- Adjust memory importance levels
- Modify tagging strategies
- Add custom search logic

## Best Practices from Examples

1. **Strategic Memory Storage**: Use importance levels and tags thoughtfully
2. **Context-Aware Operations**: Combine multiple data sources for better AI responses
3. **Error Resilience**: Always handle failures gracefully
4. **Performance Monitoring**: Use the reflection tools to optimize system usage
5. **Incremental Learning**: Let the AI build up knowledge over time

## Next Steps

After exploring these examples:

1. **Adapt for your use case**: Modify examples to fit your specific needs
2. **Monitor performance**: Use the performance tests to optimize your setup
3. **Extend functionality**: Add custom tools and memory types
4. **Share learnings**: Contribute back improvements and new examples

---

üéØ **Goal**: These examples demonstrate how AI assistants can become truly intelligent by learning from interactions, building contextual understanding, and continuously improving their effectiveness through persistent memory and self-reflection.

üöÄ **Result**: AI assistants that get better over time, understand user preferences, and provide increasingly relevant and helpful responses based on accumulated knowledge and experience!
[file content end]

/soulforge/persistent-ai-memory-new/examples/advanced_usage.py
[file content begin]
#!/usr/bin/env python3
"""
Advanced usage examples showing the full power of the Persistent AI Memory System
"""

import asyncio
import json
from datetime import datetime, timezone
from mcp_server import PersistentAIMemoryMCPServer

async def mcp_tool_integration_example():
    """Demonstrate MCP server tool integration with logging"""
    
    print("üîß MCP Tool Integration with Logging Example")
    print("=" * 50)
    
    # Initialize MCP server
    server = PersistentAIMemoryMCPServer()
    
    print("üìù Simulating AI assistant tool calls...")
    
    # Simulate storing a memory via MCP
    request1 = {
        "tool": "store_memory",
        "parameters": {
            "content": "User enjoys working with SQLite databases and appreciates clean schema design",
            "memory_type": "preference",
            "importance_level": 7,
            "tags": ["database", "sqlite", "design"]
        }
    }
    
    result1 = await server.handle_mcp_request(request1, client_id="example_ai")
    print(f"   ‚úÖ Store memory result: {result1['status']}")
    print(f"   üìä Execution time: {result1.get('execution_time_ms', 0):.2f}ms")
    print(f"   üîó Call ID: {result1.get('call_id', 'N/A')[:8]}...")
    
    # Simulate searching memories
    request2 = {
        "tool": "search_memories",
        "parameters": {
            "query": "database preferences",
            "limit": 5
        }
    }
    
    result2 = await server.handle_mcp_request(request2, client_id="example_ai")
    print(f"   ‚úÖ Search memories result: {result2['status']}")
    print(f"   üìä Execution time: {result2.get('execution_time_ms', 0):.2f}ms")
    
    # Get tool usage summary (AI self-reflection!)
    request3 = {
        "tool": "get_tool_usage_summary",
        "parameters": {"days": 1}
    }
    
    result3 = await server.handle_mcp_request(request3, client_id="example_ai")
    if result3['status'] == 'success':
        insights = result3['result']['insights']
        print(f"\nüß† AI Self-Reflection Results:")
        print(f"   üìä Total tool calls: {insights['total_tool_calls']}")
        print(f"   ‚úÖ Success rate: {insights['success_rate_percent']}%")
        print(f"   üí≠ AI's reflection: {insights['reflection']}")
    
    # Demonstrate reflection on tool usage patterns
    request4 = {
        "tool": "reflect_on_tool_usage",
        "parameters": {"days": 1}
    }
    
    result4 = await server.handle_mcp_request(request4, client_id="example_ai")
    if result4['status'] == 'success':
        reflection = result4['result']['reflection']
        print(f"\nüîç Usage Pattern Analysis:")
        if reflection['patterns']['peak_usage_tools']:
            for tool_pattern in reflection['patterns']['peak_usage_tools'][:2]:
                print(f"   üìà {tool_pattern['insight']}")
        
        if reflection['recommendations']:
            print(f"   üí° Recommendations:")
            for rec in reflection['recommendations']:
                print(f"     ‚Ä¢ {rec}")
    
    print("\n‚úÖ MCP tool integration example completed!")


async def advanced_memory_management():
    """Show advanced memory management techniques"""
    
    print("\nüéØ Advanced Memory Management Example")
    print("=" * 45)
    
    server = PersistentAIMemoryMCPServer()
    
    # Store different types of memories with strategic importance levels
    memories_to_store = [
        {
            "content": "User's primary programming language is Python, secondary is JavaScript",
            "memory_type": "skill_profile",
            "importance_level": 9,
            "tags": ["skills", "programming", "languages"]
        },
        {
            "content": "Project 'Friday' uses SQLite for persistence, aiohttp for async HTTP, watchdog for file monitoring",
            "memory_type": "project_context",
            "importance_level": 8,
            "tags": ["friday", "project", "dependencies", "architecture"]
        },
        {
            "content": "User prefers detailed commit messages and comprehensive documentation",
            "memory_type": "work_style",
            "importance_level": 7,
            "tags": ["git", "documentation", "style"]
        },
        {
            "content": "Breakthrough: Fixed foreign key constraints by letting store_message auto-create sessions",
            "memory_type": "technical_insight", 
            "importance_level": 10,
            "tags": ["breakthrough", "database", "foreign_keys", "solution"]
        }
    ]
    
    print("üìù Storing strategically important memories...")
    stored_memories = []
    
    for memory_data in memories_to_store:
        request = {
            "tool": "store_memory",
            "parameters": memory_data
        }
        
        result = await server.handle_mcp_request(request, client_id="advanced_example")
        if result['status'] == 'success':
            stored_memories.append(result['result']['memory_id'])
            print(f"   ‚úÖ Stored {memory_data['memory_type']}: importance {memory_data['importance_level']}")
    
    # Demonstrate semantic search across different memory types
    search_queries = [
        "programming languages and skills",
        "database and persistence solutions", 
        "breakthrough solutions and insights"
    ]
    
    print(f"\nüîç Testing semantic search across memory types...")
    for query in search_queries:
        request = {
            "tool": "search_memories",
            "parameters": {"query": query, "limit": 3}
        }
        
        result = await server.handle_mcp_request(request, client_id="advanced_example")
        print(f"   üéØ '{query}': {len(result.get('result', {}).get('results', []))} matches")
    
    print("\n‚úÖ Advanced memory management example completed!")


async def real_world_workflow_simulation():
    """Simulate a real-world AI assistant workflow"""
    
    print("\nüåü Real-World AI Assistant Workflow Simulation")
    print("=" * 55)
    
    server = PersistentAIMemoryMCPServer()
    
    # Simulate a development session where AI helps with coding
    workflow_steps = [
        {
            "action": "store_memory",
            "params": {
                "content": "User is working on implementing MCP tool call logging for AI self-reflection",
                "memory_type": "current_task",
                "importance_level": 8,
                "tags": ["mcp", "logging", "current"]
            },
            "description": "AI remembers current task"
        },
        {
            "action": "search_memories", 
            "params": {"query": "database logging patterns", "limit": 5},
            "description": "AI searches for relevant past knowledge"
        },
        {
            "action": "store_memory",
            "params": {
                "content": "Tool call logging should include execution time, parameters, results, and error handling",
                "memory_type": "design_decision",
                "importance_level": 7,
                "tags": ["logging", "design", "requirements"]
            },
            "description": "AI stores design insights"
        },
        {
            "action": "get_system_health",
            "params": {},
            "description": "AI checks system status"
        },
        {
            "action": "reflect_on_tool_usage",
            "params": {"days": 1},
            "description": "AI reflects on its own behavior"
        }
    ]
    
    print("ü§ñ Simulating AI assistant workflow...")
    
    for i, step in enumerate(workflow_steps, 1):
        print(f"\n{i}. {step['description']}...")
        
        request = {
            "tool": step["action"],
            "parameters": step["params"]
        }
        
        result = await server.handle_mcp_request(request, client_id="workflow_simulation")
        status_emoji = "‚úÖ" if result['status'] == 'success' else "‚ùå"
        print(f"   {status_emoji} {step['action']}: {result['status']}")
        
        if step["action"] == "reflect_on_tool_usage" and result['status'] == 'success':
            reflection = result['result']['reflection']
            print(f"   üí≠ AI Self-Assessment: Analyzed {reflection['period_days']} days of tool usage")
    
    # Final reflection on the entire workflow
    print(f"\nüéØ Final Tool Usage Analysis...")
    final_request = {
        "tool": "get_tool_usage_summary", 
        "parameters": {"days": 1}
    }
    
    final_result = await server.handle_mcp_request(final_request, client_id="workflow_simulation")
    if final_result['status'] == 'success':
        insights = final_result['result']['insights']
        print(f"   üìä Session summary: {insights['total_tool_calls']} tool calls, {insights['success_rate_percent']}% success rate")
        print(f"   üß† AI's self-reflection: {insights['reflection']}")
    
    print("\n‚úÖ Real-world workflow simulation completed!")
    print("üí° This demonstrates how AI assistants can:")
    print("   ‚Ä¢ Store and retrieve contextual memories")
    print("   ‚Ä¢ Learn from past interactions")
    print("   ‚Ä¢ Reflect on their own behavior patterns")
    print("   ‚Ä¢ Continuously improve their effectiveness")


async def main():
    """Run all advanced examples"""
    
    print("üöÄ Advanced Examples for Persistent AI Memory System")
    print("=" * 60)
    print("These examples show the full potential of AI self-reflection and learning!")
    print()
    
    try:
        await mcp_tool_integration_example()
        await advanced_memory_management()
        await real_world_workflow_simulation()
        
        print("\nüéâ All advanced examples completed successfully!")
        print("\nüåü Key Takeaways:")
        print("   ‚Ä¢ AI assistants can now maintain persistent memories")
        print("   ‚Ä¢ Tool call logging enables AI self-reflection")
        print("   ‚Ä¢ Semantic search provides contextual memory retrieval")
        print("   ‚Ä¢ Memory importance levels prioritize critical information")
        print("   ‚Ä¢ Cross-platform conversation capture works automatically")
        print("\nüöÄ This is the foundation for truly intelligent AI assistants!")
        
    except Exception as e:
        print(f"\n‚ùå Error running advanced examples: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/examples/basic_usage.py
[file content begin]
#!/usr/bin/env python3
"""
Basic usage examples for the Persistent AI Memory System

This script demonstrates the core functionality and how to get started.
"""

import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def basic_usage_example():
    """Demonstrate basic memory operations"""
    
    print("üß† Persistent AI Memory System - Basic Usage Example")
    print("=" * 60)
    
    # Initialize the memory system
    print("üîß Initializing memory system...")
    memory = PersistentAIMemorySystem()
    
    # Store some memories
    print("\nüìù Storing memories...")
    
    # Store a user preference
    result1 = await memory.create_memory(
        content="User prefers concise technical explanations with code examples",
        memory_type="preference",
        importance_level=8,
        tags=["communication", "technical", "preferences"]
    )
    print(f"   ‚úÖ Stored preference memory: {result1['memory_id'][:8]}...")
    
    # Store a fact
    result2 = await memory.create_memory(
        content="The user is working on a Python project called 'Friday' - an AI assistant",
        memory_type="fact",
        importance_level=7,
        tags=["project", "python", "ai"]
    )
    print(f"   ‚úÖ Stored fact memory: {result2['memory_id'][:8]}...")
    
    # Store an insight
    result3 = await memory.create_memory(
        content="User tends to work late at night and prefers detailed documentation",
        memory_type="insight",
        importance_level=6,
        tags=["behavior", "documentation"]
    )
    print(f"   ‚úÖ Stored insight memory: {result3['memory_id'][:8]}...")
    
    # Search memories
    print("\nüîç Searching memories...")
    search_results = await memory.search_memories("technical explanations")
    print(f"   üìä Found {len(search_results.get('results', []))} results for 'technical explanations'")
    
    # Check system health
    print("\nüíö System health check...")
    health = await memory.get_system_health()
    print(f"   üåü Status: {health['status']}")
    if 'databases' in health and 'ai_memories' in health['databases']:
        db_info = health['databases']['ai_memories']
        if 'memory_count' in db_info:
            print(f"   üíæ Total memories: {db_info['memory_count']}")
        else:
            print(f"   üíæ Database status: {db_info['status']}")
    
    print("\n‚úÖ Basic usage example completed!")
    print("üí° The memories are now stored and searchable.")


async def conversation_storage_example():
    """Demonstrate conversation storage and retrieval"""
    
    print("\nüí¨ Conversation Storage Example")
    print("=" * 40)
    
    memory = PersistentAIMemorySystem()
    
    # Simulate storing a conversation
    print("üìù Storing a sample conversation...")
    
    # Store user message
    user_msg = await memory.conversations_db.store_message(
        content="Can you help me understand how embeddings work?",
        role="user",
        metadata={"source": "example_conversation"}
    )
    session_id = user_msg["session_id"]
    conversation_id = user_msg["conversation_id"]
    
    # Store assistant response
    await memory.conversations_db.store_message(
        content="Embeddings are vector representations of text that capture semantic meaning. They allow us to measure similarity between pieces of text mathematically.",
        role="assistant",
        session_id=session_id,
        conversation_id=conversation_id,
        metadata={"source": "example_conversation"}
    )
    
    print(f"   ‚úÖ Conversation stored in session: {session_id[:8]}...")
    
    # Retrieve recent messages
    print("\nüìú Retrieving recent conversation context...")
    recent = await memory.conversations_db.get_recent_messages(limit=5)
    
    for i, msg in enumerate(recent):
        role_emoji = "üë§" if msg["role"] == "user" else "ü§ñ"
        content_preview = msg["content"][:80] + "..." if len(msg["content"]) > 80 else msg["content"]
        print(f"   {i+1}. {role_emoji} [{msg['role']}]: {content_preview}")
    
    print("\n‚úÖ Conversation storage example completed!")


async def file_monitoring_example():
    """Demonstrate file monitoring capabilities"""
    
    print("\nüìÅ File Monitoring Example")
    print("=" * 30)
    
    print("üîç File monitoring capabilities...")
    print("   This system can be configured to monitor:")
    print("   ‚Ä¢ VS Code chat sessions")
    print("   ‚Ä¢ LM Studio conversations")
    print("   ‚Ä¢ Custom conversation files")
    print("   ‚Ä¢ Real-time conversation imports")
    
    print("\nüí° Note: File monitoring requires additional setup")
    print("   See the documentation for configuration details")
    
    print("\n‚úÖ File monitoring example completed!")
    print("üéØ Ready for production conversation tracking!")


async def mcp_server_example():
    """Show how to use the MCP server"""
    
    print("\nüõ†Ô∏è MCP Server Usage Example")
    print("=" * 35)
    
    print("To use the MCP server for AI assistant integration:")
    print()
    print("1. Start the MCP server:")
    print("   python mcp_server.py")
    print()
    print("2. Your AI assistant can then call tools like:")
    print("   ‚Ä¢ store_memory(content, memory_type, importance_level)")
    print("   ‚Ä¢ search_memories(query, limit)")  
    print("   ‚Ä¢ get_system_health()")
    print("   ‚Ä¢ get_tool_usage_summary(days)")
    print("   ‚Ä¢ reflect_on_tool_usage(days)")
    print()
    print("3. All tool calls are logged for self-reflection!")
    print("   AI assistants can analyze their own behavior patterns.")
    print()
    print("‚úÖ MCP server provides standardized AI tool interface!")


async def main():
    """Run all examples"""
    
    try:
        await basic_usage_example()
        await conversation_storage_example()
        await file_monitoring_example()
        await mcp_server_example()
        
        print("\nüéâ All examples completed successfully!")
        print("\nüí° Next steps:")
        print("   1. Run 'python mcp_server.py' to start the MCP server")
        print("   2. Connect your AI assistant to use the memory tools")
        print("   3. Watch as your AI builds persistent memory!")
        
    except Exception as e:
        print(f"\n‚ùå Error running examples: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/examples/performance_tests.py
[file content begin]
#!/usr/bin/env python3
"""
Performance testing and benchmarking for the Persistent AI Memory System
"""

import asyncio
import time
import statistics
from mcp_server import PersistentAIMemoryMCPServer
import random
import string

class PerformanceTester:
    def __init__(self):
        self.server = PersistentAIMemoryMCPServer()
        self.test_data = []
        self.results = {}
    
    def generate_test_memory(self, size_category="medium"):
        """Generate test memory content of different sizes"""
        base_content = "This is a test memory for performance evaluation containing information about "
        
        if size_category == "small":
            # ~100 characters
            return base_content + "AI capabilities and user preferences in development workflows."
        elif size_category == "medium":
            # ~500 characters
            return base_content + "AI capabilities, user preferences, project context, technical decisions, " \
                   "code patterns, debugging insights, feature requirements, architectural choices, " \
                   "performance considerations, and collaborative development patterns in modern software " \
                   "engineering workflows with emphasis on maintainability and scalability."
        elif size_category == "large":
            # ~1000+ characters
            return base_content + "comprehensive AI capabilities including natural language processing, " \
                   "code generation, debugging assistance, architectural guidance, performance optimization, " \
                   "user preference learning, project context understanding, technical decision tracking, " \
                   "collaborative development support, automated testing insights, deployment strategies, " \
                   "security considerations, documentation generation, code review assistance, refactoring " \
                   "suggestions, design pattern recommendations, database optimization, API design principles, " \
                   "cross-platform compatibility, error handling patterns, logging strategies, monitoring " \
                   "implementation, and continuous integration workflows for enterprise-scale applications."
    
    async def test_memory_storage_performance(self, num_memories=100):
        """Test memory storage performance"""
        print(f"üìù Testing memory storage performance ({num_memories} memories)...")
        
        storage_times = []
        memory_sizes = ["small", "medium", "large"]
        
        for i in range(num_memories):
            size_category = random.choice(memory_sizes)
            content = self.generate_test_memory(size_category)
            
            request = {
                "tool": "store_memory",
                "parameters": {
                    "content": content,
                    "memory_type": f"performance_test_{size_category}",
                    "importance_level": random.randint(1, 10),
                    "tags": [f"test_{i}", size_category, "performance"]
                }
            }
            
            start_time = time.perf_counter()
            result = await self.server.handle_mcp_request(request, client_id=f"perf_test_{i}")
            end_time = time.perf_counter()
            
            execution_time = (end_time - start_time) * 1000  # Convert to milliseconds
            storage_times.append(execution_time)
            
            if i % 20 == 0:
                print(f"   Progress: {i+1}/{num_memories} memories stored")
        
        avg_time = statistics.mean(storage_times)
        median_time = statistics.median(storage_times)
        min_time = min(storage_times)
        max_time = max(storage_times)
        
        print(f"   ‚úÖ Storage Performance Results:")
        print(f"      Average: {avg_time:.2f}ms")
        print(f"      Median:  {median_time:.2f}ms") 
        print(f"      Min:     {min_time:.2f}ms")
        print(f"      Max:     {max_time:.2f}ms")
        
        self.results['storage'] = {
            'count': num_memories,
            'avg_ms': avg_time,
            'median_ms': median_time,
            'min_ms': min_time,
            'max_ms': max_time
        }
        
        return storage_times
    
    async def test_search_performance(self, num_searches=50):
        """Test search performance with various query types"""
        print(f"üîç Testing search performance ({num_searches} searches)...")
        
        search_times = []
        query_types = [
            "AI capabilities",
            "database performance optimization",
            "user preferences and workflow patterns",
            "technical decisions and architecture",
            "debugging insights and solutions",
            "test performance large"
        ]
        
        for i in range(num_searches):
            query = random.choice(query_types)
            limit = random.choice([5, 10, 20])
            
            request = {
                "tool": "search_memories",
                "parameters": {
                    "query": query,
                    "limit": limit
                }
            }
            
            start_time = time.perf_counter()
            result = await self.server.handle_mcp_request(request, client_id=f"search_test_{i}")
            end_time = time.perf_counter()
            
            execution_time = (end_time - start_time) * 1000
            search_times.append(execution_time)
            
            if i % 10 == 0:
                print(f"   Progress: {i+1}/{num_searches} searches completed")
        
        avg_time = statistics.mean(search_times)
        median_time = statistics.median(search_times)
        min_time = min(search_times)
        max_time = max(search_times)
        
        print(f"   ‚úÖ Search Performance Results:")
        print(f"      Average: {avg_time:.2f}ms")
        print(f"      Median:  {median_time:.2f}ms")
        print(f"      Min:     {min_time:.2f}ms")
        print(f"      Max:     {max_time:.2f}ms")
        
        self.results['search'] = {
            'count': num_searches,
            'avg_ms': avg_time,
            'median_ms': median_time,
            'min_ms': min_time,
            'max_ms': max_time
        }
        
        return search_times
    
    async def test_concurrent_operations(self, concurrent_count=20):
        """Test performance under concurrent load"""
        print(f"‚ö° Testing concurrent operations ({concurrent_count} simultaneous requests)...")
        
        async def concurrent_operation(operation_id):
            if operation_id % 2 == 0:
                # Store operation
                request = {
                    "tool": "store_memory",
                    "parameters": {
                        "content": f"Concurrent test memory {operation_id}: " + self.generate_test_memory("medium"),
                        "memory_type": "concurrent_test",
                        "importance_level": random.randint(1, 10),
                        "tags": [f"concurrent_{operation_id}", "performance"]
                    }
                }
            else:
                # Search operation
                request = {
                    "tool": "search_memories",
                    "parameters": {
                        "query": "concurrent test performance",
                        "limit": 10
                    }
                }
            
            start_time = time.perf_counter()
            result = await self.server.handle_mcp_request(request, client_id=f"concurrent_{operation_id}")
            end_time = time.perf_counter()
            
            return (end_time - start_time) * 1000, result['status']
        
        start_total = time.perf_counter()
        tasks = [concurrent_operation(i) for i in range(concurrent_count)]
        results = await asyncio.gather(*tasks)
        end_total = time.perf_counter()
        
        execution_times = [r[0] for r in results]
        statuses = [r[1] for r in results]
        
        total_time = (end_total - start_total) * 1000
        avg_time = statistics.mean(execution_times)
        success_rate = (statuses.count('success') / len(statuses)) * 100
        
        print(f"   ‚úÖ Concurrent Performance Results:")
        print(f"      Total time: {total_time:.2f}ms")
        print(f"      Avg per operation: {avg_time:.2f}ms")
        print(f"      Success rate: {success_rate:.1f}%")
        print(f"      Throughput: {(concurrent_count / total_time * 1000):.2f} ops/sec")
        
        self.results['concurrent'] = {
            'count': concurrent_count,
            'total_ms': total_time,
            'avg_ms': avg_time,
            'success_rate': success_rate,
            'throughput_ops_per_sec': concurrent_count / total_time * 1000
        }
    
    async def test_tool_call_logging_overhead(self, num_calls=100):
        """Test the performance overhead of tool call logging"""
        print(f"üìä Testing tool call logging overhead ({num_calls} logged calls)...")
        
        logged_times = []
        
        for i in range(num_calls):
            request = {
                "tool": "store_memory",
                "parameters": {
                    "content": f"Logging overhead test {i}",
                    "memory_type": "logging_test",
                    "importance_level": 5,
                    "tags": ["logging", "overhead"]
                }
            }
            
            start_time = time.perf_counter()
            result = await self.server.handle_mcp_request(request, client_id=f"logging_test_{i}")
            end_time = time.perf_counter()
            
            logged_times.append((end_time - start_time) * 1000)
            
            if i % 25 == 0:
                print(f"   Progress: {i+1}/{num_calls} logged calls")
        
        avg_overhead = statistics.mean(logged_times)
        
        print(f"   ‚úÖ Logging Overhead Results:")
        print(f"      Average time with logging: {avg_overhead:.2f}ms")
        
        self.results['logging_overhead'] = {
            'count': num_calls,
            'avg_ms': avg_overhead
        }
    
    async def generate_performance_report(self):
        """Generate a comprehensive performance report"""
        print("\nüìà Performance Test Report")
        print("=" * 50)
        
        if 'storage' in self.results:
            storage = self.results['storage']
            print(f"Memory Storage:")
            print(f"  ‚Ä¢ {storage['count']} memories stored")
            print(f"  ‚Ä¢ Average: {storage['avg_ms']:.2f}ms per memory")
            print(f"  ‚Ä¢ Throughput: {1000/storage['avg_ms']:.1f} memories/second")
        
        if 'search' in self.results:
            search = self.results['search']
            print(f"\nMemory Search:")
            print(f"  ‚Ä¢ {search['count']} searches performed")
            print(f"  ‚Ä¢ Average: {search['avg_ms']:.2f}ms per search")
            print(f"  ‚Ä¢ Throughput: {1000/search['avg_ms']:.1f} searches/second")
        
        if 'concurrent' in self.results:
            concurrent = self.results['concurrent']
            print(f"\nConcurrent Operations:")
            print(f"  ‚Ä¢ {concurrent['count']} simultaneous operations")
            print(f"  ‚Ä¢ Success rate: {concurrent['success_rate']:.1f}%")
            print(f"  ‚Ä¢ Throughput: {concurrent['throughput_ops_per_sec']:.1f} ops/second")
        
        if 'logging_overhead' in self.results:
            logging = self.results['logging_overhead']
            print(f"\nTool Call Logging:")
            print(f"  ‚Ä¢ {logging['count']} logged tool calls")
            print(f"  ‚Ä¢ Average overhead: {logging['avg_ms']:.2f}ms per call")
        
        # Performance assessment
        storage_good = self.results.get('storage', {}).get('avg_ms', 0) < 100
        search_good = self.results.get('search', {}).get('avg_ms', 0) < 50
        concurrent_good = self.results.get('concurrent', {}).get('success_rate', 0) > 95
        
        print(f"\nüéØ Performance Assessment:")
        print(f"  Storage Speed: {'‚úÖ Excellent' if storage_good else '‚ö†Ô∏è  Needs Optimization'}")
        print(f"  Search Speed:  {'‚úÖ Excellent' if search_good else '‚ö†Ô∏è  Needs Optimization'}")
        print(f"  Reliability:   {'‚úÖ Excellent' if concurrent_good else '‚ö†Ô∏è  Needs Optimization'}")


async def main():
    """Run complete performance test suite"""
    
    print("üöÄ Persistent AI Memory System - Performance Tests")
    print("=" * 60)
    print("This will test storage, search, and concurrent performance...")
    print()
    
    tester = PerformanceTester()
    
    try:
        # Run performance tests
        await tester.test_memory_storage_performance(100)
        await tester.test_search_performance(50)
        await tester.test_concurrent_operations(20)
        await tester.test_tool_call_logging_overhead(100)
        
        # Generate report
        await tester.generate_performance_report()
        
        print("\n‚úÖ Performance testing completed!")
        print("\nüí° Tips for optimization:")
        print("   ‚Ä¢ Increase LM Studio embedding batch size for better throughput")
        print("   ‚Ä¢ Use connection pooling for high-concurrency scenarios")  
        print("   ‚Ä¢ Consider memory importance-based indexing for large datasets")
        print("   ‚Ä¢ Monitor SQLite WAL mode for write-heavy workloads")
        
    except Exception as e:
        print(f"\n‚ùå Performance test error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/install.bat
[file content begin]
@echo off
REM Quick Install Script for Persistent AI Memory System (Windows)

echo üöÄ Installing Persistent AI Memory System...
echo ================================================

REM Check if Python is installed
python --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Python 3 is required but not installed.
    echo üí° Please install Python 3.8+ from python.org and try again.
    pause
    exit /b 1
)

echo ‚úÖ Python found

REM Clone the repository
echo üì• Cloning repository...
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory

REM Install dependencies
echo üì¶ Installing dependencies...
pip install -r requirements.txt

REM Install the package
echo üîß Installing Persistent AI Memory System...
pip install -e .

REM Run health check
echo üè• Running health check...
python -c "import asyncio; from ai_memory_core import PersistentAIMemorySystem; asyncio.run((lambda: PersistentAIMemorySystem().get_system_health())()).then(lambda h: print(f'System status: {h[\"status\"]}') or print('‚úÖ Installation successful!'))"

echo.
echo üéâ Installation complete!
echo.
echo üìö Quick Start:
echo    python examples\basic_usage.py
echo.
echo üß™ Run tests:
echo    python tests\test_health_check.py
echo.
echo üìñ Documentation:
echo    See README.md for detailed usage instructions
pause
[file content end]

/soulforge/persistent-ai-memory-new/install.sh
[file content begin]
#!/bin/bash
# Quick Install Script for Persistent AI Memory System
# Works on Linux, macOS, and Windows (with Git Bash)

echo "üöÄ Installing Persistent AI Memory System..."
echo "=" * 50

# Check if Python is installed
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python 3 is required but not installed."
    echo "üí° Please install Python 3.8+ and try again."
    exit 1
fi

# Check Python version
python_version=$(python3 -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
echo "‚úÖ Found Python $python_version"

# Clone the repository
echo "üì• Cloning repository..."
git clone https://github.com/savantskie/persistent-ai-memory.git
cd persistent-ai-memory

# Install dependencies
echo "üì¶ Installing dependencies..."
pip3 install -r requirements.txt

# Install the package
echo "üîß Installing Persistent AI Memory System..."
pip3 install -e .

# Run health check
echo "üè• Running health check..."
python3 -c "
import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def test():
    system = PersistentAIMemorySystem()
    health = await system.get_system_health()
    print(f'System status: {health[\"status\"]}')
    print('‚úÖ Installation successful!')

asyncio.run(test())
"

echo ""
echo "üéâ Installation complete!"
echo ""
echo "üìö Quick Start:"
echo "   python3 examples/basic_usage.py"
echo ""
echo "üß™ Run tests:"
echo "   python3 tests/test_health_check.py"
echo ""
echo "üìñ Documentation:"
echo "   See README.md for detailed usage instructions"
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/README.md
[file content begin]
# Llama AI Memory System

## Overview
The Llama AI Memory System is a comprehensive solution designed to enhance the capabilities of Llama language models by providing a persistent memory framework. This system allows for the storage, retrieval, and management of AI-curated memories, enabling the model to maintain context and continuity across interactions.

## Features
- **Persistent Memory Storage**: Store and manage memories that can be accessed across sessions.
- **Integration with Llama**: Seamlessly connect the memory system with the Llama language model for enhanced performance.
- **Database Management**: Efficiently handle conversations, appointments, and reminders.
- **Utility Functions**: Support for logging, timestamp management, and other common tasks.

## Installation
To install the required dependencies, run the following command:

```
pip install -r requirements.txt
```

## Usage
1. **Initialize the Memory System**: Create an instance of the memory system in your application.
2. **Store Memories**: Use the provided methods to store and manage memories.
3. **Integrate with Llama**: Utilize the integration functions to connect with the Llama model.

## Documentation
For detailed integration instructions, please refer to the [Integration Guide](docs/integration_guide.md).

## Contributing
Contributions are welcome! Please submit a pull request or open an issue for any enhancements or bug fixes.

## License
This project is licensed under the MIT License. See the LICENSE file for more details.
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/docs/integration_guide.md
[file content begin]
# Integration Guide for Llama AI Memory

## Overview

This guide provides detailed instructions on how to integrate the Persistent AI Memory System with the Llama language model. By following these steps, you will enable the Llama model to access and utilize stored memories effectively.

## Prerequisites

Before you begin, ensure that you have the following:

- Python 3.7 or higher
- The Persistent AI Memory System installed
- The Llama language model installed and configured

## Installation

1. **Clone the Repository**

   Clone the repository containing the AI Memory System:

   ```
   git clone <repository-url>
   cd llama-ai-memory
   ```

2. **Install Dependencies**

   Install the required dependencies listed in `requirements.txt`:

   ```
   pip install -r requirements.txt
   ```

## Configuration

1. **Set Up the Memory System**

   Ensure that the AI Memory System is properly configured. You can modify the configuration settings in `src/ai_memory_core.py` as needed.

2. **Integrate with Llama**

   In `src/llama_integration.py`, you will find functions that facilitate communication between the AI Memory System and the Llama model. Make sure to adjust any parameters specific to your Llama setup.

## Code Examples

### Storing Memories

To store a memory in the AI Memory System, you can use the following code snippet:

```python
from src.ai_memory_core import PersistentAIMemorySystem

memory_system = PersistentAIMemorySystem()
memory_id = await memory_system.create_memory(
    content="This is a sample memory for Llama integration.",
    memory_type="integration_example",
    importance_level=5,
    tags=["llama", "integration"]
)
print(f"Memory stored with ID: {memory_id}")
```

### Accessing Memories

To access stored memories from the Llama model, use the following example:

```python
from src.llama_integration import LlamaMemoryAccessor

accessor = LlamaMemoryAccessor()
memories = await accessor.retrieve_memories(query="sample memory")
print("Retrieved Memories:", memories)
```

## Conclusion

By following this integration guide, you should be able to successfully connect the Persistent AI Memory System with the Llama language model. For further assistance, refer to the documentation in the `README.md` file or consult the community forums.
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/example_integration.py
[file content begin]
import asyncio
from pathlib import Path
import sys

# Make the package importable when running this example directly
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

try:
    from src.llama_integration import LlamaIntegration
except Exception:
    # If running from repo root, import from the package path
    from llama_ai_memory.src.llama_integration import LlamaIntegration

# ...existing code... replace with real imports when integrating

async def main():
    print("Example integration for Persistent AI Memory with an LLM")
    print("1) Install requirements from `llama-ai-memory/requirements.txt`")
    print("2) Replace this example with your LLM client code and import `PersistentAIMemorySystem` from the main repo.")

if __name__ == '__main__':
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/requirements.txt
[file content begin]
Flask==2.0.1
SQLAlchemy==1.4.22
watchdog==2.1.6
zoneinfo==0.2.0
asyncio==3.4.3
jsonschema==3.2.0
numpy==1.21.0
pandas==1.3.0
requests==2.26.0
aiohttp==3.7.4
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/src/ai_memory_core.py
[file content begin]
"""
1, success_count = success_count + ?, failure_count = failure_count + ?
                   WHERE tool_name = ? AND date = ?""",
                (1 if status == "success" else 0, tool_name, today)
            )
        else:
            await self.execute_update(
                """INSERT INTO tool_usage_stats (tool_name, date, call_count, success_count, failure_count) 
                   VALUES (?, ?, ?, ?, ?)""",
                (tool_name, today, 1, 1 if status == "success" else 0, 0 if status == "success" else 1)
            )
    
    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """Get tool usage summary for the last N days"""
        
        recent_calls = await self.execute_query(
            """SELECT tool_name, status, COUNT(*) as count
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name, status
               ORDER BY count DESC""".format(days)
        )
        
        daily_stats = await self.execute_query(
            """SELECT * FROM tool_usage_stats 
               WHERE date >= date('now', '-{} days')
               ORDER BY date DESC, call_count DESC""".format(days)
        )
        
        most_used = await self.execute_query(
            """SELECT tool_name, COUNT(*) as total_calls
               FROM tool_calls 
               WHERE timestamp >= datetime('now', '-{} days')
               GROUP BY tool_name
               ORDER BY total_calls DESC
               LIMIT 10""".format(days)
        )
        
        return {
            "recent_calls": [dict(row) for row in recent_calls],
            "daily_stats": [dict(row) for row in daily_stats],
            "most_used_tools": [dict(row) for row in most_used],
            "period_days": days
        }
    
    async def get_tool_call_history(self, tool_name: str = None, limit: int = 50) -> List[Dict]:
        """Get recent tool call history, optionally filtered by tool name"""
        
        if tool_name:
            query = "SELECT * FROM tool_calls WHERE tool_name = ? ORDER BY timestamp DESC LIMIT ?"
            params = (tool_name, limit)
        else:
            query = "SELECT * FROM tool_calls ORDER BY timestamp DESC LIMIT ?"
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]

class ConversationDatabase(DatabaseManager):
    """Manages conversation auto-save database"""
    
    def __init__(self, db_path: str = "conversations.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS messages (
                    message_id TEXT PRIMARY KEY,
                    conversation_id TEXT,
                    timestamp TEXT,
                    role TEXT,
                    content TEXT,
                    metadata TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversations (
                    conversation_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    title TEXT
                )
            """)

    async def store_message(self, content: str, role: str, session_id: str = None, 
                          conversation_id: str = None, metadata: Dict = None) -> Dict[str, str]:
        """Store a message and auto-manage sessions/conversations with duplicate detection"""
        timestamp = get_current_timestamp()
        message_id = str(uuid.uuid4())

        # Advanced duplicate detection: check for existing message with same content, role, and session in last hour
        if session_id:
            existing = await self.execute_query(
                """SELECT message_id FROM messages 
                   WHERE content = ? AND role = ? AND session_id = ? 
                   AND timestamp >= datetime('now', '-1 hour')""",
                (content, role, session_id)
            )
            if existing:
                return {"message_id": existing[0]["message_id"], "duplicate": True}

        # Auto-create session if not provided or doesn't exist
        if not session_id:
            session_id = str(uuid.uuid4())

        # Auto-create conversation if not provided
        if not conversation_id:
            conversation_id = str(uuid.uuid4())

        # Store the message
        await self.execute_update(
            """INSERT INTO messages 
               (message_id, conversation_id, timestamp, role, content, metadata) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (message_id, conversation_id, timestamp, role, content, 
             json.dumps(metadata) if metadata else None)
        )

        return {
            "message_id": message_id,
            "conversation_id": conversation_id,
            "session_id": session_id,
            "duplicate": False
        }
    
    async def get_recent_messages(self, limit: int = 10, session_id: str = None) -> List[Dict]:
        """Get recent messages, optionally filtered by session"""
        
        if session_id:
            query = "SELECT * FROM messages WHERE session_id = ? ORDER BY timestamp DESC LIMIT ?"
            params = (session_id, limit)
        else:
            query = "SELECT * FROM messages ORDER BY timestamp DESC LIMIT ?"
            params = (limit,)
        
        rows = await self.execute_query(query, params)
        return [dict(row) for row in rows]

class AIMemoryDatabase(DatabaseManager):
    """Manages AI-curated memories database with enhanced operations"""
    
    def __init__(self, db_path: str = "ai_memories.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS curated_memories (
                    memory_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    timestamp_updated TEXT,
                    source_conversation_id TEXT,
                    memory_type TEXT,
                    content TEXT,
                    importance_level INTEGER,
                    tags TEXT
                )
            """)

    async def create_memory(self, content: str, memory_type: str = None, 
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> str:
        """Create a new curated memory with duplicate detection"""
        memory_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Advanced duplicate detection: check for existing memory with same content, type, and source
        existing = await self.execute_query(
            """SELECT memory_id FROM curated_memories 
                   WHERE content = ? AND memory_type = ? AND source_conversation_id IS ?""",
            (content, memory_type, source_conversation_id)
        )
        if existing:
            return existing[0]["memory_id"]

        await self.execute_update(
            """INSERT INTO curated_memories 
               (memory_id, timestamp_created, timestamp_updated, source_conversation_id, 
                memory_type, content, importance_level, tags) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (memory_id, timestamp, timestamp, source_conversation_id, 
             memory_type, content, importance_level, 
             json.dumps(tags) if tags else None)
        )
        return memory_id

class ScheduleDatabase(DatabaseManager):
    """Manages appointments and reminders database"""
    
    def __init__(self, db_path: str = "schedule.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS appointments (
                    appointment_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    scheduled_datetime TEXT,
                    title TEXT,
                    description TEXT,
                    location TEXT,
                    source_conversation_id TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS reminders (
                    reminder_id TEXT PRIMARY KEY,
                    timestamp_created TEXT,
                    due_datetime TEXT,
                    content TEXT,
                    priority_level INTEGER,
                    source_conversation_id TEXT,
                    completed INTEGER DEFAULT 0
                )
            """)

    async def create_appointment(self, title: str, scheduled_datetime: str, 
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> str:
        """Create a new appointment with duplicate detection"""
        appointment_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing appointment with same title, datetime, location, and source
        existing = await self.execute_query(
            """SELECT appointment_id FROM appointments 
                   WHERE title = ? AND scheduled_datetime = ? AND location IS ? AND source_conversation_id IS ?""",
            (title, scheduled_datetime, location, source_conversation_id)
        )
        if existing:
            return existing[0]["appointment_id"]

        await self.execute_update(
            """INSERT INTO appointments 
               (appointment_id, timestamp_created, scheduled_datetime, title, description, location, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (appointment_id, timestamp, scheduled_datetime, title, description, location, source_conversation_id)
        )
        return appointment_id
    
    async def create_reminder(self, content: str, due_datetime: str, 
                            priority_level: int = 5, source_conversation_id: str = None) -> str:
        """Create a new reminder with duplicate detection"""
        reminder_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing reminder with same content, due_datetime, and source
        existing = await self.execute_query(
            """SELECT reminder_id FROM reminders 
                   WHERE content = ? AND due_datetime = ? AND source_conversation_id IS ?""",
            (content, due_datetime, source_conversation_id)
        )
        if existing:
            return existing[0]["reminder_id"]

        await self.execute_update(
            """INSERT INTO reminders 
               (reminder_id, timestamp_created, due_datetime, content, priority_level, source_conversation_id) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (reminder_id, timestamp, due_datetime, content, priority_level, source_conversation_id)
        )
        return reminder_id
    
    async def get_upcoming_appointments(self, days_ahead: int = 7) -> List[Dict]:
        """Get upcoming appointments within specified days"""
        
        future_date = datetime.now(get_local_timezone()) + timedelta(days=days_ahead)
        
        rows = await self.execute_query(
            """SELECT * FROM appointments 
               WHERE scheduled_datetime >= ? AND scheduled_datetime <= ?
               ORDER BY scheduled_datetime ASC""",
            (get_current_timestamp(), future_date.isoformat())
        )
        
        return [dict(row) for row in rows]
    
    async def get_active_reminders(self) -> List[Dict]:
        """Get all uncompleted reminders"""
        
        rows = await self.execute_query(
            "SELECT * FROM reminders WHERE completed = 0 ORDER BY due_datetime ASC"
        )
        
        return [dict(row) for row in rows]

class VSCodeProjectDatabase(DatabaseManager):
    """Manages VS Code project context and development sessions"""
    
    def __init__(self, db_path: str = "vscode_project.db"):
        super().__init__(db_path)
        self.initialize_tables()

    def initialize_tables(self):
        """Create tables if they don't exist, and migrate schema if columns are missing"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS project_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_timestamp TEXT,
                    workspace_path TEXT,
                    active_files TEXT,
                    git_branch TEXT,
                    session_summary TEXT
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS development_conversations (
                    conversation_id TEXT PRIMARY KEY,
                    session_id TEXT,
                    timestamp TEXT,
                    chat_context_id TEXT,
                    conversation_content TEXT,
                    decisions_made TEXT,
                    code_changes TEXT
                )
            """)

    async def save_development_session(self, workspace_path: str, active_files: List[str] = None,
                                     git_branch: str = None, session_summary: str = None) -> str:
        """Save a development session"""
        
        session_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        await self.execute_update(
            """INSERT INTO project_sessions 
               (session_id, start_timestamp, workspace_path, active_files, git_branch, session_summary) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (session_id, timestamp, workspace_path, 
             json.dumps(active_files) if active_files else None,
             git_branch, session_summary)
        )
        
        return session_id
    
    async def store_development_conversation(self, content: str, session_id: str = None,
                                          chat_context_id: str = None, decisions_made: str = None,
                                          code_changes: Dict = None) -> str:
        """Store a development conversation from VS Code"""
        conversation_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        # Create session if none provided
        if not session_id:
            session_id = str(uuid.uuid4())
        
        # Store conversation
        await self.execute_update(
            """INSERT INTO development_conversations 
               (conversation_id, session_id, timestamp, chat_context_id,
                conversation_content, decisions_made, code_changes)
               VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (conversation_id, session_id, timestamp, chat_context_id,
             content, decisions_made, json.dumps(code_changes) if code_changes else None)
        )
        
        return conversation_id

    async def store_project_insight(self, content: str, insight_type: str = None,
                                  related_files: List[str] = None, importance_level: int = 5,
                                  source_conversation_id: str = None) -> str:
        """Store a project insight with duplicate detection"""
        insight_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()

        # Duplicate detection: check for existing insight with same content, type, and source
        existing = await self.execute_query(
            """SELECT insight_id FROM project_insights 
                   WHERE content = ? AND insight_type IS ? AND source_conversation_id IS ?""",
            (content, insight_type, source_conversation_id)
        )
        if existing:
            return existing[0]["insight_id"]

        await self.execute_update(
            """INSERT INTO project_insights 
               (insight_id, timestamp_created, timestamp_updated, insight_type, content, 
                related_files, source_conversation_id, importance_level) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (insight_id, timestamp, timestamp, insight_type, content,
             json.dumps(related_files) if related_files else None,
             source_conversation_id, importance_level)
        )
        return insight_id

class ConversationFileMonitor:
    def __init__(self, memory_system, watch_directories):
        self.memory_system = memory_system
        self.watch_directories = watch_directories
        self.vscode_db = memory_system.vscode_db
        self.conversations_db = memory_system.conversations_db
        self.curated_db = memory_system.curated_db

    def _parse_character_ai_format(self, data: Dict) -> List[Dict]:
        """Parse Character.ai conversation format"""
        conversations = []
        try:
            # Parsing logic here
        except Exception as e:
            logger.error(f"Error parsing Character.ai format: {e}")
        return conversations

    def _parse_text_gen_format(self, data: Dict) -> List[Dict]:
        """Parse text-generation-webui format"""
        conversations = []
        try:
            # Parsing logic here
        except Exception as e:
            logger.error(f"Error parsing text-gen format: {e}")
        return conversations

class PersistentAIMemorySystem:
    """Main memory system that coordinates all databases and operations"""
    
    def __init__(self, data_dir: str = "memory_data", enable_file_monitoring: bool = True, 
                 watch_directories: List[str] = None):
        self.data_dir = data_dir
        self.enable_file_monitoring = enable_file_monitoring
        self.watch_directories = watch_directories or []
        self.conversations_db = ConversationDatabase(f"{data_dir}/conversations.db")
        self.ai_memory_db = AIMemoryDatabase(f"{data_dir}/ai_memories.db")
        self.schedule_db = ScheduleDatabase(f"{data_dir}/schedule.db")
        self.vscode_db = VSCodeProjectDatabase(f"{data_dir}/vscode_project.db")
        self.mcp_tool_call_db = MCPToolCallDatabase(f"{data_dir}/mcp_tool_calls.db")

    async def start_file_monitoring(self):
        """Start monitoring conversation files"""
        if not self.watch_directories:
            logger.warning("No directories to monitor.")
            return
        # Monitoring logic here

    async def stop_file_monitoring(self):
        """Stop monitoring conversation files"""
        # Stopping logic here

    async def create_memory(self, content: str, memory_type: str = None,
                          importance_level: int = 5, tags: List[str] = None,
                          source_conversation_id: str = None) -> Dict:
        """Create a new memory"""
        return await self.ai_memory_db.create_memory(content, memory_type, importance_level, tags, source_conversation_id)

    async def store_conversation(self, content: str, role: str, session_id: str = None,
                               conversation_id: str = None, metadata: Dict = None) -> Dict:
        """Store a conversation"""
        return await self.conversations_db.store_message(content, role, session_id, conversation_id, metadata)

    async def create_appointment(self, title: str, scheduled_datetime: str,
                               description: str = None, location: str = None,
                               source_conversation_id: str = None) -> Dict:
        """Create a new appointment"""
        return await self.schedule_db.create_appointment(title, scheduled_datetime, description, location, source_conversation_id)

    async def create_reminder(self, content: str, due_datetime: str,
                            priority_level: int = 5, source_conversation_id: str = None) -> Dict:
        """Create a new reminder"""
        return await self.schedule_db.create_reminder(content, due_datetime, priority_level, source_conversation_id)

async def main():
    """Main entry point for the memory system"""
    memory = PersistentAIMemorySystem()
    logger.info("Persistent AI Memory System initialized.")
    # Example usage
    health = await memory.get_system_health()
    logger.info(f"System Status: {health['status']}")
    # More example usage...

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
"""
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/src/llama_integration.py
[file content begin]
from typing import Any, Dict, List

class LlamaIntegration:
    """Integrates the Persistent AI Memory System with the Llama language model."""

    def __init__(self, memory_system: Any):
        """
        Initialize the LlamaIntegration with a reference to the memory system.

        Args:
            memory_system: An instance of the Persistent AI Memory System.
        """
        self.memory_system = memory_system

    async def retrieve_memory(self, query: str) -> List[Dict]:
        """
        Retrieve memories from the AI memory system based on a query.

        Args:
            query: The query string to search for relevant memories.

        Returns:
            A list of memories that match the query.
        """
        return await self.memory_system.search_memories(query)

    async def store_memory(self, content: str, memory_type: str = None, 
                           importance_level: int = 5, tags: List[str] = None) -> str:
        """
        Store a new memory in the AI memory system.

        Args:
            content: The content of the memory to store.
            memory_type: Optional type of the memory.
            importance_level: Importance level of the memory (default is 5).
            tags: Optional list of tags associated with the memory.

        Returns:
            The ID of the created memory.
        """
        return await self.memory_system.create_memory(content, memory_type, importance_level, tags)

    async def log_tool_call(self, tool_name: str, parameters: Dict, result: Any = None) -> str:
        """
        Log a tool call to the memory system.

        Args:
            tool_name: The name of the tool being called.
            parameters: The parameters used in the tool call.
            result: The result returned from the tool call.

        Returns:
            The ID of the logged tool call.
        """
        return await self.memory_system.log_tool_call(tool_name, parameters, result)

    async def get_tool_usage_summary(self, days: int = 7) -> Dict:
        """
        Get a summary of tool usage over a specified number of days.

        Args:
            days: The number of days to summarize tool usage for.

        Returns:
            A summary of tool usage statistics.
        """
        return await self.memory_system.get_tool_usage_summary(days)
[file content end]

/soulforge/persistent-ai-memory-new/llama-ai-memory/src/utils.py
[file content begin]
def get_current_timestamp() -> str:
    """Get current timestamp in ISO format."""
    return datetime.now().isoformat()

def parse_timestamp(timestamp: Union[str, int, float, None], fallback: Optional[datetime] = None) -> str:
    """Parse various timestamp formats into ISO format string."""
    if not timestamp:
        return fallback.isoformat() if fallback else get_current_timestamp()

    if isinstance(timestamp, (int, float)):
        return datetime.fromtimestamp(timestamp).isoformat()

    try:
        return datetime.fromisoformat(timestamp).isoformat()
    except ValueError:
        return fallback.isoformat() if fallback else get_current_timestamp()

def log_message(message: str, level: str = "INFO") -> None:
    """Log a message with a specified log level."""
    timestamp = get_current_timestamp()
    print(f"[{timestamp}] [{level}] {message}")

def generate_uuid() -> str:
    """Generate a unique identifier."""
    return str(uuid.uuid4())
[file content end]

/soulforge/persistent-ai-memory-new/mcp_server.py
[file content begin]
#!/usr/bin/env python3
"""Minimal MCP shim used by tests.

Provides a small `PersistentAIMemoryMCPServer` implementation that maps a
few tool names to `PersistentAIMemorySystem` methods and logs calls via
`memory_system.log_tool_call` so unit tests can observe persisted
tool-call records.
"""
from typing import Any, Dict, Optional
import asyncio

from ai_memory_core import PersistentAIMemorySystem


class PersistentAIMemoryMCPServer:
    """Tiny MCP shim used by the test-suite."""

    def __init__(self):
        self.memory_system = PersistentAIMemorySystem()

    async def _log_call(self, tool_name: str, parameters: Optional[Dict] = None,
                        execution_time_ms: Optional[float] = None, status: str = "success",
                        result: Any = None, error_message: Optional[str] = None,
                        client_id: Optional[str] = None):
        try:
            if hasattr(self.memory_system, "log_tool_call"):
                await self.memory_system.log_tool_call(
                    tool_name,
                    parameters or {},
                    execution_time_ms,
                    status,
                    result,
                    error_message,
                    client_id,
                )
        except Exception:
            # best-effort logging in tests
            pass

    async def _call_method(self, method_name: str, *args, **kwargs):
        method = getattr(self.memory_system, method_name, None)
        if method is None or not asyncio.iscoroutinefunction(method):
            raise AttributeError(f"Method {method_name} not available on memory system")
        return await method(*args, **kwargs)

    async def handle_mcp_request(self, request: Dict[str, Any], client_id: Optional[str] = None) -> Dict[str, Any]:
        tool = request.get("tool")
        params = request.get("parameters") or {}

        try:
            if tool in ("store_memory", "create_memory"):
                content = params.get("content") or params.get("memory_content")
                memory_type = params.get("memory_type")
                importance = params.get("importance_level", 5)
                tags = params.get("tags")
                res = await self._call_method(
                    "create_memory",
                    content,
                    memory_type,
                    importance,
                    tags,
                    params.get("source_conversation_id"),
                )
                await self._log_call(tool, params, execution_time_ms=None, status="success", result=res, client_id=client_id)
                return {"status": "success", "result": res}

            if tool == "search_memories":
                query = params.get("query")
                limit = params.get("limit", 10)
                res = await self._call_method("search_memories", query, limit)
                await self._log_call(tool, params, execution_time_ms=None, status="success", result=res, client_id=client_id)
                return {"status": "success", "result": res}

            if tool == "get_system_health":
                res = await self._call_method("get_system_health")
                await self._log_call(tool, params, execution_time_ms=None, status="success", result=res, client_id=client_id)
                return {"status": "success", "result": res}

            if tool == "get_tool_call_history":
                limit = params.get("limit", 50)
                rows = []
                try:
                    if hasattr(self.memory_system, "mcp_db") and hasattr(self.memory_system.mcp_db, "get_tool_call_history"):
                        rows = await self.memory_system.mcp_db.get_tool_call_history(limit=limit)
                except Exception:
                    rows = []
                await self._log_call(tool, params, execution_time_ms=None, status="success", result={"history_count": len(rows)}, client_id=client_id)
                return {"status": "success", "result": {"history": rows}}

            if tool == "store_conversation":
                content = params.get("user_message") or params.get("content")
                assistant = params.get("assistant_response")
                session_id = params.get("session_id")
                msg1 = await self._call_method("store_conversation", content, "user", session_id, None, params.get("metadata"))
                if assistant:
                    await self._call_method("store_conversation", assistant, "assistant", session_id, None, params.get("metadata"))
                await self._log_call(tool, params, execution_time_ms=None, status="success", result={"conversation_id": msg1.get('conversation_id') if isinstance(msg1, dict) else None}, client_id=client_id)
                return {"status": "success", "result": {"conversation_id": msg1.get('conversation_id') if isinstance(msg1, dict) else None}}

            # default: unknown tool
            return {"status": "error", "error": f"Unknown tool: {tool}"}

        except Exception as e:
            await self._log_call(tool or "unknown", params, execution_time_ms=None, status="error", error_message=str(e), client_id=client_id)
            return {"status": "error", "error": str(e)}
[file content end]

/soulforge/persistent-ai-memory-new/mcp_server_clean.py
[file content begin]
#!/usr/bin/env python3
"""Clean MCP server shim used for local verification.

This is a drop-in style shim (different module name) used to validate
that the memory system's `log_tool_call` API persists entries.
"""
from typing import Any, Dict, Optional
import asyncio

from ai_memory_core import PersistentAIMemorySystem


class PersistentAIMemoryMCPServer:
    def __init__(self):
        self.memory_system = PersistentAIMemorySystem()

    async def _log_call(self, tool_name: str, parameters: Dict = None, execution_time_ms: float = None, status: str = "success", result: Any = None, error_message: str = None, client_id: Optional[str] = None):
        try:
            if hasattr(self.memory_system, "log_tool_call"):
                await self.memory_system.log_tool_call(tool_name, parameters or {}, execution_time_ms, status, result, error_message, client_id)
        except Exception:
            pass

    async def _call_method(self, method_name: str, *args, **kwargs):
        method = getattr(self.memory_system, method_name, None)
        if method is None or not asyncio.iscoroutinefunction(method):
            raise AttributeError(f"Method {method_name} not available on memory system")
        return await method(*args, **kwargs)

    async def handle_mcp_request(self, request: Dict[str, Any], client_id: Optional[str] = None) -> Dict[str, Any]:
        tool = request.get("tool")
        params = request.get("parameters") or {}

        try:
            if tool in ("store_memory", "create_memory"):
                content = params.get("content") or params.get("memory_content")
                memory_type = params.get("memory_type")
                importance = params.get("importance_level", 5)
                tags = params.get("tags")
                res = await self._call_method("create_memory", content, memory_type, importance, tags, params.get("source_conversation_id"))
                await self._log_call(tool, params, execution_time_ms=None, status="success", result=res, client_id=client_id)
                return {"status": "success", "result": res}

            if tool == "get_tool_call_history":
                limit = params.get("limit", 50)
                rows = []
                try:
                    if hasattr(self.memory_system, "mcp_db") and hasattr(self.memory_system.mcp_db, "get_tool_call_history"):
                        rows = await self.memory_system.mcp_db.get_tool_call_history(limit=limit)
                except Exception:
                    rows = []
                await self._log_call(tool, params, execution_time_ms=None, status="success", result={"history_count": len(rows)}, client_id=client_id)
                return {"status": "success", "result": {"history": rows}}

            return {"status": "error", "error": f"Unknown tool: {tool}"}

        except Exception as e:
            await self._log_call(tool or "unknown", params, execution_time_ms=None, status="error", error_message=str(e), client_id=client_id)
            return {"status": "error", "error": str(e)}
[file content end]

/soulforge/persistent-ai-memory-new/requirements.txt
[file content begin]
aiohttp>=3.8.0
watchdog>=2.1.0
numpy>=1.21.0
mcp>=1.0.0
aiosqlite>=0.21.0
sqlalchemy>=2.0.0
uvicorn>=0.35.0
fastapi>=0.116.0
python-dotenv>=1.1.1
requests>=2.32.3
[file content end]

/soulforge/persistent-ai-memory-new/scripts/OpenwebuiMCP.bat
[file content begin]
mcpo --host 0.0.0.0 --port 12345 -- <path to your memory mcp file>
[file content end]

/soulforge/persistent-ai-memory-new/scripts/check_health.py
[file content begin]
import sys, pathlib, asyncio
sys.path.insert(0, str(pathlib.Path('.').resolve()))
from ai_memory_core import PersistentAIMemorySystem

async def main():
    ms = PersistentAIMemorySystem(enable_file_monitoring=False)
    health = await ms.get_system_health()
    print('embedding endpoint:', health.get('embedding_service', {}).get('endpoint'))

if __name__ == '__main__':
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/scripts/test_mcp_shim.py
[file content begin]
import asyncio
import sys
import os

# Ensure repository root is on sys.path for local imports
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

from mcp_server_clean import PersistentAIMemoryMCPServer


async def run():
    server = PersistentAIMemoryMCPServer()

    # Create a memory via the shim
    req = {"tool": "create_memory", "parameters": {"content": "Test memory from shim", "memory_type": "note"}}
    res = await server.handle_mcp_request(req, client_id="test-client")
    print("create_memory ->", res)

    # Query tool call history
    req2 = {"tool": "get_tool_call_history", "parameters": {"limit": 10}}
    res2 = await server.handle_mcp_request(req2, client_id="test-client")
    print("get_tool_call_history ->", res2)


if __name__ == "__main__":
    asyncio.run(run())
[file content end]

/soulforge/persistent-ai-memory-new/setup.py
[file content begin]
#!/usr/bin/env python3
"""
Setup script for Persistent AI Memory System
"""

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="persistent-ai-memory",
    version="1.0.0",
    author="Collaborative AI Development Team",
    author_email="",
    description="A comprehensive, real-time memory system for AI assistants",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/savantskie/persistent-ai-memory",
    packages=find_packages(),
    py_modules=["ai_memory_core", "mcp_server"],
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers", 
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Database :: Database Engines/Servers",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Software Development :: Version Control :: Git",
    ],
    python_requires=">=3.8",
    install_requires=[
        "aiohttp>=3.8.0",
        "watchdog>=2.1.0",
        "numpy>=1.21.0",
    ],
    extras_require={
        "dev": [
            "pytest>=6.0",
            "pytest-asyncio>=0.18.0",
            "black>=22.0",
            "flake8>=4.0",
        ],
    },
    entry_points={
        "console_scripts": [
            "pams-server=persistent_ai_memory.mcp_server:main",
        ],
    },
)
[file content end]

/soulforge/persistent-ai-memory-new/start_maintenance_service.bat
[file content begin]
@echo off
REM Friday Automatic Maintenance Service
REM Run this to start the automatic database maintenance service

echo Starting Friday Automatic Database Maintenance...
echo.
echo This service will:
echo - Run database cleanup every 3 hours
echo - Optimize database performance
echo - Remove old data according to retention policies
echo - Prevent database bloat
echo.
echo Press Ctrl+C to stop the service
echo.

cd /d "%~dp0"
python friday_auto_maintenance.py --interval 3

echo.
echo Friday Maintenance Service stopped.
pause
[file content end]

/soulforge/persistent-ai-memory-new/start_maintenance_service.ps1
[file content begin]
# Friday Automatic Maintenance Service
# PowerShell script to run the automatic database maintenance service

Write-Host "Starting Friday Automatic Database Maintenance..." -ForegroundColor Green
Write-Host ""
Write-Host "This service will:" -ForegroundColor Cyan
Write-Host "- Run database cleanup every 3 hours" -ForegroundColor White
Write-Host "- Optimize database performance" -ForegroundColor White  
Write-Host "- Remove old data according to retention policies" -ForegroundColor White
Write-Host "- Prevent database bloat" -ForegroundColor White
Write-Host ""
Write-Host "Press Ctrl+C to stop the service" -ForegroundColor Yellow
Write-Host ""

# Change to script directory
Set-Location $PSScriptRoot

try {
    # Start the maintenance service
    python friday_auto_maintenance.py --interval 3
}
catch {
    Write-Host "Error starting maintenance service: $_" -ForegroundColor Red
}
finally {
    Write-Host ""
    Write-Host "Friday Maintenance Service stopped." -ForegroundColor Yellow
    Read-Host "Press Enter to exit"
}
[file content end]

/soulforge/persistent-ai-memory-new/tests/README.md
[file content begin]
# Test Suite for Persistent AI Memory System

This directory contains comprehensive tests to validate all aspects of the Persistent AI Memory System. These tests help ensure the system works correctly across different environments and use cases.

## Test Files Overview

### Core System Tests

#### `test_health_check.py`
- **Purpose**: Basic system health and database connectivity validation
- **What it tests**: Database initialization, table creation, basic operations
- **Run when**: First installation, after system changes, troubleshooting
- **Expected output**: All database tables healthy, connections working

#### `test_live_import.py`
- **Purpose**: Real-world import and integration testing
- **What it tests**: Live import functionality, cross-system compatibility
- **Run when**: Setting up in new environment, testing integrations
- **Expected output**: Successful imports, no dependency conflicts

#### `test_embeddings.py`
- **Purpose**: LM Studio integration and embedding functionality
- **What it tests**: Embedding generation, semantic search, similarity detection
- **Run when**: Setting up LM Studio integration, embedding troubleshooting
- **Expected output**: Consistent embeddings, accurate semantic search

#### `test_tool_logging.py`
- **Purpose**: MCP tool call logging and AI self-reflection
- **What it tests**: Tool call tracking, execution logging, reflection capabilities
- **Run when**: Validating MCP server functionality, debugging tool calls
- **Expected output**: Accurate tool logs, meaningful AI reflections

#### `test_validation_suite.py`
- **Purpose**: Comprehensive end-to-end system validation
- **What it tests**: All major features, error handling, system readiness
- **Run when**: Before production deployment, full system verification
- **Expected output**: High success rate across all test categories

## Test Categories

### üß† Memory Operations
- Memory storage with embeddings
- Semantic search functionality
- Memory importance and tagging
- Cross-session memory persistence

### üí¨ Conversation Handling
- Conversation storage and retrieval
- Session management
- Metadata handling
- Conversation history tracking

### üîß Tool Call Logging
- MCP tool call tracking
- Execution time monitoring
- Success/failure rate analysis
- AI self-reflection capabilities

### üè• System Health
- Database connectivity
- Embedding service status
- Performance metrics
- Resource utilization

### üìÅ File Monitoring
- Conversation file detection
- VS Code project tracking
- Real-time file watching
- Cross-platform compatibility

### ‚ö†Ô∏è Error Handling
- Invalid input handling
- Network failure recovery
- Database error management
- Graceful degradation

## Running Tests

### Quick Health Check
```bash
python test_health_check.py
```
*Recommended for first-time setup and basic troubleshooting*

### Complete Validation
```bash
python test_validation_suite.py
```
*Comprehensive test - run before production deployment*

### Embedding System Test
```bash
python test_embeddings.py
```
*Requires LM Studio running with text-embedding-nomic-embed-text-v1.5*

### Individual Component Tests
```bash
python test_live_import.py      # Import functionality
python test_tool_logging.py     # MCP tool logging
```

## Test Environment Setup

### Prerequisites
1. **Python 3.8+** with required packages installed
2. **LM Studio** running for embedding tests (optional for basic tests)
3. **SQLite** support (usually built into Python)
4. **Network access** for embedding service tests

### Environment Variables (Optional)
```bash
# Custom embedding service URL
export EMBEDDING_URL="http://your-server:port/v1/embeddings"

# Custom database location
export MEMORY_DB_PATH="/path/to/your/memory.db"
```

## Understanding Test Results

### Success Indicators ‚úÖ
- **Green checkmarks**: Individual tests passed
- **High percentage**: Category success rates >80%
- **"READY FOR PRODUCTION"**: System validated for use

### Warning Signs ‚ö†Ô∏è
- **Yellow warnings**: Partial functionality, may need attention
- **60-80% success rates**: System mostly working but has issues
- **"NEEDS ATTENTION"**: Some components require fixes

### Failure Indicators ‚ùå
- **Red X marks**: Critical failures
- **<60% success rates**: Significant problems
- **Error messages**: Specific issues to address

## Troubleshooting Common Issues

### Database Issues
```bash
# Check database file permissions
ls -la memory.db*

# Reset database (WARNING: loses data)
rm memory.db* && python test_health_check.py
```

### Embedding Service Issues
```bash
# Test LM Studio connection manually
curl -X POST http://192.168.1.50:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model":"text-embedding-nomic-embed-text-v1.5","input":"test"}'
```

### Import Issues
```bash
# Check Python path and dependencies
python -c "import ai_memory_core, mcp_server; print('Imports OK')"

# Install missing dependencies
pip install -r requirements.txt
```

## Performance Benchmarks

### Expected Performance
- **Memory Storage**: <100ms per memory
- **Semantic Search**: <50ms per query
- **Tool Call Logging**: <10ms overhead
- **Database Operations**: <25ms typical

### Performance Testing
See `../examples/performance_tests.py` for detailed performance benchmarking.

## Test Data and Cleanup

### Test Data
Tests create temporary data with identifiable tags:
- `validation_test`, `embedding_test`, `performance_test`
- Test data is automatically isolated and can be safely removed

### Cleanup (Optional)
```sql
-- Remove test data from database
DELETE FROM ai_memories WHERE tags LIKE '%test%';
DELETE FROM conversations WHERE metadata LIKE '%test%';
DELETE FROM mcp_tool_calls WHERE client_id LIKE '%test%';
```

## Contributing Test Cases

When adding new tests:

1. **Follow naming convention**: `test_[component]_[feature].py`
2. **Include error handling**: Test both success and failure cases
3. **Add documentation**: Clear comments explaining test purpose
4. **Update this README**: Document new test files and categories
5. **Test isolation**: Ensure tests don't interfere with each other

## Test Results Interpretation

### For Developers
- Use test results to validate code changes
- Focus on maintaining >95% success rate for critical tests
- Add new test cases when implementing features

### For Users
- Run validation suite before relying on system
- Health check sufficient for basic functionality verification
- Performance tests help optimize for specific workloads

### For System Administrators
- Use health checks for monitoring and alerting
- Validation suite results indicate deployment readiness
- Performance data helps with capacity planning

---

üí° **Tip**: Run tests in a clean environment to ensure accurate results. Consider using virtual environments or containers for isolation.

üöÄ **Goal**: All tests passing indicates a production-ready AI memory system capable of supporting intelligent, learning AI assistants!
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_embeddings.py
[file content begin]

[file content end]

/soulforge/persistent-ai-memory-new/tests/test_health_check.py
[file content begin]

[file content end]

/soulforge/persistent-ai-memory-new/tests/test_integration.py
[file content begin]
#!/usr/bin/env python3
"""
Integration test for the GitHub version to ensure parity with Friday's system
"""

import asyncio
import sys
import tempfile
import shutil
from pathlib import Path

async def test_integration():
    """Test that all major components work together"""
    
    # Create a temporary directory for testing
    test_dir = Path(tempfile.mkdtemp(prefix="ai_memory_test_"))
    print(f"üß™ Testing in: {test_dir}")
    
    try:
        # Import and initialize the memory system
        from ai_memory_core import PersistentAIMemorySystem
        print("‚úÖ Successfully imported PersistentAIMemorySystem")
        
        # Initialize with test directory
        memory_system = PersistentAIMemorySystem(
            data_dir=str(test_dir / "memory_data"),
            enable_file_monitoring=False
        )
        print("‚úÖ Memory system initialized")
        
        # Test basic memory operations
        await memory_system.store_conversation(
            content="This is a test message for the GitHub version",
            role="user",
            session_id="test_session"
        )
        print("‚úÖ Stored conversation successfully")
        
        # Test memory creation
        memory_id = await memory_system.create_memory(
            content="Test memory for integration testing",
            memory_type="test",
            importance_level=5,
            tags=["integration", "test"]
        )
        print(f"‚úÖ Created memory: {memory_id}")
        
        # Test search
        results = await memory_system.search_memories(
            query="test message",
            limit=5
        )
        print(f"‚úÖ Search returned {len(results)} results")
        
        # Test appointment creation
        appointment_id = await memory_system.create_appointment(
            title="Integration Test Appointment",
            scheduled_datetime="2025-08-06T10:00:00-05:00",
            description="Test appointment for GitHub version"
        )
        print(f"‚úÖ Created appointment: {appointment_id}")
        
        # Test system health
        health = await memory_system.get_system_health()
        print(f"‚úÖ System health check passed: {health['status']}")
        
        # Test MCP server import
        from mcp_server import AIMemoryMCPServer
        mcp_server = AIMemoryMCPServer()
        print("‚úÖ MCP server initialized successfully")
        
        print("\nüéâ ALL INTEGRATION TESTS PASSED!")
        print("‚úÖ GitHub version has full parity with Friday's system")
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # Cleanup
        if test_dir.exists():
            shutil.rmtree(test_dir)
            print(f"üßπ Cleaned up test directory: {test_dir}")
    
    return True

if __name__ == "__main__":
    success = asyncio.run(test_integration())
    sys.exit(0 if success else 1)
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_live_import.py
[file content begin]
#!/usr/bin/env python3
"""Live test of file monitoring with conversation import"""

import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def test_live_import():
    print('üéØ Testing Live Conversation Import')
    print('=' * 50)
    
    # Initialize with file monitoring
    memory_system = PersistentAIMemorySystem(enable_file_monitoring=True)
    
    print('üìä Before import - checking current database...')
    health_before = await memory_system.get_system_health()
    messages_before = health_before["databases"]["conversations"]["message_count"]
    print(f'üí¨ Messages in database before: {messages_before}')
    
    print('\nüìÅ Starting file monitoring (this will import existing conversations)...')
    await memory_system.start_file_monitoring()
    
    # Wait a moment for initial scan to complete
    print('‚è≥ Waiting for initial scan to complete...')
    await asyncio.sleep(5)
    
    print('\nüìä After import - checking database...')
    health_after = await memory_system.get_system_health()
    messages_after = health_after["databases"]["conversations"]["message_count"]
    print(f'üí¨ Messages in database after: {messages_after}')
    print(f'üìà New messages imported: {messages_after - messages_before}')
    
    if messages_after > messages_before:
        print('\nüéâ SUCCESS! Conversations were imported!')
        print('üìú Getting recent imported messages...')
        recent = await memory_system.get_recent_context(limit=3)
        
        for i, msg in enumerate(recent["messages"]):
            role_emoji = "üë§" if msg["role"] == "user" else "ü§ñ"
            content_preview = msg["content"][:100] + "..." if len(msg["content"]) > 100 else msg["content"]
            print(f'  {i+1}. {role_emoji} [{msg["role"]}]: {content_preview}')
            
            # Show metadata if available
            if msg.get("metadata"):
                import json
                metadata = json.loads(msg["metadata"]) if isinstance(msg["metadata"], str) else msg["metadata"]
                if "application" in metadata:
                    print(f'     üìç Source: {metadata["application"]} ({metadata.get("file_type", "unknown")})')
    else:
        print('\nü§î No new conversations imported. This might mean:')
        print('   - Conversations were already imported previously')
        print('   - No conversation files found in monitored directories')
        print('   - Files were filtered out as non-conversation files')
    
    print(f'\nüìÅ Monitored directories:')
    for path in health_after["file_monitoring"]["monitored_paths"]:
        print(f'   ‚Ä¢ {path}')
    
    # Stop monitoring
    await memory_system.stop_file_monitoring()
    print(f'\n‚úÖ Test completed!')

if __name__ == "__main__":
    asyncio.run(test_live_import())
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_maintenance.py
[file content begin]
#!/usr/bin/env python3
"""Test the new database maintenance system"""

import asyncio
from ai_memory_core import PersistentAIMemorySystem

async def test_maintenance():
    print("üßπ Testing Database Maintenance System...")
    
    memory = PersistentAIMemorySystem()
    
    # Get system health before maintenance
    print("\nüìä System Health Before Maintenance:")
    health_before = await memory.get_system_health()
    for db_name, info in health_before["databases"].items():
        if "count" in str(info):
            print(f"  {db_name}: {info}")
    
    # Run maintenance
    print("\nüîß Running Database Maintenance...")
    maintenance_result = await memory.run_database_maintenance(force=True)
    
    if "error" in maintenance_result:
        print(f"‚ùå Maintenance failed: {maintenance_result['error']}")
        if maintenance_result.get("manual_cleanup_recommended"):
            print("üí° Manual cleanup steps:")
            print("   1. Check database file sizes in memory_data/")
            print("   2. Consider manually deleting old records")
            print("   3. Run VACUUM on SQLite files if needed")
    else:
        print("‚úÖ Maintenance completed successfully!")
        
        # Show cleanup results
        if "cleanup_results" in maintenance_result:
            print("\nüóëÔ∏è Cleanup Results:")
            for category, results in maintenance_result["cleanup_results"].items():
                if isinstance(results, dict) and "deleted" in str(results):
                    print(f"  {category}: {results}")
        
        # Show optimization results
        if "optimization_results" in maintenance_result:
            print("\n‚ö° Optimization Results:")
            for db_name, results in maintenance_result["optimization_results"].items():
                if "space_saved_mb" in results:
                    print(f"  {db_name}: Saved {results['space_saved_mb']} MB")
    
    # Get system health after maintenance
    print("\nüìä System Health After Maintenance:")
    health_after = await memory.get_system_health()
    for db_name, info in health_after["databases"].items():
        if "count" in str(info):
            print(f"  {db_name}: {info}")

if __name__ == "__main__":
    asyncio.run(test_maintenance())
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_tool_logging.py
[file content begin]
#!/usr/bin/env python3
"""
Test script for MCP Tool Call Logger functionality
"""

import asyncio
import json
from datetime import datetime
from ai_memory_core import PersistentAIMemorySystem
from mcp_server import PersistentAIMemoryMCPServer

async def test_tool_call_logging():
    """Test the MCP tool call logging functionality"""
    
    print("üîß Testing MCP Tool Call Logger")
    print("=" * 50)
    
    # Initialize the MCP server
    server = PersistentAIMemoryMCPServer()
    
    # Test storing a memory (this will be logged)
    print("üìù Testing store_memory tool...")
    request = {
        "tool": "store_memory",
        "parameters": {
            "content": "User prefers detailed technical explanations with code examples",
            "memory_type": "preference", 
            "importance_level": 8,
            "tags": ["communication", "technical", "preferences"]
        }
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    print(f"   Result: {result['status']}")
    print(f"   Call ID: {result.get('call_id')}")
    print(f"   Execution time: {result.get('execution_time_ms', 0):.2f}ms")
    print()
    
    # Test searching memories (this will be logged)
    print("üîç Testing search_memories tool...")
    request = {
        "tool": "search_memories", 
        "parameters": {
            "query": "technical explanations",
            "limit": 5
        }
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    print(f"   Result: {result['status']}")
    print(f"   Call ID: {result.get('call_id')}")
    print(f"   Execution time: {result.get('execution_time_ms', 0):.2f}ms")
    print()
    
    # Test system health (this will be logged)
    print("üíö Testing get_system_health tool...")
    request = {
        "tool": "get_system_health",
        "parameters": {}
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    print(f"   Result: {result['status']}")
    print(f"   Call ID: {result.get('call_id')}")
    print(f"   Execution time: {result.get('execution_time_ms', 0):.2f}ms")
    print()
    
    # Now test the reflection tools
    print("üß† Testing tool usage reflection...")
    request = {
        "tool": "get_tool_usage_summary",
        "parameters": {"days": 1}
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    if result["status"] == "success":
        summary = result["result"]
        print(f"   Total tool calls tracked: {summary['insights']['total_tool_calls']}")
        print(f"   Success rate: {summary['insights']['success_rate_percent']}%")
        print(f"   Reflection: {summary['insights']['reflection']}")
        
        if summary['summary']['most_used_tools']:
            print("   Most used tools:")
            for tool in summary['summary']['most_used_tools'][:3]:
                print(f"     - {tool['tool_name']}: {tool['total_calls']} calls")
    print()
    
    # Test getting tool call history
    print("üìä Testing tool call history...")
    request = {
        "tool": "get_tool_call_history",
        "parameters": {"limit": 10}
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    if result["status"] == "success":
        history = result["result"]["history"]
        print(f"   Retrieved {len(history)} tool calls")
        if history:
            latest = history[0]
            print(f"   Latest call: {latest['tool_name']} at {latest['timestamp']}")
            print(f"   Status: {latest['status']}, Time: {latest.get('execution_time_ms', 0)}ms")
    print()
    
    # Test reflection on tool usage
    print("üí≠ Testing usage pattern reflection...")
    request = {
        "tool": "reflect_on_tool_usage", 
        "parameters": {"days": 1}
    }
    
    result = await server.handle_mcp_request(request, client_id="test_client")
    if result["status"] == "success":
        reflection = result["result"]["reflection"]
        print(f"   Analysis period: {reflection['period_days']} days")
        print("   Usage patterns:")
        for tool in reflection['patterns']['peak_usage_tools']:
            print(f"     - {tool['insight']}")
        
        if reflection['recommendations']:
            print("   Recommendations:")
            for rec in reflection['recommendations']:
                print(f"     - {rec}")
    print()
    
    print("‚úÖ Tool call logging test completed!")
    print()
    print("üéØ Summary:")
    print("   - Tool calls are being logged with timing and results")
    print("   - Usage statistics are tracked daily")
    print("   - Reflection tools provide insights about tool usage patterns") 
    print("   - AI assistants can now analyze their own behavior!")


async def main():
    await test_tool_call_logging()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_utils.py
[file content begin]
"""
Test utilities for Friday's Memory System
"""

import os
import sys

def setup_test_paths():
    """Add the parent directory to Python path for test imports"""
    # Get the directory containing this file
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # Add parent directory (main Friday folder) to Python path
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)
[file content end]

/soulforge/persistent-ai-memory-new/tests/test_validation_suite.py
[file content begin]
#!/usr/bin/env python3
"""
Complete validation test suite for the Persistent AI Memory System
Validates all major functionality and integrations
"""

import asyncio
import os
import tempfile
import shutil
from pathlib import Path
from mcp_server import PersistentAIMemoryMCPServer

class ValidationSuite:
    def __init__(self):
        self.server = PersistentAIMemoryMCPServer()
        self.test_results = {}
        self.total_tests = 0
        self.passed_tests = 0
    
    def log_test_result(self, test_name, passed, details=""):
        """Log test result"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
        
        self.test_results[test_name] = {
            'passed': passed,
            'details': details
        }
        
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"   {status} {test_name}")
        if details and not passed:
            print(f"      Details: {details}")
    
    async def test_memory_operations(self):
        """Test core memory storage and retrieval"""
        print("üß† Testing Memory Operations...")
        
        # Test memory storage
        try:
            store_request = {
                "tool": "store_memory",
                "parameters": {
                    "content": "Validation test memory for system verification",
                    "memory_type": "validation_test",
                    "importance_level": 8,
                    "tags": ["validation", "test", "system_check"]
                }
            }
            
            result = await self.server.handle_mcp_request(store_request, client_id="validation_suite")
            memory_id = result.get('result', {}).get('memory_id')
            
            self.log_test_result(
                "Memory Storage", 
                result['status'] == 'success' and memory_id is not None,
                f"Memory ID: {memory_id}" if memory_id else "No memory ID returned"
            )
            
        except Exception as e:
            self.log_test_result("Memory Storage", False, str(e))
        
        # Test memory search
        try:
            search_request = {
                "tool": "search_memories",
                "parameters": {
                    "query": "validation test system",
                    "limit": 5
                }
            }
            
            result = await self.server.handle_mcp_request(search_request, client_id="validation_suite")
            results = result.get('result', {}).get('results', [])
            
            self.log_test_result(
                "Memory Search",
                result['status'] == 'success' and len(results) > 0,
                f"Found {len(results)} results"
            )
            
        except Exception as e:
            self.log_test_result("Memory Search", False, str(e))
        
        # Test memory listing
        try:
            list_request = {
                "tool": "list_memories",
                "parameters": {"limit": 10}
            }
            
            result = await self.server.handle_mcp_request(list_request, client_id="validation_suite")
            memories = result.get('result', {}).get('memories', [])
            
            self.log_test_result(
                "Memory Listing",
                result['status'] == 'success' and len(memories) > 0,
                f"Listed {len(memories)} memories"
            )
            
        except Exception as e:
            self.log_test_result("Memory Listing", False, str(e))
    
    async def test_conversation_handling(self):
        """Test conversation storage and retrieval"""
        print("\nüí¨ Testing Conversation Handling...")
        
        # Test conversation storage
        try:
            conv_request = {
                "tool": "store_conversation",
                "parameters": {
                    "user_message": "This is a validation test conversation",
                    "assistant_response": "I understand this is a test for system validation",
                    "session_id": "validation_session_001",
                    "metadata": {"test_type": "validation", "purpose": "system_check"}
                }
            }
            
            result = await self.server.handle_mcp_request(conv_request, client_id="validation_suite")
            
            self.log_test_result(
                "Conversation Storage",
                result['status'] == 'success',
                result.get('result', {}).get('conversation_id', 'No conversation ID')
            )
            
        except Exception as e:
            self.log_test_result("Conversation Storage", False, str(e))
        
        # Test conversation retrieval
        try:
            get_conv_request = {
                "tool": "get_conversation_history",
                "parameters": {
                    "session_id": "validation_session_001",
                    "limit": 5
                }
            }
            
            result = await self.server.handle_mcp_request(get_conv_request, client_id="validation_suite")
            conversations = result.get('result', {}).get('conversations', [])
            
            self.log_test_result(
                "Conversation Retrieval",
                result['status'] == 'success' and len(conversations) > 0,
                f"Retrieved {len(conversations)} conversations"
            )
            
        except Exception as e:
            self.log_test_result("Conversation Retrieval", False, str(e))
    
    async def test_tool_call_logging(self):
        """Test MCP tool call logging functionality"""
        print("\nüîß Testing Tool Call Logging...")
        
        # The previous operations should have generated tool call logs
        # Test retrieving tool usage summary
        try:
            usage_request = {
                "tool": "get_tool_usage_summary",
                "parameters": {"days": 1}
            }
            
            result = await self.server.handle_mcp_request(usage_request, client_id="validation_suite")
            insights = result.get('result', {}).get('insights', {})
            
            self.log_test_result(
                "Tool Usage Summary",
                result['status'] == 'success' and insights.get('total_tool_calls', 0) > 0,
                f"Total calls: {insights.get('total_tool_calls', 0)}"
            )
            
        except Exception as e:
            self.log_test_result("Tool Usage Summary", False, str(e))
        
        # Test reflection on tool usage
        try:
            reflection_request = {
                "tool": "reflect_on_tool_usage",
                "parameters": {"days": 1}
            }
            
            result = await self.server.handle_mcp_request(reflection_request, client_id="validation_suite")
            reflection = result.get('result', {}).get('reflection', {})
            
            self.log_test_result(
                "Tool Usage Reflection",
                result['status'] == 'success' and 'insights' in reflection,
                f"Reflection generated: {len(reflection.get('insights', ''))> 0}"
            )
            
        except Exception as e:
            self.log_test_result("Tool Usage Reflection", False, str(e))
    
    async def test_system_health(self):
        """Test system health monitoring"""
        print("\nüè• Testing System Health...")
        
        try:
            health_request = {
                "tool": "get_system_health",
                "parameters": {}
            }
            
            result = await self.server.handle_mcp_request(health_request, client_id="validation_suite")
            health_data = result.get('result', {})
            
            # Check required health metrics
            required_metrics = ['database_status', 'embedding_service', 'memory_stats']
            all_metrics_present = all(metric in health_data for metric in required_metrics)
            
            self.log_test_result(
                "System Health Check",
                result['status'] == 'success' and all_metrics_present,
                f"Health data keys: {list(health_data.keys())}"
            )
            
            # Check individual components
            if 'database_status' in health_data:
                db_status = health_data['database_status']
                self.log_test_result(
                    "Database Health",
                    db_status.get('status') == 'healthy',
                    f"DB tables: {len(db_status.get('tables', []))}"
                )
            
            if 'embedding_service' in health_data:
                embed_status = health_data['embedding_service']
                self.log_test_result(
                    "Embedding Service Health",
                    embed_status.get('status') == 'healthy',
                    f"Service: {embed_status.get('service', 'unknown')}"
                )
            
        except Exception as e:
            self.log_test_result("System Health Check", False, str(e))
    
    async def test_file_monitoring(self):
        """Test file monitoring capabilities"""
        print("\nüìÅ Testing File Monitoring...")
        
        # Test VS Code project detection
        try:
            project_request = {
                "tool": "get_vscode_projects",
                "parameters": {}
            }
            
            result = await self.server.handle_mcp_request(project_request, client_id="validation_suite")
            projects = result.get('result', {}).get('projects', [])
            
            self.log_test_result(
                "VS Code Project Detection",
                result['status'] == 'success',
                f"Found {len(projects)} projects"
            )
            
        except Exception as e:
            self.log_test_result("VS Code Project Detection", False, str(e))
        
        # Test conversation file monitoring (simulated)
        try:
            # This would normally be tested with actual file operations
            # For validation, we just check if the monitoring system is initialized
            monitor_status = hasattr(self.server.memory_system, 'conversation_monitor')
            
            self.log_test_result(
                "File Monitor Initialization",
                monitor_status,
                "Conversation monitor available" if monitor_status else "Monitor not found"
            )
            
        except Exception as e:
            self.log_test_result("File Monitor Initialization", False, str(e))
    
    async def test_error_handling(self):
        """Test error handling and edge cases"""
        print("\n‚ö†Ô∏è  Testing Error Handling...")
        
        # Test invalid tool call
        try:
            invalid_request = {
                "tool": "nonexistent_tool",
                "parameters": {}
            }
            
            result = await self.server.handle_mcp_request(invalid_request, client_id="validation_suite")
            
            self.log_test_result(
                "Invalid Tool Handling",
                result['status'] == 'error',
                f"Error message: {result.get('error', 'No error message')}"
            )
            
        except Exception as e:
            self.log_test_result("Invalid Tool Handling", False, str(e))
        
        # Test malformed parameters
        try:
            malformed_request = {
                "tool": "store_memory",
                "parameters": {
                    "content": "",  # Empty content
                    "importance_level": "invalid"  # Wrong type
                }
            }
            
            result = await self.server.handle_mcp_request(malformed_request, client_id="validation_suite")
            
            self.log_test_result(
                "Malformed Parameter Handling",
                result['status'] == 'error',
                f"Handled gracefully: {result.get('error', 'No error')}"
            )
            
        except Exception as e:
            self.log_test_result("Malformed Parameter Handling", False, str(e))
    
    async def generate_validation_report(self):
        """Generate comprehensive validation report"""
        print("\nüìä Validation Report")
        print("=" * 50)
        
        # Overall statistics
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        print(f"Overall Success Rate: {self.passed_tests}/{self.total_tests} ({success_rate:.1f}%)")
        
        # Category breakdown
        categories = {
            "Memory Operations": ["Memory Storage", "Memory Search", "Memory Listing"],
            "Conversation Handling": ["Conversation Storage", "Conversation Retrieval"],
            "Tool Call Logging": ["Tool Usage Summary", "Tool Usage Reflection"],
            "System Health": ["System Health Check", "Database Health", "Embedding Service Health"],
            "File Monitoring": ["VS Code Project Detection", "File Monitor Initialization"],
            "Error Handling": ["Invalid Tool Handling", "Malformed Parameter Handling"]
        }
        
        print("\nüìã Category Results:")
        for category, tests in categories.items():
            category_passed = sum(1 for test in tests if self.test_results.get(test, {}).get('passed', False))
            category_total = len([test for test in tests if test in self.test_results])
            if category_total > 0:
                category_rate = (category_passed / category_total * 100)
                status = "‚úÖ" if category_rate >= 80 else "‚ö†Ô∏è" if category_rate >= 60 else "‚ùå"
                print(f"   {status} {category}: {category_passed}/{category_total} ({category_rate:.0f}%)")
        
        # System readiness assessment
        critical_tests = [
            "Memory Storage", "Memory Search", "System Health Check", 
            "Database Health", "Tool Usage Summary"
        ]
        critical_passed = sum(1 for test in critical_tests if self.test_results.get(test, {}).get('passed', False))
        system_ready = critical_passed == len(critical_tests)
        
        print(f"\nüéØ System Status: {'‚úÖ READY FOR PRODUCTION' if system_ready else '‚ö†Ô∏è  NEEDS ATTENTION'}")
        
        if system_ready:
            print("   üí° All critical systems are operational!")
            print("   üöÄ The AI memory system is ready for real-world use")
        else:
            print("   üí° Some critical components need attention:")
            for test in critical_tests:
                if not self.test_results.get(test, {}).get('passed', False):
                    print(f"      ‚Ä¢ {test}: {self.test_results.get(test, {}).get('details', 'Failed')}")
        
        # Recommendations
        print(f"\nüí° Recommendations:")
        if success_rate >= 90:
            print("   ‚Ä¢ System is performing excellently")
            print("   ‚Ä¢ Consider adding advanced features or optimizations")
        elif success_rate >= 70:
            print("   ‚Ä¢ System is mostly functional") 
            print("   ‚Ä¢ Address failing components for full reliability")
        else:
            print("   ‚Ä¢ System needs significant attention")
            print("   ‚Ä¢ Review and fix failing components before production use")


async def main():
    """Run complete validation suite"""
    
    print("üîç Persistent AI Memory System - Validation Suite")
    print("=" * 60)
    print("Running comprehensive system validation tests...")
    print()
    
    validator = ValidationSuite()
    
    try:
        # Run all validation tests
        await validator.test_memory_operations()
        await validator.test_conversation_handling()
        await validator.test_tool_call_logging()
        await validator.test_system_health()
        await validator.test_file_monitoring()
        await validator.test_error_handling()
        
        # Generate comprehensive report
        await validator.generate_validation_report()
        
        print("\n‚úÖ Validation suite completed!")
        print("\nüéØ Next Steps:")
        print("   ‚Ä¢ Review any failing tests and address issues")
        print("   ‚Ä¢ Run performance tests if validation passes")
        print("   ‚Ä¢ Deploy to production environment if ready")
        print("   ‚Ä¢ Set up monitoring and alerting for production use")
        
    except Exception as e:
        print(f"\n‚ùå Validation suite error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
[file content end]

/soulforge/persistent-ai-memory-new/utils.py
[file content begin]
"""Utility functions for the persistent AI memory system."""

from datetime import datetime, timezone
from zoneinfo import ZoneInfo
from typing import Optional, Union
import logging

logger = logging.getLogger(__name__)

def get_local_timezone() -> ZoneInfo:
    """Get local timezone based on system settings"""
    try:
        import time
        return ZoneInfo(time.tzname[0])
    except:
        # Fallback to a common timezone if detection fails
        return ZoneInfo("America/Chicago")  # Central Time fallback

def parse_timestamp(timestamp: Union[str, int, float, None], fallback: Optional[datetime] = None) -> str:
    """
    Parse a timestamp into a consistent ISO 8601 local timezone string.

    Args:
        timestamp (Union[str, int, float, None]): The input timestamp to parse.
            - ISO 8601 string (e.g., "2025-08-04T18:30:29Z")
            - Unix timestamp in seconds or milliseconds (e.g., 1628100000 or 1628100000000)
        fallback (Optional[datetime]): A fallback datetime if parsing fails.

    Returns:
        str: The parsed timestamp as an ISO 8601 string in local timezone.
    """
    if timestamp is None:
        # Use fallback or current local time if no timestamp is provided
        fallback_time = fallback or datetime.now(get_local_timezone())
        return fallback_time.isoformat()

    try:
        # Handle ISO 8601 strings
        if isinstance(timestamp, str):
            # Adjust for common quirks (e.g., "Z" for UTC)
            if "Z" in timestamp:
                timestamp = timestamp.replace("Z", "+00:00")
            return datetime.fromisoformat(timestamp).astimezone(get_local_timezone()).isoformat()

        # Handle Unix timestamps
        if isinstance(timestamp, (int, float)):
            # Automatically handle milliseconds vs. seconds
            if timestamp > 10**10:  # Likely milliseconds
                timestamp /= 1000
            return datetime.fromtimestamp(timestamp, get_local_timezone()).isoformat()

    except Exception as e:
        # Log the error and use fallback
        logger.warning(f"Failed to parse timestamp '{timestamp}': {e}")
        fallback_time = fallback or datetime.now(get_local_timezone())
        return fallback_time.isoformat()
    
    # If all parsing attempts fail, use fallback
    fallback_time = fallback or datetime.now(get_local_timezone())
    return fallback_time.isoformat()
[file content end]
[file content end]
